{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliaghababaee/SLO_Asieh/blob/main/VGG_%2B_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBagzD7at0TS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZJ4AJnMNWpL",
        "outputId": "588a35e3-a736-425e-91be-1434e5fe9529"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5KmBxL842CDf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2WqdgK3AfLh-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/train_thickness_retina_iran.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56a63bd-4168-4afe-d3c3-cf9f692c0d31",
        "id": "EShLuXN8vGY9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/train_thickness_retina_iran.zip\n",
            " extracting: train_labels.pkl        \n",
            " extracting: train_thickness_retina.pkl  \n",
            " extracting: train_sp.pkl            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/test_thickness_retina_iran.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa7bdd7-c57a-4cb5-e217-4be4726d4110",
        "id": "raLWKZg2vGY9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/test_thickness_retina_iran.zip\n",
            " extracting: test_labels.pkl         \n",
            " extracting: test_thickness_retina.pkl  \n",
            " extracting: test_sp.pkl             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/train_SLO_iran.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZD6wTi_GBoD",
        "outputId": "68031dd4-6793-4990-de12-96b05253296a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/train_SLO_iran.zip\n",
            " extracting: train_slo_iran.pkl      \n",
            " extracting: train_labels_Iran.pkl   \n",
            " extracting: train_sp_iran.pkl       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/test_SLO_iran.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m7cRiWbGBr2",
        "outputId": "9e4b0afc-9ba6-4f79-b625-32d5f48e4c8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/test_SLO_iran.zip\n",
            " extracting: test_slo_iran.pkl       \n",
            " extracting: test_labels_Iran.pkl    \n",
            " extracting: test_sp_iran.pkl        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Typical Run"
      ],
      "metadata": {
        "id": "y7NA4pKZD6Mt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "shEJuyQAwvel"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L8o2VchRwven"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6eT7xZu5SecF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def Augmentation_1 (x_train, labels_train):\n",
        "\n",
        "    # augmentation\n",
        "    batch=np.zeros_like(x_train, dtype=np.float32)\n",
        "    batch_label=np.zeros_like(labels_train, dtype=np.float32)\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "    rotation_range= 5, # rotation\n",
        "    zoom_range = 0.1,\n",
        "    fill_mode='nearest',\n",
        "    data_format='channels_last',\n",
        "      )\n",
        "\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        x1=x_train[i,:,:,:].copy()\n",
        "        x1=x1.reshape((1, ) + x1.shape)\n",
        "        x = datagen.flow(x1, batch_size=1, seed=2020) # to make the result reproducible\n",
        "\n",
        "\n",
        "        batch[i,:,:,:] = x.next()\n",
        "        batch_label[i] = labels_train[i]\n",
        "\n",
        "    return batch, batch_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aqhPh34AfcOe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def Augmentation_2 (x_train, labels_train):\n",
        "\n",
        "    # augmentation\n",
        "    batch=np.zeros_like(x_train, dtype=np.float32)\n",
        "    batch_label=np.zeros_like(labels_train, dtype=np.float32)\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        #rotation_range= 5, # rotation\n",
        "        width_shift_range= [-30, 30], # horizontal shift\n",
        "        #height_shift_range= [-5, 5] , # vertical shift\n",
        "        zoom_range= 0.2,\n",
        "        vertical_flip= True , # vertical flip\n",
        "        #brightness_range= [0.2, 1.5],\n",
        "          )\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        x1=x_train[i,:,:,:].copy()\n",
        "        x1=x1.reshape((1, ) + x1.shape)\n",
        "        x = datagen.flow(x1, batch_size=1, seed=2020) # to make the result reproducible\n",
        "\n",
        "\n",
        "        batch[i,:,:,:] = x.next()\n",
        "        batch_label[i] = labels_train[i]\n",
        "\n",
        "    ###################################################################\n",
        "    # Final data\n",
        "    ###################################################################\n",
        "\n",
        "    x = np.concatenate([x_train,batch])\n",
        "\n",
        "    labels = np.concatenate([labels_train,batch_label])\n",
        "\n",
        "    ############################\n",
        "\n",
        "    ############################\n",
        "    return x, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.transform import resize"
      ],
      "metadata": {
        "id": "wHGAuOGlHAf0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d8cj2xfPfhyn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "\n",
        "            #data.append(np.array(x[i][j]))\n",
        "            #label.append(y[i])\n",
        "\n",
        "            img = resize (x[i][j], (224, 224, 1), mode = 'constant', preserve_range= True)\n",
        "            data.append(img)\n",
        "            label.append(y[i])\n",
        "\n",
        "    data = np.reshape(data, np.shape(data))\n",
        "    return data, np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PkgmjZZ8foyX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import sklearn\n",
        "\n",
        "# TP = confusion[1,1] # true positive\n",
        "# TN = confusion[0,0] # true negatives\n",
        "# FP = confusion[0,1] # false positives\n",
        "# FN = confusion[1,0] # false negatives\n",
        "\n",
        "def metrics_calculation(y_valid, y_pred, y_prob):\n",
        "\n",
        "    #####################################################\n",
        "    #Get the confusion matrix\n",
        "    #####################################################\n",
        "    ROC_AUC = roc_auc_score(y_valid, y_prob)\n",
        "    f1 = metrics.f1_score(y_valid, y_pred, average='weighted')\n",
        "    precision, recall, thresholds = precision_recall_curve(y_valid, y_prob)\n",
        "    P_R_AUC = auc(recall, precision)\n",
        "    cm = sklearn.metrics.confusion_matrix(y_valid, y_pred, normalize='pred')\n",
        "    #Now the normalize the diagonal entries\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    class_acc = cm.diagonal()\n",
        "\n",
        "    Specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "    Sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
        "    Precision   = cm[1,1]/(cm[0,1]+cm[1,1])\n",
        "\n",
        "\n",
        "    return Specificity, Sensitivity, Precision, f1, ROC_AUC, P_R_AUC, class_acc, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GNzoww19fvXa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "def curve_ploting(ax, axx, mean_fpr, aucs, tprs, y_test, y_pred, classifier, kernel=[]):\n",
        "\n",
        "    ###################### Continuing Ploting ROC curve for each fold and the mean ############\n",
        "\n",
        "    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
        "\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(aucs)\n",
        "    ax.plot(\n",
        "        mean_fpr,\n",
        "        mean_tpr,\n",
        "        color=\"b\",\n",
        "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
        "        lw=2,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "    ax.fill_between(\n",
        "        mean_fpr,\n",
        "        tprs_lower,\n",
        "        tprs_upper,\n",
        "        color=\"grey\",\n",
        "        alpha=0.2,\n",
        "        label=r\"$\\pm$ 1 std. dev.\",\n",
        "    )\n",
        "\n",
        "    ax.set(\n",
        "        xlim=[-0.05, 1.05],\n",
        "        ylim=[-0.05, 1.05],\n",
        "    )\n",
        "\n",
        "    if kernel:\n",
        "        ax.set_title(f\"ROC Curve of {classifier} classifier ({kernel} kernel) \")\n",
        "    else:\n",
        "        ax.set_title(f\"ROC Curve of {classifier} classifier\")\n",
        "    ax.legend(loc=\"lower right\")\n",
        "\n",
        "    ###################### Continuing Ploting P_R_curve for each fold and the mean ############\n",
        "    ###\n",
        "\n",
        "    no_skill = len(np.array(y_test)[np.array(y_test)==1]) / len(np.array(y_test))\n",
        "\n",
        "    axx.plot([0, 1], [no_skill, no_skill], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
        "\n",
        "    axx.plot(\n",
        "        recall,\n",
        "        precision,\n",
        "        color=\"b\",\n",
        "        label=r\"Mean P_R curve (AUC =  %0.2f)\" % (auc(recall, precision)),\n",
        "        lw=2,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "\n",
        "\n",
        "    # axis labels\n",
        "    axx.set_xlabel('Recall')\n",
        "    axx.set_ylabel('Precision')\n",
        "    # show the legend\n",
        "    axx.legend(loc=\"lower left\")\n",
        "    if kernel:\n",
        "        axx.set_title(f'Precision-Recall Curve of {classifier} classifier ({kernel} kernel)')\n",
        "    else:\n",
        "        axx.set_title(f'Precision-Recall Curve of {classifier} classifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Bb-zc6m7f2fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc, roc_curve, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def fold_curves(ax, axx, y_valid, fold_number, mean_fpr, pred_proba, tprs=[], aucs=[]):\n",
        "    ############ ROC Curve\n",
        "    lr_fpr, lr_tpr, _ = roc_curve(y_valid, pred_proba)\n",
        "    roc_auc = roc_auc_score(y_valid, pred_proba)\n",
        "    ax.plot(lr_fpr, lr_tpr, lw=1, alpha=0.3, label=r\"P_R_curve fold %d (AUC =  %0.2f)\" % (fold_number, roc_auc))\n",
        "    # axis labels\n",
        "    ax.set_xlabel('False Positive Rate (Positive label: 1.0)')\n",
        "    ax.set_ylabel('True Positive Rate (Positive label: 1.0)')\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, lr_fpr, lr_tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(roc_auc)\n",
        "\n",
        "\n",
        "    ############ P_R Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_valid, pred_proba)\n",
        "    # plot the model precision-recall curve\n",
        "    axx.plot(recall, precision, lw=1, alpha=0.3, label=r\"P_R_curve fold %d (AUC =  %0.2f)\" % (fold_number, auc(recall, precision)))\n",
        "\n",
        "    return tprs, aucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVC8y_-Z96K9"
      },
      "outputs": [],
      "source": [
        "number_class=2\n",
        "cnn_accc_5iteration=[]\n",
        "cnn_spp_5iteration=[]\n",
        "cnn_see_5iteration=[]\n",
        "cnn_prr_5iteration=[]\n",
        "cnn_f11_5iteration=[]\n",
        "cnn_aucc_5iteration=[]\n",
        "cnn_pr_aucc_5iteration=[]\n",
        "class_acc_5iteration = np.zeros((5,number_class))\n",
        "\n",
        "\n",
        "test_accc_5iteration=[]\n",
        "test_spp_5iteration=[]\n",
        "test_see_5iteration=[]\n",
        "test_prr_5iteration=[]\n",
        "test_f11_5iteration=[]\n",
        "test_aucc_5iteration=[]\n",
        "test_pr_aucc_5iteration=[]\n",
        "class_acc_test_5iteration = np.zeros((5,number_class))\n",
        "n_iter=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "QFU6pHxQ51BQ",
        "outputId": "2ca6ea85-f380-466c-e7a9-ad9edc2b6bf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(304, 60, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extractor for vgg16\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "\n",
        "def cnn_feature_extractor(input_img, n_filters=32, n_class=2):\n",
        "\n",
        "    # # Creating model\n",
        "    base_model = tf.keras.applications.ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(60, 256, 3))\n",
        "\n",
        "\n",
        "      ################## fine tune\n",
        "    # for layer in base_model.layers[:-5]:\n",
        "    #   layer.trainable = False\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = Sequential ()\n",
        "\n",
        "    model.add (base_model)\n",
        "    #flatten\n",
        "    model.add(Flatten())\n",
        "\n",
        "    #model.add (Dense (16000, activation = 'relu'))\n",
        "    #model.add(Dropout(0.7))\n",
        "    #model.add (Dense (8000, activation = 'relu'))\n",
        "    #model.add(Dropout(0.3))\n",
        "    #model.add(Dense(2000, activation='relu'))\n",
        "    #model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.7))\n",
        "    ##################################\n",
        "    #################################  EFFECT OF Hidden NEURONS  ###################\n",
        "    #model.add(Dense(3544, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "VLgBVsG6x-1u"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "\n",
        "def cnn_feature_extractor(n_filters=32, n_class=2):\n",
        "\n",
        "\n",
        "    model = Sequential ()\n",
        "    input_img = Input((60, 256, 1))\n",
        "    model.add (Conv2D (n_filters, kernel_size = 3, activation='relu',input_shape=input_img.shape[1:], padding='same'))\n",
        "    model.add (MaxPool2D (2, 2))\n",
        "    model.add (BatchNormalization ())\n",
        "\n",
        "    model.add (Conv2D (n_filters * 2, kernel_size = 3, activation='relu', padding='same'))\n",
        "    model.add (MaxPool2D (2, 2))\n",
        "    model.add (BatchNormalization ())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(32000, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(n_class-1, activation='sigmoid'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "HCcG34mVEBlK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow (x_train [270,:,:,0], cmap = 'jet')"
      ],
      "metadata": {
        "id": "a6IyC3bi6jmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary ()"
      ],
      "metadata": {
        "id": "EjzT_Y1dBpho",
        "outputId": "9919a74a-fee5-4b5d-e147-8f39ec8b4b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 60, 256, 32)       320       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 30, 128, 32)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 30, 128, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 30, 128, 64)       18496     \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 15, 64, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 15, 64, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 61440)             0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 20)                1228820   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1248041 (4.76 MB)\n",
            "Trainable params: 1247849 (4.76 MB)\n",
            "Non-trainable params: 192 (768.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_test.shape"
      ],
      "metadata": {
        "id": "ybPfE9BuGtKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFAFCCYCRDl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn_feature_extractor ()"
      ],
      "metadata": {
        "id": "EpdJuAQCRDpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87860060-b840-4e53-a3d4-b346c8848dd6",
        "id": "hbR00EyOvGY9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------          \n",
            " \t\t\t 1th fold \n",
            "---------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "images_train = pickle.load(open(\"/content/\" + \"train_thickness_retina.pkl\", 'rb'))\n",
        "labels_train = pickle.load(open(\"/content/\" +\"train_labels.pkl\", 'rb'))\n",
        "\n",
        "images_test = pickle.load(open(\"/content/drive/MyDrive/\" +\"test_thickness_new.pkl\", 'rb'))\n",
        "labels_test = pickle.load(open(\"/content/\" +\"test_labels.pkl\", 'rb'))\n",
        "images_test,labels_test = preparing(images_test,labels_test)\n",
        "\n",
        "images_test = np.repeat (images_test, repeats = 3, axis = 3)\n",
        "#####################################################################\n",
        "## Parameters\n",
        "#####################################################################\n",
        "channel = 3\n",
        "number_class = 2\n",
        "\n",
        "cnn_acc    = []\n",
        "cnn_se     = []\n",
        "cnn_sp     = []\n",
        "cnn_pr     = []\n",
        "cnn_f1     = []\n",
        "cnn_auc    = []\n",
        "cnn_pr_auc = []\n",
        "\n",
        "test_acc    = []\n",
        "test_se     = []\n",
        "test_sp     = []\n",
        "test_pr     = []\n",
        "test_f1     = []\n",
        "test_auc    = []\n",
        "test_pr_auc = []\n",
        "\n",
        "\n",
        "class_acc = np.zeros((number_class))\n",
        "class_acc_test = np.zeros((number_class))\n",
        "\n",
        "target_names = ['Normal' , 'MS']\n",
        "confusion_matrix = np.zeros((number_class, number_class))\n",
        "confusion_matrix_test = np.zeros((number_class, number_class))\n",
        "\n",
        "y_test = []\n",
        "tprs   = []\n",
        "aucs   = []\n",
        "y_pred = []\n",
        "x_test = {}\n",
        "\n",
        "mean_fpr  = np.linspace(0, 1, 100)\n",
        "fig, ax   = plt.subplots(figsize=(5, 5))\n",
        "fig1, ax1 = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "\n",
        "#### model parameters for vgg\n",
        "batch_size_vgg    = 8\n",
        "epoch         = 100\n",
        "filter_num    = 32\n",
        "learning_rate_vgg =  0.0004036650839061106\n",
        "\n",
        "\n",
        "\n",
        "#### model parameters for res\n",
        "batch_size_res    = 64\n",
        "epoch         = 100\n",
        "filter_num    = 32\n",
        "learning_rate_res =  0.0003494330691436937\n",
        "\n",
        "\n",
        "#### model parameters for cnn\n",
        "batch_size_cnn    = 8\n",
        "epoch         = 100\n",
        "filter_num    = 32\n",
        "learning_rate_cnn =  0.0005522789895942119\n",
        "\n",
        "#####################################################################\n",
        "## Applying kfold\n",
        "#####################################################################\n",
        "\n",
        "nfold = 5  #please enter number of folds\n",
        "\n",
        "kf_nfold = StratifiedKFold(n_splits=nfold, random_state=None, shuffle=True)\n",
        "\n",
        "n = 0\n",
        "for train_index, val_index in kf_nfold.split(images_train,list(labels_train.values())):\n",
        "    n = n+1\n",
        "    # print(train_index, val_index)  # you can watch train and validation index using this comment\n",
        "    print(f'---------------------------------------------------------------------\\\n",
        "          \\n \\t\\t\\t {n}th fold \\n---------------------------------------------------------------------'\\\n",
        "          ,end = '\\n\\n\\n' )\n",
        "    train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "    x_train = {i: images_train[list(images_train.keys())[i]]  for i in train_index}\n",
        "\n",
        "    x_valid = {i: images_train[list(images_train.keys())[i]]  for i in val_index}\n",
        "\n",
        "    y_train = {i: labels_train[list(labels_train.keys())[i]] for i in train_index}\n",
        "\n",
        "    y_valid = {i: labels_train[list(labels_train.keys())[i]] for i in val_index}\n",
        "\n",
        "    x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "    x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "    #x_test[n] = x_valid\n",
        "    ################# Augmentation\n",
        "\n",
        "    indices = np.where(y_train == 1)[0]\n",
        "\n",
        "    x_train_ms = x_train[indices]\n",
        "    y_train_ms = y_train[indices]\n",
        "\n",
        "    x_train_ms_aug,y_train_ms_aug = Augmentation_1(x_train_ms,y_train_ms)\n",
        "\n",
        "    x_train = np.concatenate ((x_train, x_train_ms_aug), axis=0)\n",
        "    y_train = np.concatenate ((y_train, y_train_ms_aug), axis = 0)\n",
        "\n",
        "    indices = np.random.permutation (len (x_train))\n",
        "\n",
        "    x_train_shuf = x_train [indices]\n",
        "    y_train_shuf = y_train [indices]\n",
        "\n",
        "    x_train,y_train = Augmentation_2(x_train_shuf,y_train_shuf)\n",
        "\n",
        "    x_train = np.repeat (x_train, repeats = 3, axis = 3)\n",
        "\n",
        "    x_valid = np.repeat (x_valid, repeats = 3, axis = 3)\n",
        "\n",
        "    ####################################################################\n",
        "    # classification\n",
        "    ####################################################################\n",
        "\n",
        "    input_img = Input((np.shape(x_train)[1], np.shape(x_train)[2], channel))\n",
        "\n",
        "    model = cnn_feature_extractor(input_img)\n",
        "\n",
        "\n",
        "    METRICS = [\n",
        "#      keras.metrics.TruePositives(name='tp'),\n",
        "#      keras.metrics.FalsePositives(name='fp'),\n",
        "#      keras.metrics.TrueNegatives(name='tn'),\n",
        "#      keras.metrics.FalseNegatives(name='fn'),\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "#      keras.metrics.Precision(name='precision'),\n",
        "#      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "#      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "      ]\n",
        "\n",
        "\n",
        "    my_optimizer =  tf.keras.optimizers.Adam(lr=learning_rate_cnn)\n",
        "    model.compile(optimizer=my_optimizer, loss=\"binary_crossentropy\", metrics=METRICS)\n",
        "    callbacks = [EarlyStopping(patience=20, verbose=1),\n",
        "        ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "        ModelCheckpoint(f'oct{n}.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    #################################\n",
        "    ###### Applying model  ###########\n",
        "    #################################\n",
        "    results = model.fit(x_train, y_train, batch_size=batch_size_cnn, epochs=epoch, callbacks=callbacks,\\\n",
        "                    validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.title(f\"Learning curve {n}th fold\")\n",
        "    plt.plot(results.history[\"loss\"][:-7], label=\"loss\")\n",
        "    plt.plot(results.history[\"val_loss\"][:-7], label=\"val_loss\")\n",
        "    plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"log_loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.title(f\"Learning curve {n}th fold\")\n",
        "    plt.plot(results.history[\"accuracy\"], label=\"accuracy\")\n",
        "    plt.plot(results.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.plot( np.argmax(results.history[\"val_accuracy\"]), np.max(results.history[\"val_accuracy\"]),\\\n",
        "             marker=\"x\", color=\"r\", label=\"best accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    # load the best model\n",
        "    model.load_weights(f'oct{n}.h5')\n",
        "\n",
        "\n",
        "    pred_proba = model.predict(x_valid).ravel()\n",
        "    pred_class = (pred_proba > 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "    ##### calculating metrics\n",
        "\n",
        "    cnn_acc.append(metrics.accuracy_score(y_valid, pred_class))\n",
        "    print(f'accuracy of {n}th fold : {metrics.accuracy_score(y_valid, pred_class)}')\n",
        "    SP, SE, PR, f1, ROC_AUC, P_R_AUC, Class_acc, cm = metrics_calculation(y_valid, pred_class, pred_proba)\n",
        "\n",
        "    cnn_sp.append(SP)\n",
        "    cnn_se.append(SE)\n",
        "    cnn_pr.append(PR)\n",
        "    cnn_f1.append(f1)\n",
        "    cnn_auc.append(ROC_AUC)\n",
        "    cnn_pr_auc.append(P_R_AUC)\n",
        "\n",
        "    #################### acc for each class ##################\n",
        "    class_acc  = np.add(class_acc,Class_acc)\n",
        "\n",
        "    ###################### Total confusion_matrix for poly kernel ############\n",
        "    confusion_matrix = np.add(confusion_matrix,cm)\n",
        "\n",
        "######################## internal test\n",
        "    pred_proba_test = model.predict(images_test).ravel()\n",
        "    pred_class_test = (pred_proba_test > 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "    ##### calculating metrics\n",
        "    print(f'test accuracy of {n}th fold : {metrics.accuracy_score(labels_test, pred_class_test)}')\n",
        "    SP, SE, PR, f1, ROC_AUC, P_R_AUC, Class_acc, cm = metrics_calculation(labels_test, pred_class_test, pred_proba_test)\n",
        "\n",
        "    test_acc.append(metrics.accuracy_score(labels_test, pred_class_test))\n",
        "    test_sp.append(SP)\n",
        "    test_se.append(SE)\n",
        "    test_pr.append(PR)\n",
        "    test_f1.append(f1)\n",
        "    test_auc.append(ROC_AUC)\n",
        "    test_pr_auc.append(P_R_AUC)\n",
        "\n",
        "    #################### acc for each class ##################\n",
        "    class_acc_test  = np.add(class_acc_test,Class_acc)\n",
        "\n",
        "    ###################### Total confusion_matrix for poly kernel ############\n",
        "    confusion_matrix_test = np.add(confusion_matrix_test,cm)\n",
        "\n",
        "    ###################### Ploting ROC and PR curves for each fold ############\n",
        "    y_test = np.append(y_test, y_valid, axis = 0)\n",
        "    y_pred = np.append(y_pred, pred_proba, axis = 0)\n",
        "    ###\n",
        "    tprs, aucs = fold_curves(ax, ax1, y_valid, n, mean_fpr, pred_proba, tprs, aucs)\n",
        "\n",
        "######################  the mean Ploting ROC and PR curves ############\n",
        "###\n",
        "curve_ploting(ax, ax1, mean_fpr, aucs, tprs, y_test, y_pred, 'CNN' )\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#######################################\n",
        "    # ploting confusion matrix\n",
        "#######################################\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix/nfold, display_labels=target_names)\n",
        "disp.plot()\n",
        "\n",
        "\n",
        "########################################\n",
        "#     Metrics printing\n",
        "########################################\n",
        "cnn_accc     = np.mean(cnn_acc)\n",
        "cnn_spp      = np.mean(cnn_sp)\n",
        "cnn_see      = np.mean(cnn_se)\n",
        "cnn_prr      = np.mean(cnn_pr)\n",
        "cnn_f11      = np.mean(cnn_f1)\n",
        "cnn_aucc     = np.mean(cnn_auc)\n",
        "cnn_pr_aucc  = np.mean(cnn_pr_auc)\n",
        "\n",
        "###################### internal test\n",
        "test_accc     = np.mean(test_acc)\n",
        "test_spp      = np.mean(test_sp)\n",
        "test_see      = np.mean(test_se)\n",
        "test_prr      = np.mean(test_pr)\n",
        "test_f11      = np.mean(test_f1)\n",
        "test_aucc     = np.mean(test_auc)\n",
        "test_pr_aucc  = np.mean(test_pr_auc)\n",
        "\n",
        "#################### acc for each class ##################\n",
        "class_acc  = class_acc/nfold\n",
        "class_acc_test  = class_acc_test/nfold\n",
        "\n",
        "print('cnn_acc     = %f' % cnn_accc)\n",
        "print('cnn_sp      = %f' % cnn_spp)\n",
        "print('cnn_se      = %f' % cnn_see)\n",
        "print('cnn_pr      = %f' % cnn_prr)\n",
        "print('cnn_f1      = %f' % cnn_f11)\n",
        "print('cnn_auc     = %f' % cnn_aucc)\n",
        "print('cnn_pr_auc  = %f' % cnn_pr_aucc, end='\\n\\n')\n",
        "\n",
        "\n",
        "print('acc of class %s' % target_names[0], '= %f' % class_acc[0])\n",
        "print('acc of class %s' % target_names[1], '= %f' % class_acc[1], end='\\n\\n')\n",
        "\n",
        "print('test_acc     = %f' % test_accc)\n",
        "print('test_sp      = %f' % test_spp)\n",
        "print('test_se      = %f' % test_see)\n",
        "print('test_pr      = %f' % test_prr)\n",
        "print('test_f1      = %f' % test_f11)\n",
        "print('test_auc     = %f' % test_aucc)\n",
        "print('test_pr_auc  = %f' % test_pr_aucc, end='\\n\\n')\n",
        "\n",
        "\n",
        "print('test acc of class %s' % target_names[0], '= %f' % class_acc_test[0])\n",
        "print('test acc of class %s' % target_names[1], '= %f' % class_acc_test[1], end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7MmYLbwI2Nl",
        "outputId": "f6f9551e-c5f2-4777-b060-824117eb09a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6052631578947368,\n",
              " 0.6052631578947368,\n",
              " 0.6052631578947368,\n",
              " 0.6052631578947368,\n",
              " 0.6052631578947368]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_acc"
      ],
      "metadata": {
        "id": "Q17VngQML192",
        "outputId": "a4953ca7-677b-405d-e032-1d0b39539274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7741935483870968,\n",
              " 0.8064516129032258,\n",
              " 0.8064516129032258,\n",
              " 0.8064516129032258,\n",
              " 0.8064516129032258]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna State of the arts"
      ],
      "metadata": {
        "id": "wOtCIsqsBE8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93603208-4084-4c9c-950c-8fff40e9b662",
        "id": "H_JgMB9avGY-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Requirement already satisfied: optuna-integration in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (from optuna-integration) (3.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (2.0.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->optuna-integration) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from optuna.integration import TFKerasPruningCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################### state of the arts ##############################\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "\n",
        "def classifier (trial):\n",
        "\n",
        "  # Load the pre-trained VGG16 model without the top (fully connected) layers\n",
        "  base_model = tf.keras.applications.resnet.ResNet101(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(60,256, 3),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "  # Freeze the pre-trained layers so they are not trainable during training\n",
        "  #for layer in base_model.layers:\n",
        "    #layer.trainable = False\n",
        "\n",
        "  base_model.trainable = False\n",
        "\n",
        "  model = Sequential ()\n",
        "\n",
        "  model.add (base_model)\n",
        "\n",
        "  model.add (Flatten ())\n",
        "\n",
        "  #dropout_l0 = trial.suggest_float(\"dropout_l0\", 0, 0.7,step=0.1)\n",
        "\n",
        "  #model.add (Dropout (dropout_l0))\n",
        "\n",
        "\n",
        "  n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "\n",
        "\n",
        "  for i in range(n_layers):\n",
        "\n",
        "\n",
        "      n_units = trial.suggest_int(\"n_units_l{}\".format(i), 16, 4096, log = True)\n",
        "\n",
        "      model.add (Dense (n_units, activation = 'relu'))\n",
        "\n",
        "      dropout = trial.suggest_float(\"dropout_l{}\".format (i), 0, 0.7,step=0.1)\n",
        "\n",
        "      model.add (Dropout (rate = dropout))\n",
        "\n",
        "  model.add (Dense (1, activation = 'sigmoid'))\n",
        "\n",
        "  lr = trial.suggest_float ('lr', 1e-5, 1e-1, log = True)\n",
        "\n",
        "  my_optimizer = tf.keras.optimizers.Adam (learning_rate= lr)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=my_optimizer, loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "mpxIKQ-qvGY_"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = 42)\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "            data.append(np.array(x[i][j] * 255))\n",
        "            label.append(y[i])\n",
        "            #img = resize (x[i][j], (224, 224, 1), mode = 'constant', preserve_range= True)\n",
        "            #data.append (img)\n",
        "            #label.append (y [i])\n",
        "\n",
        "\n",
        "    data = np.reshape(data , np.shape(data))\n",
        "    return data, np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def objective (trial):\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "  rotation_range= 5, # rotation\n",
        "  zoom_range = 0.1,\n",
        "  fill_mode='nearest',\n",
        "  data_format='channels_last',\n",
        "    )\n",
        "\n",
        "  datagen2 = ImageDataGenerator(\n",
        "  #rotation_range= 5, # rotation\n",
        "  width_shift_range= [-30, 30], # horizontal shift\n",
        "  #height_shift_range= [-5, 5] , # vertical shift\n",
        "  zoom_range= 0.2,\n",
        "  vertical_flip= True , # vertical flip\n",
        "  #brightness_range= [0.2, 1.5]\n",
        "    )\n",
        "\n",
        "\n",
        "  images_train = pickle.load(open(\"/content/\"+\"train_thickness_retina.pkl\", 'rb'))\n",
        "  labels_train = pickle.load(open(\"/content/\"+\"train_labels.pkl\", 'rb'))\n",
        "\n",
        "  train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "  x_train = {i: images_train[list(images_train.keys())[i]]  for i in train_index}\n",
        "\n",
        "  x_valid = {i: images_train[list(images_train.keys())[i]]  for i in val_index}\n",
        "\n",
        "  y_train = {i: labels_train[list(labels_train.keys())[i]] for i in train_index}\n",
        "\n",
        "  y_valid = {i: labels_train[list(labels_train.keys())[i]] for i in val_index}\n",
        "\n",
        "  x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "  x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "\n",
        "  #rotation_angle  = trial.suggest_float(\"rotation_angle\", 0, 40, step = 10)\n",
        "  #zoom_range = trial.suggest_float(\"zoom_range\", 0, 0.5, step = 0.1)\n",
        "\n",
        "  #color_jitter_brightness = trial.suggest_float (\"color_jitter_brightness\", 0.1, 0.8, step = 0.1)\n",
        "  #shift_horizontal = trial.suggest_uniform(\"shift_horizontal\", 0, 0.7)\n",
        "  #shift_vertical = trial.suggest_uniform(\"shift_vertical\", 0, 0.7)\n",
        "  #vertical_flip_prob = trial.suggest_float (\"vertical_flip_prob\", 0, 1, step = 0.1)\n",
        "\n",
        "  indices = np.where(y_train == 1)[0]\n",
        "\n",
        "  x_train_ms = x_train[indices]\n",
        "  y_train_ms = y_train[indices]\n",
        "  #################Augmentation##################################\n",
        "\n",
        "  # First Augmentation\n",
        "\n",
        "  x_train_ms_aug =np.zeros_like(x_train_ms, dtype=np.float32)\n",
        "\n",
        "  y_train_ms_aug =np.zeros_like(y_train_ms, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_ms)):\n",
        "\n",
        "    x1= x_train_ms[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_ms_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_ms_aug [i] = y_train[i]\n",
        "\n",
        "  x_train = np.concatenate ((x_train, x_train_ms_aug), axis=0)\n",
        "  y_train = np.concatenate ((y_train, y_train_ms_aug), axis = 0)\n",
        "\n",
        "  indices = np.random.permutation (len (x_train))\n",
        "\n",
        "  x_train_shuf = x_train [indices]\n",
        "  y_train_shuf = y_train [indices]\n",
        "\n",
        "  # Second Augmentation\n",
        "\n",
        "  x_train_aug =np.zeros_like(x_train_shuf, dtype=np.float32)\n",
        "\n",
        "  y_train_aug =np.zeros_like(y_train_shuf, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_aug)):\n",
        "\n",
        "    x1= x_train_aug[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen2.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_aug [i] = y_train[i]\n",
        "\n",
        "    x_train = np.concatenate ((x_train_shuf, x_train_aug), axis=0)\n",
        "    y_train = np.concatenate ((y_train_shuf, y_train_aug), axis = 0)\n",
        "\n",
        "\n",
        "  ###############################################################################33\n",
        "\n",
        "\n",
        "  x_train = np.repeat (x_train, repeats = 3, axis = 3)\n",
        "\n",
        "  x_valid = np.repeat (x_valid, repeats = 3, axis = 3)\n",
        "        ####################################################################\n",
        "        # classification via my model\n",
        "        ####################################################################\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  model = classifier (trial)\n",
        "\n",
        "\n",
        "  # Generate our trial model.\n",
        "\n",
        "\n",
        "  model.fit(x_train,\n",
        "            np.asarray(y_train, dtype=np.float64),\n",
        "            batch_size= batch_size,\n",
        "            epochs=50,\n",
        "            callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\"), EarlyStopping(patience=20, verbose=1),\n",
        "            ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "            ModelCheckpoint(f'slo.h5', verbose=1, save_best_only=True, save_weights_only=True)],\n",
        "            validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)),\n",
        "            )\n",
        "  model.load_weights(f'slo.h5')\n",
        "  score = model.evaluate (x_valid, np.asarray(y_valid, dtype=np.float64), verbose = 1)\n",
        "\n",
        "  return score [1]\n"
      ],
      "metadata": {
        "id": "KhGw92auvGY_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "outputId": "3ac6c859-9b9b-402a-e57a-d8a2f528755f",
        "id": "JchBxE8tvGZA"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:27:45,099] A new study created in memory with name: no-name-befd6549-7665-4ded-9d69-1212039c1304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171446536/171446536 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2024-02-27 06:28:05,272] Trial 0 failed with parameters: {'batch_size': 16, 'n_layers': 5, 'n_units_l0': 83, 'dropout_l0': 0.2, 'n_units_l1': 1741, 'dropout_l1': 0.0, 'n_units_l2': 19, 'dropout_l2': 0.5, 'n_units_l3': 142, 'dropout_l3': 0.5, 'n_units_l4': 637, 'dropout_l4': 0.2, 'lr': 0.01157615138379035} because of the following error: InternalError().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-64-9f7e61481ded>\", line 148, in objective\n",
            "    model.fit(x_train,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\", line 103, in convert_to_eager_tensor\n",
            "    return ops.EagerTensor(value, ctx.device_name, dtype)\n",
            "tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n",
            "[W 2024-02-27 06:28:05,277] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-71d9bb1b13ff>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-9f7e61481ded>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m   model.fit(x_train,\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ('1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myw09Hadgqz1",
        "outputId": "3435b2bb-d5b2-46da-cd4c-ca209c85b094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "55CwNaKQvGZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/train_thickness_retina_iran.zip"
      ],
      "metadata": {
        "id": "Abghu8jri54J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/test_thickness_retina_iran.zip"
      ],
      "metadata": {
        "id": "FGd1V92Djg1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna Custom CNN"
      ],
      "metadata": {
        "id": "jwhjDizFBLVK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qF1IO0ku538H",
        "outputId": "57b95f05-244e-4dce-d9f2-878688cfd6da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.2 alembic-1.13.1 colorlog-6.8.2 optuna-3.5.0\n",
            "Collecting optuna-integration\n",
            "  Downloading optuna_integration-3.5.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (from optuna-integration) (3.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (2.0.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->optuna-integration) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration) (2.1.5)\n",
            "Installing collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from optuna.integration import TFKerasPruningCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SFwjWa-eQFQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmLlmRflQG61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input"
      ],
      "metadata": {
        "id": "oHmrmbMJQG9j"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SELSRZwVQHAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "from keras.layers import Input\n",
        "\n",
        "def classifier (trial):\n",
        "  model = Sequential()\n",
        "  input_img = Input((60, 256, 1))\n",
        "  n_filters = trial.suggest_categorical(\"n_filters\", [8, 16, 20, 25])\n",
        "  print(f'n_filters = {n_filters}')\n",
        "\n",
        "  model.add(Conv2D(n_filters * 1, kernel_size=3, activation='relu',input_shape=input_img.shape[1:], padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "\n",
        "  model.add(Conv2D(n_filters * 2, kernel_size=3, activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "\n",
        "  model.add(Conv2D(n_filters * 4, kernel_size=3, activation='relu',padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "\n",
        "  model.add(Conv2D(n_filters * 8, kernel_size=3, activation='relu',padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "\n",
        "\n",
        "  model.add (Flatten ())\n",
        "\n",
        "  dropout_l0 = trial.suggest_float(\"dropout_l0\", 0, 0.7,step=0.1)\n",
        "\n",
        "  model.add (Dropout (dropout_l0))\n",
        "\n",
        "\n",
        "  n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "\n",
        "\n",
        "  for i in range(n_layers):\n",
        "\n",
        "      n_units = trial.suggest_int(\"n_units_l{}\".format(i), 8, 2048, log = True)\n",
        "\n",
        "      model.add (Dense (n_units, activation = 'relu'))\n",
        "\n",
        "      dropout = trial.suggest_float(\"dropout_l{}\".format (i+1), 0, 0.7,step=0.1)\n",
        "\n",
        "      model.add (Dropout (rate = dropout))\n",
        "\n",
        "  model.add (Dense (1, activation = 'sigmoid'))\n",
        "\n",
        "  lr = trial.suggest_float ('lr', 1e-5, 1e-3, log = True)\n",
        "\n",
        "  my_optimizer = tf.keras.optimizers.Adam (learning_rate= lr)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=my_optimizer, loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "EUBYqz_tM8bM"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = 42)\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "            data.append(np.array(x[i][j]))\n",
        "            label.append(y[i])\n",
        "\n",
        "\n",
        "    data = np.reshape(data, np.shape(data))\n",
        "    return data, np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def objective (trial):\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "  rotation_range= 20, # rotation\n",
        "  fill_mode='nearest',\n",
        "  data_format='channels_last',\n",
        "    )\n",
        "\n",
        "  datagen2 = ImageDataGenerator(\n",
        "  rotation_range= 5, # rotation\n",
        "  width_shift_range= [-30, 30], # horizontal shift\n",
        "  height_shift_range= [-5, 5] , # vertical shift\n",
        "  zoom_range= 0.2,\n",
        "  vertical_flip= True , # vertical flip\n",
        "  brightness_range= [0.2, 1.5]\n",
        "    )\n",
        "\n",
        "\n",
        "  images_train = pickle.load(open(\"/content/\"+\"train_thickness_retina.pkl\", 'rb'))\n",
        "  labels_train = pickle.load(open(\"/content/\"+\"train_labels.pkl\", 'rb'))\n",
        "\n",
        "  train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "  x_train = {i: images_train[list(images_train.keys())[i]]  for i in train_index}\n",
        "\n",
        "  x_valid = {i: images_train[list(images_train.keys())[i]]  for i in val_index}\n",
        "\n",
        "  y_train = {i: labels_train[list(labels_train.keys())[i]] for i in train_index}\n",
        "\n",
        "  y_valid = {i: labels_train[list(labels_train.keys())[i]] for i in val_index}\n",
        "\n",
        "  x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "  x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "\n",
        "  #rotation_angle  = trial.suggest_float(\"rotation_angle\", 0, 40, step = 10)\n",
        "  #zoom_range = trial.suggest_float(\"zoom_range\", 0, 0.5, step = 0.1)\n",
        "\n",
        "  #color_jitter_brightness = trial.suggest_float (\"color_jitter_brightness\", 0.1, 0.8, step = 0.1)\n",
        "  #shift_horizontal = trial.suggest_uniform(\"shift_horizontal\", 0, 0.7)\n",
        "  #shift_vertical = trial.suggest_uniform(\"shift_vertical\", 0, 0.7)\n",
        "  #vertical_flip_prob = trial.suggest_float (\"vertical_flip_prob\", 0, 1, step = 0.1)\n",
        "\n",
        "  indices = np.where(y_train == 1)[0]\n",
        "\n",
        "  x_train_ms = x_train[indices]\n",
        "  y_train_ms = y_train[indices]\n",
        "  #################Augmentation##################################\n",
        "\n",
        "  # First Augmentation\n",
        "\n",
        "  x_train_ms_aug =np.zeros_like(x_train_ms, dtype=np.float32)\n",
        "\n",
        "  y_train_ms_aug =np.zeros_like(y_train_ms, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_ms)):\n",
        "\n",
        "    x1= x_train_ms[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_ms_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_ms_aug [i] = y_train[i]\n",
        "\n",
        "  x_train = np.concatenate ((x_train, x_train_ms_aug), axis=0)\n",
        "  y_train = np.concatenate ((y_train, y_train_ms_aug), axis = 0)\n",
        "\n",
        "  indices = np.random.permutation (len (x_train))\n",
        "\n",
        "  x_train_shuf = x_train [indices]\n",
        "  y_train_shuf = y_train [indices]\n",
        "\n",
        "  # Second Augmentation\n",
        "\n",
        "  x_train_aug =np.zeros_like(x_train_shuf, dtype=np.float32)\n",
        "\n",
        "  y_train_aug =np.zeros_like(y_train_shuf, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_aug)):\n",
        "\n",
        "    x1= x_train_aug[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen2.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_aug [i] = y_train[i]\n",
        "\n",
        "    x_train = np.concatenate ((x_train_shuf, x_train_aug), axis=0)\n",
        "    y_train = np.concatenate ((y_train_shuf, y_train_aug), axis = 0)\n",
        "\n",
        "\n",
        "  ###############################################################################33\n",
        "\n",
        "\n",
        "  # x_train = np.repeat (x_train, repeats = 3, axis = 3)\n",
        "\n",
        "  # x_valid = np.repeat (x_valid, repeats = 3, axis = 3)\n",
        "        ####################################################################\n",
        "        # classification via my model\n",
        "        ####################################################################\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  model = classifier (trial)\n",
        "\n",
        "\n",
        "  # Generate our trial model.\n",
        "\n",
        "\n",
        "  model.fit(x_train,\n",
        "            np.asarray(y_train, dtype=np.float64),\n",
        "            batch_size= batch_size,\n",
        "            epochs=50,\n",
        "            callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\"), EarlyStopping(patience=10, verbose=1),\n",
        "            ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "            ModelCheckpoint(f'oct.h5', verbose=1, save_best_only=True, save_weights_only=True)],\n",
        "            validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)),\n",
        "            )\n",
        "  model.load_weights(f'oct.h5')\n",
        "  score = model.evaluate (x_valid, np.asarray(y_valid, dtype=np.float64), verbose = 1)\n",
        "\n",
        "  return score [1]\n"
      ],
      "metadata": {
        "id": "qSVsGFwCM8d1"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NPjODkANDkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8A9srEUdNDpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier (trial):\n",
        "\n",
        "  #num_conv_blocks = trial.suggest_int(\"num_conv_blocks\", 1, 2)\n",
        "\n",
        "  model = Sequential()\n",
        "  input_img = Input((60, 256, 1))\n",
        "  n_filters_1 = trial.suggest_categorical(\"n_filters_1\", [8, 16, 32, 64])\n",
        "  #print(f'n_filters = {n_filters}')\n",
        "\n",
        "  model.add(Conv2D(n_filters_1, kernel_size=3, activation='relu',input_shape=input_img.shape[1:], padding='same'))\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  n_filters_2 = trial.suggest_categorical(\"n_filters_2\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  model.add(Conv2D(n_filters_2, kernel_size=3, activation='relu', padding='same'))\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "  #for i in range(1,num_conv_blocks):\n",
        "\n",
        "    #num_filter = trial.suggest_categorical ('num_filter_l{}'.format (i+2), [16, 32, 64, 128])\n",
        "    #model.add (Conv2D (num_filter, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
        "    #model.add (MaxPool2D (2, 2))\n",
        "    #model.add (BatchNormalization ())\n",
        "\n",
        "  model.add (Flatten ())\n",
        "\n",
        "  num_layers = trial.suggest_int ('num_layers', 1, 4)\n",
        "\n",
        "  for j in range (num_layers):\n",
        "\n",
        "    n_units = trial.suggest_int(\"n_units_l{}\".format(j), 16, 4096, log = True)\n",
        "\n",
        "    model.add (Dense (n_units, activation = 'relu'))\n",
        "\n",
        "    dropout = trial.suggest_float(\"dropout_l{}\".format (j), 0, 0.7,step=0.1)\n",
        "\n",
        "    model.add (Dropout (rate = dropout))\n",
        "\n",
        "  model.add (Dense (1, activation = 'sigmoid'))\n",
        "\n",
        "  lr = trial.suggest_float ('lr', 1e-5, 1e-3, log = True)\n",
        "\n",
        "  my_optimizer = tf.keras.optimizers.Adam (learning_rate= lr)\n",
        "\n",
        "  model.compile(optimizer=my_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "mFFMZWwXIy33"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "            data.append(np.array(x[i][j]))\n",
        "            #data.append (resize (x [i] [j], (128, 128, 1), mode = 'constant', preserve_range = True))\n",
        "            label.append(y[i])\n",
        "\n",
        "\n",
        "    data = np.reshape(data, np.shape(data))\n",
        "    return data, np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def objective (trial):\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "  rotation_range= 5, # rotation\n",
        "  zoom_range =0.1,\n",
        "  fill_mode='nearest',\n",
        "  data_format='channels_last',\n",
        "    )\n",
        "\n",
        "  datagen2 = ImageDataGenerator(\n",
        "  #rotation_range= 5, # rotation\n",
        "  width_shift_range= [-30, 30], # horizontal shift\n",
        "  #height_shift_range= [-5, 5] , # vertical shift\n",
        "  zoom_range= 0.2,\n",
        "  vertical_flip= True , # vertical flip\n",
        "  #brightness_range= [0.2, 1.5]\n",
        "    )\n",
        "\n",
        "\n",
        "  images_train = pickle.load(open(\"/content/\"+\"train_thickness_retina.pkl\", 'rb'))\n",
        "  labels_train = pickle.load(open(\"/content/\"+\"train_labels.pkl\", 'rb'))\n",
        "\n",
        "  train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "  x_train = {i: images_train[list(images_train.keys())[i]]  for i in train_index}\n",
        "\n",
        "  x_valid = {i: images_train[list(images_train.keys())[i]]  for i in val_index}\n",
        "\n",
        "  y_train = {i: labels_train[list(labels_train.keys())[i]] for i in train_index}\n",
        "\n",
        "  y_valid = {i: labels_train[list(labels_train.keys())[i]] for i in val_index}\n",
        "\n",
        "  x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "  x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "\n",
        "  #rotation_angle  = trial.suggest_float(\"rotation_angle\", 0, 40, step = 10)\n",
        "  #zoom_range = trial.suggest_float(\"zoom_range\", 0, 0.5, step = 0.1)\n",
        "\n",
        "  #color_jitter_brightness = trial.suggest_float (\"color_jitter_brightness\", 0.1, 0.8, step = 0.1)\n",
        "  #shift_horizontal = trial.suggest_uniform(\"shift_horizontal\", 0, 0.7)\n",
        "  #shift_vertical = trial.suggest_uniform(\"shift_vertical\", 0, 0.7)\n",
        "  #vertical_flip_prob = trial.suggest_float (\"vertical_flip_prob\", 0, 1, step = 0.1)\n",
        "\n",
        "  indices = np.where(y_train == 1)[0]\n",
        "\n",
        "  x_train_ms = x_train[indices]\n",
        "  y_train_ms = y_train[indices]\n",
        "  #################Augmentation##################################\n",
        "\n",
        "  # First Augmentation\n",
        "\n",
        "  x_train_ms_aug =np.zeros_like(x_train_ms, dtype=np.float32)\n",
        "\n",
        "  y_train_ms_aug =np.zeros_like(y_train_ms, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_ms)):\n",
        "\n",
        "    x1= x_train_ms[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_ms_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_ms_aug [i] = y_train[i]\n",
        "\n",
        "  x_train = np.concatenate ((x_train, x_train_ms_aug), axis=0)\n",
        "  y_train = np.concatenate ((y_train, y_train_ms_aug), axis = 0)\n",
        "\n",
        "  indices = np.random.permutation (len (x_train))\n",
        "\n",
        "  x_train_shuf = x_train [indices]\n",
        "  y_train_shuf = y_train [indices]\n",
        "\n",
        "  # Second Augmentation\n",
        "\n",
        "  x_train_aug =np.zeros_like(x_train_shuf, dtype=np.float32)\n",
        "\n",
        "  y_train_aug  =np.zeros_like(y_train_shuf, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_aug)):\n",
        "\n",
        "    x1= x_train_aug[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen2.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_aug [i] = y_train[i]\n",
        "\n",
        "    x_train = np.concatenate ((x_train_shuf, x_train_aug), axis=0)\n",
        "    y_train = np.concatenate ((y_train_shuf, y_train_aug), axis = 0)\n",
        "\n",
        "\n",
        "  ###############################################################################33\n",
        "\n",
        "\n",
        "  #x_train = np.repeat (x_train, repeats = 3, axis = 3)\n",
        "\n",
        "  #x_valid = np.repeat (x_valid, repeats = 3, axis = 3)\n",
        "        ####################################################################\n",
        "        # classification via my model\n",
        "        ####################################################################\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  model = classifier (trial)\n",
        "\n",
        "\n",
        "  # Generate our trial model.\n",
        "\n",
        "\n",
        "  model.fit(x_train,\n",
        "            np.asarray(y_train, dtype=np.float64),\n",
        "            batch_size= batch_size,\n",
        "            epochs=50,\n",
        "            callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\"), EarlyStopping(patience=10, verbose=1),\n",
        "            ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "            ModelCheckpoint(f'slo.h5', verbose=1, save_best_only=True, save_weights_only=True)],\n",
        "            validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)),\n",
        "            )\n",
        "  model.load_weights(f'slo.h5')\n",
        "  score = model.evaluate (x_valid, np.asarray(y_valid, dtype=np.float64), verbose = 1)\n",
        "\n",
        "  return score [1]\n"
      ],
      "metadata": {
        "id": "anpfy1hrIy60"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "metadata": {
        "id": "fOvpRl7Jh9c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "TEjYNByGh9c-",
        "outputId": "004beb4f-4a64-4d35-b5f3-5a3e1452ec53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  26\n",
            "  Number of pruned trials:  15\n",
            "  Number of complete trials:  10\n",
            "Best trial:\n",
            "  Value:  0.9333333373069763\n",
            "  Params: \n",
            "    batch_size: 8\n",
            "    num_conv_blocks: 3\n",
            "    n_filters_1: 32\n",
            "    num_layers: 1\n",
            "    n_units_l0: 20\n",
            "    dropout_l0: 0.4\n",
            "    lr: 0.0005522789895942119\n"
          ]
        }
      ]
    }
  ]
}