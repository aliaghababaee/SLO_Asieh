{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliaghababaee/SLO_Asieh/blob/main/VGG_%2B_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBagzD7at0TS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZJ4AJnMNWpL",
        "outputId": "58f1476e-132c-4d90-a725-63d5adab8cae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5KmBxL842CDf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2WqdgK3AfLh-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/train_thickness_retina_iran.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b79d976-08de-4677-dc3f-1c710652b0fa",
        "id": "EShLuXN8vGY9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/train_thickness_retina_iran.zip\n",
            " extracting: train_labels.pkl        \n",
            " extracting: train_thickness_retina.pkl  \n",
            " extracting: train_sp.pkl            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/test_thickness_retina_iran.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de621745-efe3-42ae-dd3e-bc7daaa9efdb",
        "id": "raLWKZg2vGY9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/test_thickness_retina_iran.zip\n",
            " extracting: test_labels.pkl         \n",
            " extracting: test_thickness_retina.pkl  \n",
            " extracting: test_sp.pkl             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/train_SLO_iran.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZD6wTi_GBoD",
        "outputId": "68031dd4-6793-4990-de12-96b05253296a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/train_SLO_iran.zip\n",
            " extracting: train_slo_iran.pkl      \n",
            " extracting: train_labels_Iran.pkl   \n",
            " extracting: train_sp_iran.pkl       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/test_SLO_iran.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m7cRiWbGBr2",
        "outputId": "9e4b0afc-9ba6-4f79-b625-32d5f48e4c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/test_SLO_iran.zip\n",
            " extracting: test_slo_iran.pkl       \n",
            " extracting: test_labels_Iran.pkl    \n",
            " extracting: test_sp_iran.pkl        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Typical Run"
      ],
      "metadata": {
        "id": "y7NA4pKZD6Mt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "shEJuyQAwvel"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L8o2VchRwven"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6eT7xZu5SecF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def Augmentation_1 (x_train, labels_train):\n",
        "\n",
        "    # augmentation\n",
        "    batch=np.zeros_like(x_train, dtype=np.float32)\n",
        "    batch_label=np.zeros_like(labels_train, dtype=np.float32)\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "    rotation_range= 5, # rotation\n",
        "    zoom_range = 0.1,\n",
        "    fill_mode='nearest',\n",
        "    data_format='channels_last',\n",
        "      )\n",
        "\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        x1=x_train[i,:,:,:].copy()\n",
        "        x1=x1.reshape((1, ) + x1.shape)\n",
        "        x = datagen.flow(x1, batch_size=1, seed=2020) # to make the result reproducible\n",
        "\n",
        "\n",
        "        batch[i,:,:,:] = x.next()\n",
        "        batch_label[i] = labels_train[i]\n",
        "\n",
        "    return batch, batch_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aqhPh34AfcOe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def Augmentation_2 (x_train, labels_train):\n",
        "\n",
        "    # augmentation\n",
        "    batch=np.zeros_like(x_train, dtype=np.float32)\n",
        "    batch_label=np.zeros_like(labels_train, dtype=np.float32)\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        #rotation_range= 5, # rotation\n",
        "        width_shift_range= [-30, 30], # horizontal shift\n",
        "        #height_shift_range= [-5, 5] , # vertical shift\n",
        "        zoom_range= 0.2,\n",
        "        vertical_flip= True , # vertical flip\n",
        "        #brightness_range= [0.2, 1.5],\n",
        "          )\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        x1=x_train[i,:,:,:].copy()\n",
        "        x1=x1.reshape((1, ) + x1.shape)\n",
        "        x = datagen.flow(x1, batch_size=1, seed=2020) # to make the result reproducible\n",
        "\n",
        "\n",
        "        batch[i,:,:,:] = x.next()\n",
        "        batch_label[i] = labels_train[i]\n",
        "\n",
        "    ###################################################################\n",
        "    # Final data\n",
        "    ###################################################################\n",
        "\n",
        "    x = np.concatenate([x_train,batch])\n",
        "\n",
        "    labels = np.concatenate([labels_train,batch_label])\n",
        "\n",
        "    ############################\n",
        "\n",
        "    ############################\n",
        "    return x, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.transform import resize"
      ],
      "metadata": {
        "id": "wHGAuOGlHAf0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d8cj2xfPfhyn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "\n",
        "            data.append(np.array(x[i][j]* 255))\n",
        "            label.append(y[i])\n",
        "\n",
        "            #img = resize (x[i][j], (224, 224, 1), mode = 'constant', preserve_range= True)\n",
        "            #data.append(img)\n",
        "            #label.append(y[i])\n",
        "\n",
        "    data = np.reshape(data, np.shape(data))\n",
        "    return data, np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PkgmjZZ8foyX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import sklearn\n",
        "\n",
        "# TP = confusion[1,1] # true positive\n",
        "# TN = confusion[0,0] # true negatives\n",
        "# FP = confusion[0,1] # false positives\n",
        "# FN = confusion[1,0] # false negatives\n",
        "\n",
        "def metrics_calculation(y_valid, y_pred, y_prob):\n",
        "\n",
        "    #####################################################\n",
        "    #Get the confusion matrix\n",
        "    #####################################################\n",
        "    ROC_AUC = roc_auc_score(y_valid, y_prob)\n",
        "    f1 = metrics.f1_score(y_valid, y_pred, average='weighted')\n",
        "    precision, recall, thresholds = precision_recall_curve(y_valid, y_prob)\n",
        "    P_R_AUC = auc(recall, precision)\n",
        "    cm = sklearn.metrics.confusion_matrix(y_valid, y_pred, normalize='pred')\n",
        "    #Now the normalize the diagonal entries\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    class_acc = cm.diagonal()\n",
        "\n",
        "    Specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "    Sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
        "    Precision   = cm[1,1]/(cm[0,1]+cm[1,1])\n",
        "\n",
        "\n",
        "    return Specificity, Sensitivity, Precision, f1, ROC_AUC, P_R_AUC, class_acc, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GNzoww19fvXa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "def curve_ploting(ax, axx, mean_fpr, aucs, tprs, y_test, y_pred, classifier, kernel=[]):\n",
        "\n",
        "    ###################### Continuing Ploting ROC curve for each fold and the mean ############\n",
        "\n",
        "    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
        "\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    std_auc = np.std(aucs)\n",
        "    ax.plot(\n",
        "        mean_fpr,\n",
        "        mean_tpr,\n",
        "        color=\"b\",\n",
        "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
        "        lw=2,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "\n",
        "    std_tpr = np.std(tprs, axis=0)\n",
        "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "    ax.fill_between(\n",
        "        mean_fpr,\n",
        "        tprs_lower,\n",
        "        tprs_upper,\n",
        "        color=\"grey\",\n",
        "        alpha=0.2,\n",
        "        label=r\"$\\pm$ 1 std. dev.\",\n",
        "    )\n",
        "\n",
        "    ax.set(\n",
        "        xlim=[-0.05, 1.05],\n",
        "        ylim=[-0.05, 1.05],\n",
        "    )\n",
        "\n",
        "    if kernel:\n",
        "        ax.set_title(f\"ROC Curve of {classifier} classifier ({kernel} kernel) \")\n",
        "    else:\n",
        "        ax.set_title(f\"ROC Curve of {classifier} classifier\")\n",
        "    ax.legend(loc=\"lower right\")\n",
        "\n",
        "    ###################### Continuing Ploting P_R_curve for each fold and the mean ############\n",
        "    ###\n",
        "\n",
        "    no_skill = len(np.array(y_test)[np.array(y_test)==1]) / len(np.array(y_test))\n",
        "\n",
        "    axx.plot([0, 1], [no_skill, no_skill], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
        "\n",
        "    axx.plot(\n",
        "        recall,\n",
        "        precision,\n",
        "        color=\"b\",\n",
        "        label=r\"Mean P_R curve (AUC =  %0.2f)\" % (auc(recall, precision)),\n",
        "        lw=2,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "\n",
        "\n",
        "    # axis labels\n",
        "    axx.set_xlabel('Recall')\n",
        "    axx.set_ylabel('Precision')\n",
        "    # show the legend\n",
        "    axx.legend(loc=\"lower left\")\n",
        "    if kernel:\n",
        "        axx.set_title(f'Precision-Recall Curve of {classifier} classifier ({kernel} kernel)')\n",
        "    else:\n",
        "        axx.set_title(f'Precision-Recall Curve of {classifier} classifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Bb-zc6m7f2fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc, roc_curve, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def fold_curves(ax, axx, y_valid, fold_number, mean_fpr, pred_proba, tprs=[], aucs=[]):\n",
        "    ############ ROC Curve\n",
        "    lr_fpr, lr_tpr, _ = roc_curve(y_valid, pred_proba)\n",
        "    roc_auc = roc_auc_score(y_valid, pred_proba)\n",
        "    ax.plot(lr_fpr, lr_tpr, lw=1, alpha=0.3, label=r\"P_R_curve fold %d (AUC =  %0.2f)\" % (fold_number, roc_auc))\n",
        "    # axis labels\n",
        "    ax.set_xlabel('False Positive Rate (Positive label: 1.0)')\n",
        "    ax.set_ylabel('True Positive Rate (Positive label: 1.0)')\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, lr_fpr, lr_tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(roc_auc)\n",
        "\n",
        "\n",
        "    ############ P_R Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_valid, pred_proba)\n",
        "    # plot the model precision-recall curve\n",
        "    axx.plot(recall, precision, lw=1, alpha=0.3, label=r\"P_R_curve fold %d (AUC =  %0.2f)\" % (fold_number, auc(recall, precision)))\n",
        "\n",
        "    return tprs, aucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVC8y_-Z96K9"
      },
      "outputs": [],
      "source": [
        "number_class=2\n",
        "cnn_accc_5iteration=[]\n",
        "cnn_spp_5iteration=[]\n",
        "cnn_see_5iteration=[]\n",
        "cnn_prr_5iteration=[]\n",
        "cnn_f11_5iteration=[]\n",
        "cnn_aucc_5iteration=[]\n",
        "cnn_pr_aucc_5iteration=[]\n",
        "class_acc_5iteration = np.zeros((5,number_class))\n",
        "\n",
        "\n",
        "test_accc_5iteration=[]\n",
        "test_spp_5iteration=[]\n",
        "test_see_5iteration=[]\n",
        "test_prr_5iteration=[]\n",
        "test_f11_5iteration=[]\n",
        "test_aucc_5iteration=[]\n",
        "test_pr_aucc_5iteration=[]\n",
        "class_acc_test_5iteration = np.zeros((5,number_class))\n",
        "n_iter=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "QFU6pHxQ51BQ",
        "outputId": "2ca6ea85-f380-466c-e7a9-ad9edc2b6bf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(304, 60, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extractor for vgg16\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "\n",
        "def cnn_feature_extractor(input_img, n_filters=32, n_class=2):\n",
        "\n",
        "    # # Creating model\n",
        "    base_model = tf.keras.applications.resnet.ResNet101(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(60, 256, 3))\n",
        "\n",
        "\n",
        "      ################## fine tune\n",
        "    # for layer in base_model.layers[:-5]:\n",
        "    #   layer.trainable = False\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = Sequential ()\n",
        "\n",
        "    model.add (base_model)\n",
        "    #flatten\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add (Dense (482, activation = 'relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add (Dense (4019, activation = 'relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(332, activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "    #model.add(Dense(256, activation='relu'))\n",
        "    #model.add(Dropout(0.7))\n",
        "    ##################################\n",
        "    #################################  EFFECT OF Hidden NEURONS  ###################\n",
        "    #model.add(Dense(3544, activation='relu'))\n",
        "    model.add(Dense(n_class-1, activation='sigmoid'))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "VLgBVsG6x-1u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "\n",
        "def cnn_feature_extractor(n_filters=32, n_class=2):\n",
        "\n",
        "\n",
        "    model = Sequential ()\n",
        "    input_img = Input((60, 256, 1))\n",
        "    model.add (Conv2D (n_filters, kernel_size = 3, activation='relu',input_shape=input_img.shape[1:], padding='same'))\n",
        "    model.add (MaxPool2D (2, 2))\n",
        "    model.add (BatchNormalization ())\n",
        "\n",
        "    model.add (Conv2D (n_filters * 2, kernel_size = 3, activation='relu', padding='same'))\n",
        "    model.add (MaxPool2D (2, 2))\n",
        "    model.add (BatchNormalization ())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(32000, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(n_class-1, activation='sigmoid'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "HCcG34mVEBlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow (x_train [270,:,:,0], cmap = 'jet')"
      ],
      "metadata": {
        "id": "a6IyC3bi6jmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary ()"
      ],
      "metadata": {
        "id": "EjzT_Y1dBpho",
        "outputId": "9919a74a-fee5-4b5d-e147-8f39ec8b4b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 60, 256, 32)       320       \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 30, 128, 32)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 30, 128, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 30, 128, 64)       18496     \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 15, 64, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 15, 64, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 61440)             0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 20)                1228820   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1248041 (4.76 MB)\n",
            "Trainable params: 1247849 (4.76 MB)\n",
            "Non-trainable params: 192 (768.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_test.shape"
      ],
      "metadata": {
        "id": "ybPfE9BuGtKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFAFCCYCRDl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn_feature_extractor ()"
      ],
      "metadata": {
        "id": "EpdJuAQCRDpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e12b0cf1-ea70-45d0-fdcb-d951c29b36a3",
        "id": "hbR00EyOvGY9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------          \n",
            " \t\t\t 1th fold \n",
            "---------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d72bc3019a4d>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0minput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-9e13914da580>\u001b[0m in \u001b[0;36mcnn_feature_extractor\u001b[0;34m(input_img, n_filters, n_class)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# # Creating model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     base_model = tf.keras.applications.resnet.ResNet101(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mResNet101\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     return ResNet(\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0mstack_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pool1_pool\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreact\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mstack_fn\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mstack1\u001b[0;34m(x, filters, blocks, stride1, name)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_block1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         x = block1(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_shortcut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_block\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mblock1\u001b[0;34m(x, filters, kernel_size, stride, conv_shortcut, name)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_1_relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     x = layers.Conv2D(\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_2_conv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     )(x)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_fold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m             return tf.random.stateless_uniform(\n\u001b[0m\u001b[1;32m   2101\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                 \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAAGyCAYAAAB3OsSEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbUklEQVR4nO3df0zd1f3H8RfQcqmx0DrGhbKrrHX+rJYKltHaGJc7STS4/rHIrCmM+GMqM9qbzRbbgrVaOqcNiUWJVad/6Kgz1hhLcMpsjMrSSEuis62pVGHGS8tcuR1VaLnn+4fx+sVC7ecWuNj385HcPziez/2ce4I+/VzujyTnnBMAAEYlJ3oBAAAkEiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmOY5hG+99ZZKS0s1a9YsJSUl6eWXX/7eY7Zv367LLrtMPp9P5557rp555pk4lgoAwNjzHML+/n7NmzdPDQ0NJzV///79uvbaa3XVVVepo6NDd999t26++Wa99tprnhcLAMBYSzqVD91OSkrS1q1btWTJklHnrFixQtu2bdMHH3wQG/vNb36jQ4cOqaWlJd5TAwAwJqaM9wna2toUDAaHjZWUlOjuu+8e9ZiBgQENDAzEfo5Go/riiy/0ox/9SElJSeO1VADAJOac0+HDhzVr1iwlJ4/dS1zGPYThcFh+v3/YmN/vVyQS0Zdffqlp06Ydd0xdXZ3Wrl073ksDAPwAdXd36yc/+cmY3d+4hzAe1dXVCoVCsZ/7+vp09tlnq7u7W+np6QlcGQAgUSKRiAKBgKZPnz6m9zvuIczOzlZPT8+wsZ6eHqWnp494NShJPp9PPp/vuPH09HRCCADGjfWfyMb9fYTFxcVqbW0dNvb666+ruLh4vE8NAMD38hzC//3vf+ro6FBHR4ekr98e0dHRoa6uLklfP61ZXl4em3/bbbeps7NT99xzj/bs2aPHHntML7zwgpYvXz42jwAAgFPgOYTvvfee5s+fr/nz50uSQqGQ5s+fr5qaGknS559/HouiJP30pz/Vtm3b9Prrr2vevHl65JFH9OSTT6qkpGSMHgIAAPE7pfcRTpRIJKKMjAz19fXxN0IAMGq8WsBnjQIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0+IKYUNDg/Ly8pSWlqaioiLt2LHjhPPr6+t1/vnna9q0aQoEAlq+fLm++uqruBYMAMBY8hzCLVu2KBQKqba2Vjt37tS8efNUUlKiAwcOjDj/+eef18qVK1VbW6vdu3frqaee0pYtW3Tvvfee8uIBADhVnkO4ceNG3XLLLaqsrNRFF12kxsZGnXHGGXr66adHnP/uu+9q0aJFWrp0qfLy8nT11Vfrhhtu+N6rSAAAJoKnEA4ODqq9vV3BYPDbO0hOVjAYVFtb24jHLFy4UO3t7bHwdXZ2qrm5Wddcc82o5xkYGFAkEhl2AwBgPEzxMrm3t1dDQ0Py+/3Dxv1+v/bs2TPiMUuXLlVvb6+uuOIKOed07Ngx3XbbbSd8arSurk5r1671sjQAAOIy7q8a3b59u9avX6/HHntMO3fu1EsvvaRt27Zp3bp1ox5TXV2tvr6+2K27u3u8lwkAMMrTFWFmZqZSUlLU09MzbLynp0fZ2dkjHrNmzRotW7ZMN998syTpkksuUX9/v2699VatWrVKycnHt9jn88nn83lZGgAAcfF0RZiamqqCggK1trbGxqLRqFpbW1VcXDziMUeOHDkudikpKZIk55zX9QIAMKY8XRFKUigUUkVFhQoLC7VgwQLV19erv79flZWVkqTy8nLl5uaqrq5OklRaWqqNGzdq/vz5Kioq0r59+7RmzRqVlpbGgggAQKJ4DmFZWZkOHjyompoahcNh5efnq6WlJfYCmq6urmFXgKtXr1ZSUpJWr16tzz77TD/+8Y9VWlqqBx98cOweBQAAcUpyP4DnJyORiDIyMtTX16f09PRELwcAkADj1QI+axQAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmBZXCBsaGpSXl6e0tDQVFRVpx44dJ5x/6NAhVVVVKScnRz6fT+edd56am5vjWjAAAGNpitcDtmzZolAopMbGRhUVFam+vl4lJSXau3evsrKyjps/ODioX/7yl8rKytKLL76o3Nxcffrpp5oxY8ZYrB8AgFOS5JxzXg4oKirS5Zdfrk2bNkmSotGoAoGA7rzzTq1cufK4+Y2Njfrzn/+sPXv2aOrUqXEtMhKJKCMjQ319fUpPT4/rPgAAP2zj1QJPT40ODg6qvb1dwWDw2ztITlYwGFRbW9uIx7zyyisqLi5WVVWV/H6/5s6dq/Xr12toaGjU8wwMDCgSiQy7AQAwHjyFsLe3V0NDQ/L7/cPG/X6/wuHwiMd0dnbqxRdf1NDQkJqbm7VmzRo98sgjeuCBB0Y9T11dnTIyMmK3QCDgZZkAAJy0cX/VaDQaVVZWlp544gkVFBSorKxMq1atUmNj46jHVFdXq6+vL3br7u4e72UCAIzy9GKZzMxMpaSkqKenZ9h4T0+PsrOzRzwmJydHU6dOVUpKSmzswgsvVDgc1uDgoFJTU487xufzyefzeVkaAABx8XRFmJqaqoKCArW2tsbGotGoWltbVVxcPOIxixYt0r59+xSNRmNjH330kXJyckaMIAAAE8nzU6OhUEibN2/Ws88+q927d+v2229Xf3+/KisrJUnl5eWqrq6Ozb/99tv1xRdf6K677tJHH32kbdu2af369aqqqhq7RwEAQJw8v4+wrKxMBw8eVE1NjcLhsPLz89XS0hJ7AU1XV5eSk7/tayAQ0Guvvably5fr0ksvVW5uru666y6tWLFi7B4FAABx8vw+wkTgfYQAgEnxPkIAAE43hBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGBaXCFsaGhQXl6e0tLSVFRUpB07dpzUcU1NTUpKStKSJUviOS0AAGPOcwi3bNmiUCik2tpa7dy5U/PmzVNJSYkOHDhwwuM++eQT/eEPf9DixYvjXiwAAGPNcwg3btyoW265RZWVlbrooovU2NioM844Q08//fSoxwwNDenGG2/U2rVrNXv27FNaMAAAY8lTCAcHB9Xe3q5gMPjtHSQnKxgMqq2tbdTj7r//fmVlZemmm246qfMMDAwoEokMuwEAMB48hbC3t1dDQ0Py+/3Dxv1+v8Lh8IjHvP3223rqqae0efPmkz5PXV2dMjIyYrdAIOBlmQAAnLRxfdXo4cOHtWzZMm3evFmZmZknfVx1dbX6+vpit+7u7nFcJQDAsileJmdmZiolJUU9PT3Dxnt6epSdnX3c/I8//liffPKJSktLY2PRaPTrE0+Zor1792rOnDnHHefz+eTz+bwsDQCAuHi6IkxNTVVBQYFaW1tjY9FoVK2trSouLj5u/gUXXKD3339fHR0dsdt1112nq666Sh0dHTzlCQBIOE9XhJIUCoVUUVGhwsJCLViwQPX19erv71dlZaUkqby8XLm5uaqrq1NaWprmzp077PgZM2ZI0nHjAAAkgucQlpWV6eDBg6qpqVE4HFZ+fr5aWlpiL6Dp6upScjIfWAMA+GFIcs65RC/i+0QiEWVkZKivr0/p6emJXg4AIAHGqwVcugEATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMA0QggAMI0QAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0+IKYUNDg/Ly8pSWlqaioiLt2LFj1LmbN2/W4sWLNXPmTM2cOVPBYPCE8wEAmEieQ7hlyxaFQiHV1tZq586dmjdvnkpKSnTgwIER52/fvl033HCD3nzzTbW1tSkQCOjqq6/WZ599dsqLBwDgVCU555yXA4qKinT55Zdr06ZNkqRoNKpAIKA777xTK1eu/N7jh4aGNHPmTG3atEnl5eUndc5IJKKMjAz19fUpPT3dy3IBAKeJ8WqBpyvCwcFBtbe3KxgMfnsHyckKBoNqa2s7qfs4cuSIjh49qrPOOmvUOQMDA4pEIsNuAACMB08h7O3t1dDQkPx+/7Bxv9+vcDh8UvexYsUKzZo1a1hMv6uurk4ZGRmxWyAQ8LJMAABO2oS+anTDhg1qamrS1q1blZaWNuq86upq9fX1xW7d3d0TuEoAgCVTvEzOzMxUSkqKenp6ho339PQoOzv7hMc+/PDD2rBhg9544w1deumlJ5zr8/nk8/m8LA0AgLh4uiJMTU1VQUGBWltbY2PRaFStra0qLi4e9biHHnpI69atU0tLiwoLC+NfLQAAY8zTFaEkhUIhVVRUqLCwUAsWLFB9fb36+/tVWVkpSSovL1dubq7q6uokSX/6059UU1Oj559/Xnl5ebG/JZ555pk688wzx/ChAADgnecQlpWV6eDBg6qpqVE4HFZ+fr5aWlpiL6Dp6upScvK3F5qPP/64BgcH9etf/3rY/dTW1uq+++47tdUDAHCKPL+PMBF4HyEAYFK8jxAAgNMNIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJhGCAEAphFCAIBphBAAYBohBACYRggBAKYRQgCAaYQQAGAaIQQAmEYIAQCmEUIAgGmEEABgGiEEAJgWVwgbGhqUl5entLQ0FRUVaceOHSec/7e//U0XXHCB0tLSdMkll6i5uTmuxQIAMNY8h3DLli0KhUKqra3Vzp07NW/ePJWUlOjAgQMjzn/33Xd1ww036KabbtKuXbu0ZMkSLVmyRB988MEpLx4AgFOV5JxzXg4oKirS5Zdfrk2bNkmSotGoAoGA7rzzTq1cufK4+WVlZerv79err74aG/v5z3+u/Px8NTY2ntQ5I5GIMjIy1NfXp/T0dC/LBQCcJsarBVO8TB4cHFR7e7uqq6tjY8nJyQoGg2praxvxmLa2NoVCoWFjJSUlevnll0c9z8DAgAYGBmI/9/X1Sfp6EwAANn3TAI/Xb9/LUwh7e3s1NDQkv98/bNzv92vPnj0jHhMOh0ecHw6HRz1PXV2d1q5de9x4IBDwslwAwGnoP//5jzIyMsbs/jyFcKJUV1cPu4o8dOiQzjnnHHV1dY3pgz+dRSIRBQIBdXd383SyB+ybd+xZfNg37/r6+nT22WfrrLPOGtP79RTCzMxMpaSkqKenZ9h4T0+PsrOzRzwmOzvb03xJ8vl88vl8x41nZGTwC+NReno6exYH9s079iw+7Jt3yclj+84/T/eWmpqqgoICtba2xsai0ahaW1tVXFw84jHFxcXD5kvS66+/Pup8AAAmkuenRkOhkCoqKlRYWKgFCxaovr5e/f39qqyslCSVl5crNzdXdXV1kqS77rpLV155pR555BFde+21ampq0nvvvacnnnhibB8JAABx8BzCsrIyHTx4UDU1NQqHw8rPz1dLS0vsBTFdXV3DLlsXLlyo559/XqtXr9a9996rn/3sZ3r55Zc1d+7ckz6nz+dTbW3tiE+XYmTsWXzYN+/Ys/iwb96N1555fh8hAACnEz5rFABgGiEEAJhGCAEAphFCAIBpkyaEfLWTd172bPPmzVq8eLFmzpypmTNnKhgMfu8en668/q59o6mpSUlJSVqyZMn4LnAS8rpnhw4dUlVVlXJycuTz+XTeeefx7+hJ7Ft9fb3OP/98TZs2TYFAQMuXL9dXX301QatNvLfeekulpaWaNWuWkpKSTviZ1N/Yvn27LrvsMvl8Pp177rl65plnvJ/YTQJNTU0uNTXVPf300+5f//qXu+WWW9yMGTNcT0/PiPPfeecdl5KS4h566CH34YcfutWrV7upU6e6999/f4JXnjhe92zp0qWuoaHB7dq1y+3evdv99re/dRkZGe7f//73BK88sbzu2zf279/vcnNz3eLFi92vfvWriVnsJOF1zwYGBlxhYaG75ppr3Ntvv+3279/vtm/f7jo6OiZ45Ynldd+ee+455/P53HPPPef279/vXnvtNZeTk+OWL18+wStPnObmZrdq1Sr30ksvOUlu69atJ5zf2dnpzjjjDBcKhdyHH37oHn30UZeSkuJaWlo8nXdShHDBggWuqqoq9vPQ0JCbNWuWq6urG3H+9ddf76699tphY0VFRe53v/vduK5zMvG6Z9917NgxN336dPfss8+O1xInpXj27dixY27hwoXuySefdBUVFeZC6HXPHn/8cTd79mw3ODg4UUuclLzuW1VVlfvFL34xbCwUCrlFixaN6zonq5MJ4T333OMuvvjiYWNlZWWupKTE07kS/tToN1/tFAwGY2Mn89VO/3++9PVXO402/3QTz55915EjR3T06NEx//DaySzefbv//vuVlZWlm266aSKWOanEs2evvPKKiouLVVVVJb/fr7lz52r9+vUaGhqaqGUnXDz7tnDhQrW3t8eePu3s7FRzc7OuueaaCVnzD9FYtSDh3z4xUV/tdDqJZ8++a8WKFZo1a9Zxv0Sns3j27e2339ZTTz2ljo6OCVjh5BPPnnV2duof//iHbrzxRjU3N2vfvn264447dPToUdXW1k7EshMunn1bunSpent7dcUVV8g5p2PHjum2227TvffeOxFL/kEarQWRSERffvmlpk2bdlL3k/ArQky8DRs2qKmpSVu3blVaWlqilzNpHT58WMuWLdPmzZuVmZmZ6OX8YESjUWVlZemJJ55QQUGBysrKtGrVKjU2NiZ6aZPa9u3btX79ej322GPauXOnXnrpJW3btk3r1q1L9NJOewm/Ipyor3Y6ncSzZ994+OGHtWHDBr3xxhu69NJLx3OZk47Xffv444/1ySefqLS0NDYWjUYlSVOmTNHevXs1Z86c8V10gsXzu5aTk6OpU6cqJSUlNnbhhRcqHA5rcHBQqamp47rmySCefVuzZo2WLVumm2++WZJ0ySWXqL+/X7feeqtWrVo15l89dDoYrQXp6eknfTUoTYIrQr7aybt49kySHnroIa1bt04tLS0qLCyciKVOKl737YILLtD777+vjo6O2O26667TVVddpY6ODgUCgYlcfkLE87u2aNEi7du3L/Y/DZL00UcfKScnx0QEpfj27ciRI8fF7pv/mXB8JPSIxqwF3l7HMz6ampqcz+dzzzzzjPvwww/drbfe6mbMmOHC4bBzzrlly5a5lStXxua/8847bsqUKe7hhx92u3fvdrW1tSbfPuFlzzZs2OBSU1Pdiy++6D7//PPY7fDhw4l6CAnhdd++y+KrRr3uWVdXl5s+fbr7/e9/7/bu3eteffVVl5WV5R544IFEPYSE8LpvtbW1bvr06e6vf/2r6+zsdH//+9/dnDlz3PXXX5+ohzDhDh8+7Hbt2uV27drlJLmNGze6Xbt2uU8//dQ559zKlSvdsmXLYvO/efvEH//4R7d7927X0NDww337hHPOPfroo+7ss892qampbsGCBe6f//xn7J9deeWVrqKiYtj8F154wZ133nkuNTXVXXzxxW7btm0TvOLE87Jn55xzjpN03K22tnbiF55gXn/X/j+LIXTO+569++67rqioyPl8Pjd79mz34IMPumPHjk3wqhPPy74dPXrU3XfffW7OnDkuLS3NBQIBd8cdd7j//ve/E7/wBHnzzTdH/O/UN/tUUVHhrrzyyuOOyc/Pd6mpqW727NnuL3/5i+fz8jVMAADTEv43QgAAEokQAgBMI4QAANMIIQDANEIIADCNEAIATCOEAADTCCEAwDRCCAAwjRACAEwjhAAA0wghAMC0/wPPG/tz4omkYgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "images_train = pickle.load(open(\"/content/\" + \"train_thickness_retina.pkl\", 'rb'))\n",
        "labels_train = pickle.load(open(\"/content/\" +\"train_labels.pkl\", 'rb'))\n",
        "\n",
        "images_test = pickle.load(open(\"/content/drive/MyDrive/\" +\"test_thickness_new.pkl\", 'rb'))\n",
        "labels_test = pickle.load(open(\"/content/\" +\"test_labels.pkl\", 'rb'))\n",
        "images_test,labels_test = preparing(images_test,labels_test)\n",
        "\n",
        "images_test = np.repeat (images_test, repeats = 3, axis = 3)\n",
        "#####################################################################\n",
        "## Parameters\n",
        "#####################################################################\n",
        "channel = 3\n",
        "number_class = 2\n",
        "\n",
        "cnn_acc    = []\n",
        "cnn_se     = []\n",
        "cnn_sp     = []\n",
        "cnn_pr     = []\n",
        "cnn_f1     = []\n",
        "cnn_auc    = []\n",
        "cnn_pr_auc = []\n",
        "\n",
        "test_acc    = []\n",
        "test_se     = []\n",
        "test_sp     = []\n",
        "test_pr     = []\n",
        "test_f1     = []\n",
        "test_auc    = []\n",
        "test_pr_auc = []\n",
        "\n",
        "\n",
        "class_acc = np.zeros((number_class))\n",
        "class_acc_test = np.zeros((number_class))\n",
        "\n",
        "target_names = ['Normal' , 'MS']\n",
        "confusion_matrix = np.zeros((number_class, number_class))\n",
        "confusion_matrix_test = np.zeros((number_class, number_class))\n",
        "\n",
        "y_test = []\n",
        "tprs   = []\n",
        "aucs   = []\n",
        "y_pred = []\n",
        "x_test = {}\n",
        "\n",
        "mean_fpr  = np.linspace(0, 1, 100)\n",
        "fig, ax   = plt.subplots(figsize=(5, 5))\n",
        "fig1, ax1 = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "\n",
        "#### model parameters for vgg\n",
        "batch_size_vgg    = 8\n",
        "epoch         = 100\n",
        "filter_num    = 32\n",
        "learning_rate_vgg =  0.0004036650839061106\n",
        "\n",
        "\n",
        "\n",
        "#### model parameters for res\n",
        "batch_size_res    = 64\n",
        "epoch         = 100\n",
        "filter_num    = 32\n",
        "learning_rate_res =  0.0003720165816110892\n",
        "\n",
        "\n",
        "#### model parameters for cnn\n",
        "batch_size_cnn    = 8\n",
        "epoch         = 100\n",
        "filter_num    = 32\n",
        "learning_rate_cnn =  0.0005522789895942119\n",
        "\n",
        "#####################################################################\n",
        "## Applying kfold\n",
        "#####################################################################\n",
        "\n",
        "nfold = 5  #please enter number of folds\n",
        "\n",
        "kf_nfold = StratifiedKFold(n_splits=nfold, random_state=None, shuffle=True)\n",
        "\n",
        "n = 0\n",
        "for train_index, val_index in kf_nfold.split(images_train,list(labels_train.values())):\n",
        "    n = n+1\n",
        "    # print(train_index, val_index)  # you can watch train and validation index using this comment\n",
        "    print(f'---------------------------------------------------------------------\\\n",
        "          \\n \\t\\t\\t {n}th fold \\n---------------------------------------------------------------------'\\\n",
        "          ,end = '\\n\\n\\n' )\n",
        "    train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "    x_train = {i: images_train[list(images_train.keys())[i]]  for i in train_index}\n",
        "\n",
        "    x_valid = {i: images_train[list(images_train.keys())[i]]  for i in val_index}\n",
        "\n",
        "    y_train = {i: labels_train[list(labels_train.keys())[i]] for i in train_index}\n",
        "\n",
        "    y_valid = {i: labels_train[list(labels_train.keys())[i]] for i in val_index}\n",
        "\n",
        "    x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "    x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "    #x_test[n] = x_valid\n",
        "    ################# Augmentation\n",
        "\n",
        "    indices = np.where(y_train == 1)[0]\n",
        "\n",
        "    x_train_ms = x_train[indices]\n",
        "    y_train_ms = y_train[indices]\n",
        "\n",
        "    x_train_ms_aug,y_train_ms_aug = Augmentation_1(x_train_ms,y_train_ms)\n",
        "\n",
        "    x_train = np.concatenate ((x_train, x_train_ms_aug), axis=0)\n",
        "    y_train = np.concatenate ((y_train, y_train_ms_aug), axis = 0)\n",
        "\n",
        "    indices = np.random.permutation (len (x_train))\n",
        "\n",
        "    x_train_shuf = x_train [indices]\n",
        "    y_train_shuf = y_train [indices]\n",
        "\n",
        "    x_train,y_train = Augmentation_2(x_train_shuf,y_train_shuf)\n",
        "\n",
        "    x_train = np.repeat (x_train, repeats = 3, axis = 3)\n",
        "\n",
        "    x_valid = np.repeat (x_valid, repeats = 3, axis = 3)\n",
        "\n",
        "    ####################################################################\n",
        "    # classification\n",
        "    ####################################################################\n",
        "\n",
        "    input_img = Input((np.shape(x_train)[1], np.shape(x_train)[2], channel))\n",
        "\n",
        "    model = cnn_feature_extractor(input_img)\n",
        "\n",
        "\n",
        "    METRICS = [\n",
        "#      keras.metrics.TruePositives(name='tp'),\n",
        "#      keras.metrics.FalsePositives(name='fp'),\n",
        "#      keras.metrics.TrueNegatives(name='tn'),\n",
        "#      keras.metrics.FalseNegatives(name='fn'),\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "#      keras.metrics.Precision(name='precision'),\n",
        "#      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "#      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "      ]\n",
        "\n",
        "\n",
        "    my_optimizer =  tf.keras.optimizers.Adam(lr=learning_rate_res)\n",
        "    model.compile(optimizer=my_optimizer, loss=\"binary_crossentropy\", metrics=METRICS)\n",
        "    callbacks = [EarlyStopping(patience=20, verbose=1),\n",
        "        ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "        ModelCheckpoint(f'oct{n}.h5', verbose=1, save_best_only=True, save_weights_only=True)]\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    #################################\n",
        "    ###### Applying model  ###########\n",
        "    #################################\n",
        "    results = model.fit(x_train, y_train, batch_size=batch_size_res, epochs=epoch, callbacks=callbacks,\\\n",
        "                    validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.title(f\"Learning curve {n}th fold\")\n",
        "    plt.plot(results.history[\"loss\"][:-7], label=\"loss\")\n",
        "    plt.plot(results.history[\"val_loss\"][:-7], label=\"val_loss\")\n",
        "    plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"log_loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.title(f\"Learning curve {n}th fold\")\n",
        "    plt.plot(results.history[\"accuracy\"], label=\"accuracy\")\n",
        "    plt.plot(results.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.plot( np.argmax(results.history[\"val_accuracy\"]), np.max(results.history[\"val_accuracy\"]),\\\n",
        "             marker=\"x\", color=\"r\", label=\"best accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    # load the best model\n",
        "    model.load_weights(f'oct{n}.h5')\n",
        "\n",
        "\n",
        "    pred_proba = model.predict(x_valid).ravel()\n",
        "    pred_class = (pred_proba > 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "    ##### calculating metrics\n",
        "\n",
        "    cnn_acc.append(metrics.accuracy_score(y_valid, pred_class))\n",
        "    print(f'accuracy of {n}th fold : {metrics.accuracy_score(y_valid, pred_class)}')\n",
        "    SP, SE, PR, f1, ROC_AUC, P_R_AUC, Class_acc, cm = metrics_calculation(y_valid, pred_class, pred_proba)\n",
        "\n",
        "    cnn_sp.append(SP)\n",
        "    cnn_se.append(SE)\n",
        "    cnn_pr.append(PR)\n",
        "    cnn_f1.append(f1)\n",
        "    cnn_auc.append(ROC_AUC)\n",
        "    cnn_pr_auc.append(P_R_AUC)\n",
        "\n",
        "    #################### acc for each class ##################\n",
        "    class_acc  = np.add(class_acc,Class_acc)\n",
        "\n",
        "    ###################### Total confusion_matrix for poly kernel ############\n",
        "    confusion_matrix = np.add(confusion_matrix,cm)\n",
        "\n",
        "######################## internal test\n",
        "    pred_proba_test = model.predict(images_test).ravel()\n",
        "    pred_class_test = (pred_proba_test > 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "    ##### calculating metrics\n",
        "    print(f'test accuracy of {n}th fold : {metrics.accuracy_score(labels_test, pred_class_test)}')\n",
        "    SP, SE, PR, f1, ROC_AUC, P_R_AUC, Class_acc, cm = metrics_calculation(labels_test, pred_class_test, pred_proba_test)\n",
        "\n",
        "    test_acc.append(metrics.accuracy_score(labels_test, pred_class_test))\n",
        "    test_sp.append(SP)\n",
        "    test_se.append(SE)\n",
        "    test_pr.append(PR)\n",
        "    test_f1.append(f1)\n",
        "    test_auc.append(ROC_AUC)\n",
        "    test_pr_auc.append(P_R_AUC)\n",
        "\n",
        "    #################### acc for each class ##################\n",
        "    class_acc_test  = np.add(class_acc_test,Class_acc)\n",
        "\n",
        "    ###################### Total confusion_matrix for poly kernel ############\n",
        "    confusion_matrix_test = np.add(confusion_matrix_test,cm)\n",
        "\n",
        "    ###################### Ploting ROC and PR curves for each fold ############\n",
        "    y_test = np.append(y_test, y_valid, axis = 0)\n",
        "    y_pred = np.append(y_pred, pred_proba, axis = 0)\n",
        "    ###\n",
        "    tprs, aucs = fold_curves(ax, ax1, y_valid, n, mean_fpr, pred_proba, tprs, aucs)\n",
        "\n",
        "######################  the mean Ploting ROC and PR curves ############\n",
        "###\n",
        "curve_ploting(ax, ax1, mean_fpr, aucs, tprs, y_test, y_pred, 'CNN' )\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#######################################\n",
        "    # ploting confusion matrix\n",
        "#######################################\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix/nfold, display_labels=target_names)\n",
        "disp.plot()\n",
        "\n",
        "\n",
        "########################################\n",
        "#     Metrics printing\n",
        "########################################\n",
        "cnn_accc     = np.mean(cnn_acc)\n",
        "cnn_spp      = np.mean(cnn_sp)\n",
        "cnn_see      = np.mean(cnn_se)\n",
        "cnn_prr      = np.mean(cnn_pr)\n",
        "cnn_f11      = np.mean(cnn_f1)\n",
        "cnn_aucc     = np.mean(cnn_auc)\n",
        "cnn_pr_aucc  = np.mean(cnn_pr_auc)\n",
        "\n",
        "###################### internal test\n",
        "test_accc     = np.mean(test_acc)\n",
        "test_spp      = np.mean(test_sp)\n",
        "test_see      = np.mean(test_se)\n",
        "test_prr      = np.mean(test_pr)\n",
        "test_f11      = np.mean(test_f1)\n",
        "test_aucc     = np.mean(test_auc)\n",
        "test_pr_aucc  = np.mean(test_pr_auc)\n",
        "\n",
        "#################### acc for each class ##################\n",
        "class_acc  = class_acc/nfold\n",
        "class_acc_test  = class_acc_test/nfold\n",
        "\n",
        "print('cnn_acc     = %f' % cnn_accc)\n",
        "print('cnn_sp      = %f' % cnn_spp)\n",
        "print('cnn_se      = %f' % cnn_see)\n",
        "print('cnn_pr      = %f' % cnn_prr)\n",
        "print('cnn_f1      = %f' % cnn_f11)\n",
        "print('cnn_auc     = %f' % cnn_aucc)\n",
        "print('cnn_pr_auc  = %f' % cnn_pr_aucc, end='\\n\\n')\n",
        "\n",
        "\n",
        "print('acc of class %s' % target_names[0], '= %f' % class_acc[0])\n",
        "print('acc of class %s' % target_names[1], '= %f' % class_acc[1], end='\\n\\n')\n",
        "\n",
        "print('test_acc     = %f' % test_accc)\n",
        "print('test_sp      = %f' % test_spp)\n",
        "print('test_se      = %f' % test_see)\n",
        "print('test_pr      = %f' % test_prr)\n",
        "print('test_f1      = %f' % test_f11)\n",
        "print('test_auc     = %f' % test_aucc)\n",
        "print('test_pr_auc  = %f' % test_pr_aucc, end='\\n\\n')\n",
        "\n",
        "\n",
        "print('test acc of class %s' % target_names[0], '= %f' % class_acc_test[0])\n",
        "print('test acc of class %s' % target_names[1], '= %f' % class_acc_test[1], end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7MmYLbwI2Nl",
        "outputId": "f6f9551e-c5f2-4777-b060-824117eb09a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6052631578947368,\n",
              " 0.6052631578947368,\n",
              " 0.6052631578947368,\n",
              " 0.6052631578947368,\n",
              " 0.6052631578947368]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_acc"
      ],
      "metadata": {
        "id": "Q17VngQML192",
        "outputId": "a4953ca7-677b-405d-e032-1d0b39539274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7741935483870968,\n",
              " 0.8064516129032258,\n",
              " 0.8064516129032258,\n",
              " 0.8064516129032258,\n",
              " 0.8064516129032258]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna State of the arts"
      ],
      "metadata": {
        "id": "wOtCIsqsBE8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_JgMB9avGY-"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from optuna.integration import TFKerasPruningCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################### state of the arts ##############################\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "\n",
        "def classifier (trial):\n",
        "\n",
        "  # Load the pre-trained VGG16 model without the top (fully connected) layers\n",
        "  base_model = tf.keras.applications.resnet.ResNet101(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(60,256, 3),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "  # Freeze the pre-trained layers so they are not trainable during training\n",
        "  #for layer in base_model.layers:\n",
        "    #layer.trainable = False\n",
        "\n",
        "  base_model.trainable = False\n",
        "\n",
        "  model = Sequential ()\n",
        "\n",
        "  model.add (base_model)\n",
        "\n",
        "  model.add (Flatten ())\n",
        "\n",
        "  #dropout_l0 = trial.suggest_float(\"dropout_l0\", 0, 0.7,step=0.1)\n",
        "\n",
        "  #model.add (Dropout (dropout_l0))\n",
        "\n",
        "\n",
        "  n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "\n",
        "\n",
        "  for i in range(n_layers):\n",
        "\n",
        "\n",
        "      n_units = trial.suggest_int(\"n_units_l{}\".format(i), 16, 4096, log = True)\n",
        "\n",
        "      model.add (Dense (n_units, activation = 'relu'))\n",
        "\n",
        "      dropout = trial.suggest_float(\"dropout_l{}\".format (i), 0, 0.7,step=0.1)\n",
        "\n",
        "      model.add (Dropout (rate = dropout))\n",
        "\n",
        "  model.add (Dense (1, activation = 'sigmoid'))\n",
        "\n",
        "  lr = trial.suggest_float ('lr', 1e-5, 1e-1, log = True)\n",
        "\n",
        "  my_optimizer = tf.keras.optimizers.Adam (learning_rate= lr)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=my_optimizer, loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "mpxIKQ-qvGY_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = 42)\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "            data.append(np.array(x[i][j] * 255))\n",
        "            label.append(y[i])\n",
        "            #img = resize (x[i][j], (224, 224, 1), mode = 'constant', preserve_range= True)\n",
        "            #data.append (img)\n",
        "            #label.append (y [i])\n",
        "\n",
        "\n",
        "    data = np.reshape(data , np.shape(data))\n",
        "    return data, np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def objective (trial):\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "  rotation_range= 5, # rotation\n",
        "  zoom_range = 0.1,\n",
        "  fill_mode='nearest',\n",
        "  data_format='channels_last',\n",
        "    )\n",
        "\n",
        "  datagen2 = ImageDataGenerator(\n",
        "  #rotation_range= 5, # rotation\n",
        "  width_shift_range= [-30, 30], # horizontal shift\n",
        "  #height_shift_range= [-5, 5] , # vertical shift\n",
        "  zoom_range= 0.2,\n",
        "  vertical_flip= True , # vertical flip\n",
        "  #brightness_range= [0.2, 1.5]\n",
        "    )\n",
        "\n",
        "\n",
        "  images_train = pickle.load(open(\"/content/\"+\"train_thickness_retina.pkl\", 'rb'))\n",
        "  labels_train = pickle.load(open(\"/content/\"+\"train_labels.pkl\", 'rb'))\n",
        "\n",
        "  train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "  x_train = {i: images_train[list(images_train.keys())[i]]  for i in train_index}\n",
        "\n",
        "  x_valid = {i: images_train[list(images_train.keys())[i]]  for i in val_index}\n",
        "\n",
        "  y_train = {i: labels_train[list(labels_train.keys())[i]] for i in train_index}\n",
        "\n",
        "  y_valid = {i: labels_train[list(labels_train.keys())[i]] for i in val_index}\n",
        "\n",
        "  x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "  x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "\n",
        "  #rotation_angle  = trial.suggest_float(\"rotation_angle\", 0, 40, step = 10)\n",
        "  #zoom_range = trial.suggest_float(\"zoom_range\", 0, 0.5, step = 0.1)\n",
        "\n",
        "  #color_jitter_brightness = trial.suggest_float (\"color_jitter_brightness\", 0.1, 0.8, step = 0.1)\n",
        "  #shift_horizontal = trial.suggest_uniform(\"shift_horizontal\", 0, 0.7)\n",
        "  #shift_vertical = trial.suggest_uniform(\"shift_vertical\", 0, 0.7)\n",
        "  #vertical_flip_prob = trial.suggest_float (\"vertical_flip_prob\", 0, 1, step = 0.1)\n",
        "\n",
        "  indices = np.where(y_train == 1)[0]\n",
        "\n",
        "  x_train_ms = x_train[indices]\n",
        "  y_train_ms = y_train[indices]\n",
        "  #################Augmentation##################################\n",
        "\n",
        "  # First Augmentation\n",
        "\n",
        "  x_train_ms_aug =np.zeros_like(x_train_ms, dtype=np.float32)\n",
        "\n",
        "  y_train_ms_aug =np.zeros_like(y_train_ms, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_ms)):\n",
        "\n",
        "    x1= x_train_ms[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_ms_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_ms_aug [i] = y_train[i]\n",
        "\n",
        "  x_train = np.concatenate ((x_train, x_train_ms_aug), axis=0)\n",
        "  y_train = np.concatenate ((y_train, y_train_ms_aug), axis = 0)\n",
        "\n",
        "  indices = np.random.permutation (len (x_train))\n",
        "\n",
        "  x_train_shuf = x_train [indices]\n",
        "  y_train_shuf = y_train [indices]\n",
        "\n",
        "  # Second Augmentation\n",
        "\n",
        "  x_train_aug =np.zeros_like(x_train_shuf, dtype=np.float32)\n",
        "\n",
        "  y_train_aug =np.zeros_like(y_train_shuf, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_aug)):\n",
        "\n",
        "    x1= x_train_aug[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen2.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_aug [i] = y_train[i]\n",
        "\n",
        "    x_train = np.concatenate ((x_train_shuf, x_train_aug), axis=0)\n",
        "    y_train = np.concatenate ((y_train_shuf, y_train_aug), axis = 0)\n",
        "\n",
        "\n",
        "  ###############################################################################33\n",
        "\n",
        "\n",
        "  x_train = np.repeat (x_train, repeats = 3, axis = 3)\n",
        "\n",
        "  x_valid = np.repeat (x_valid, repeats = 3, axis = 3)\n",
        "        ####################################################################\n",
        "        # classification via my model\n",
        "        ####################################################################\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  model = classifier (trial)\n",
        "\n",
        "\n",
        "  # Generate our trial model.\n",
        "\n",
        "\n",
        "  model.fit(x_train,\n",
        "            np.asarray(y_train, dtype=np.float64),\n",
        "            batch_size= batch_size,\n",
        "            epochs=50,\n",
        "            callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\"), EarlyStopping(patience=20, verbose=1),\n",
        "            ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "            ModelCheckpoint(f'slo.h5', verbose=1, save_best_only=True, save_weights_only=True)],\n",
        "            validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)),\n",
        "            )\n",
        "  model.load_weights(f'slo.h5')\n",
        "  score = model.evaluate (x_valid, np.asarray(y_valid, dtype=np.float64), verbose = 1)\n",
        "\n",
        "  return score [1]\n"
      ],
      "metadata": {
        "id": "KhGw92auvGY_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f314f13d-1bc2-41bc-e0da-8466a8c03d0a",
        "id": "JchBxE8tvGZA"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:36:18,475] A new study created in memory with name: no-name-45e90376-b286-408b-87d6-f163a37cfaf2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171446536/171446536 [==============================] - 1s 0us/step\n",
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.9670 - accuracy: 0.5789\n",
            "Epoch 1: val_loss improved from inf to 0.26099, saving model to slo.h5\n",
            "5/5 [==============================] - 21s 2s/step - loss: 0.9670 - accuracy: 0.5789 - val_loss: 0.2610 - val_accuracy: 0.9032 - lr: 3.7202e-04\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.7039\n",
            "Epoch 2: val_loss improved from 0.26099 to 0.23219, saving model to slo.h5\n",
            "5/5 [==============================] - 4s 960ms/step - loss: 0.5300 - accuracy: 0.7039 - val_loss: 0.2322 - val_accuracy: 0.9032 - lr: 3.7202e-04\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6115 - accuracy: 0.7368\n",
            "Epoch 3: val_loss improved from 0.23219 to 0.13890, saving model to slo.h5\n",
            "5/5 [==============================] - 2s 369ms/step - loss: 0.6115 - accuracy: 0.7368 - val_loss: 0.1389 - val_accuracy: 0.9677 - lr: 3.7202e-04\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4592 - accuracy: 0.7401\n",
            "Epoch 4: val_loss improved from 0.13890 to 0.12170, saving model to slo.h5\n",
            "5/5 [==============================] - 4s 1s/step - loss: 0.4592 - accuracy: 0.7401 - val_loss: 0.1217 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.7599\n",
            "Epoch 5: val_loss did not improve from 0.12170\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.4602 - accuracy: 0.7599 - val_loss: 0.3965 - val_accuracy: 0.8387 - lr: 3.7202e-04\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.7697\n",
            "Epoch 6: val_loss did not improve from 0.12170\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.3822 - accuracy: 0.7697 - val_loss: 0.1928 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3895 - accuracy: 0.7632\n",
            "Epoch 7: val_loss did not improve from 0.12170\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.3895 - accuracy: 0.7632 - val_loss: 0.2905 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3589 - accuracy: 0.7961\n",
            "Epoch 8: val_loss did not improve from 0.12170\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.3589 - accuracy: 0.7961 - val_loss: 0.3362 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.7961\n",
            "Epoch 9: val_loss improved from 0.12170 to 0.02954, saving model to slo.h5\n",
            "5/5 [==============================] - 2s 501ms/step - loss: 0.3531 - accuracy: 0.7961 - val_loss: 0.0295 - val_accuracy: 1.0000 - lr: 3.7202e-04\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3511 - accuracy: 0.7796\n",
            "Epoch 10: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.3511 - accuracy: 0.7796 - val_loss: 0.0432 - val_accuracy: 0.9677 - lr: 3.7202e-04\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.7928\n",
            "Epoch 11: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.3426 - accuracy: 0.7928 - val_loss: 0.2086 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.7928\n",
            "Epoch 12: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.3332 - accuracy: 0.7928 - val_loss: 0.3084 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3520 - accuracy: 0.7862\n",
            "Epoch 13: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.3520 - accuracy: 0.7862 - val_loss: 0.1917 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.8059\n",
            "Epoch 14: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.3365 - accuracy: 0.8059 - val_loss: 0.0355 - val_accuracy: 0.9677 - lr: 3.7202e-04\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8026\n",
            "Epoch 15: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.3445 - accuracy: 0.8026 - val_loss: 0.0333 - val_accuracy: 0.9677 - lr: 3.7202e-04\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.7928\n",
            "Epoch 16: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.3405 - accuracy: 0.7928 - val_loss: 0.0804 - val_accuracy: 0.9677 - lr: 3.7202e-04\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3458 - accuracy: 0.7829\n",
            "Epoch 17: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.3458 - accuracy: 0.7829 - val_loss: 0.1387 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.7895\n",
            "Epoch 18: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.3429 - accuracy: 0.7895 - val_loss: 0.1729 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3388 - accuracy: 0.7763\n",
            "Epoch 19: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.3388 - accuracy: 0.7763 - val_loss: 0.1946 - val_accuracy: 0.9355 - lr: 3.7202e-04\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.7928\n",
            "Epoch 20: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.3412 - accuracy: 0.7928 - val_loss: 0.1942 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.7961\n",
            "Epoch 21: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.3413 - accuracy: 0.7961 - val_loss: 0.1928 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.8026\n",
            "Epoch 22: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.3378 - accuracy: 0.8026 - val_loss: 0.1929 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.7895\n",
            "Epoch 23: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.3395 - accuracy: 0.7895 - val_loss: 0.1982 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.7961\n",
            "Epoch 24: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.3406 - accuracy: 0.7961 - val_loss: 0.2001 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.7961\n",
            "Epoch 25: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.3424 - accuracy: 0.7961 - val_loss: 0.2023 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.7993\n",
            "Epoch 26: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 123ms/step - loss: 0.3429 - accuracy: 0.7993 - val_loss: 0.2018 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.7928\n",
            "Epoch 27: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.3425 - accuracy: 0.7928 - val_loss: 0.2028 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.7961\n",
            "Epoch 28: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.3406 - accuracy: 0.7961 - val_loss: 0.2023 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.7961\n",
            "Epoch 29: val_loss did not improve from 0.02954\n",
            "5/5 [==============================] - 1s 123ms/step - loss: 0.3360 - accuracy: 0.7961 - val_loss: 0.2016 - val_accuracy: 0.9355 - lr: 3.7202e-05\n",
            "Epoch 29: early stopping\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0295 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:37:16,588] Trial 0 finished with value: 1.0 and parameters: {'batch_size': 64, 'n_layers': 3, 'n_units_l0': 482, 'dropout_l0': 0.1, 'n_units_l1': 4019, 'dropout_l1': 0.30000000000000004, 'n_units_l2': 332, 'dropout_l2': 0.6000000000000001, 'lr': 0.0003720165816110892}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8142 - accuracy: 0.6086\n",
            "Epoch 1: val_loss improved from inf to 0.25273, saving model to slo.h5\n",
            "3/3 [==============================] - 15s 3s/step - loss: 0.8142 - accuracy: 0.6086 - val_loss: 0.2527 - val_accuracy: 0.9032 - lr: 5.5578e-05\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.6974\n",
            "Epoch 2: val_loss did not improve from 0.25273\n",
            "3/3 [==============================] - 1s 174ms/step - loss: 0.5587 - accuracy: 0.6974 - val_loss: 0.3160 - val_accuracy: 0.8387 - lr: 5.5578e-05\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5367 - accuracy: 0.6908\n",
            "Epoch 3: val_loss improved from 0.25273 to 0.22795, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 630ms/step - loss: 0.5367 - accuracy: 0.6908 - val_loss: 0.2279 - val_accuracy: 0.9032 - lr: 5.5578e-05\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4587 - accuracy: 0.7368\n",
            "Epoch 4: val_loss improved from 0.22795 to 0.11485, saving model to slo.h5\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.4587 - accuracy: 0.7368 - val_loss: 0.1149 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4498 - accuracy: 0.7303\n",
            "Epoch 5: val_loss improved from 0.11485 to 0.10512, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 620ms/step - loss: 0.4498 - accuracy: 0.7303 - val_loss: 0.1051 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3967 - accuracy: 0.7829\n",
            "Epoch 6: val_loss did not improve from 0.10512\n",
            "3/3 [==============================] - 1s 184ms/step - loss: 0.3967 - accuracy: 0.7829 - val_loss: 0.1422 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.7599\n",
            "Epoch 7: val_loss did not improve from 0.10512\n",
            "3/3 [==============================] - 1s 182ms/step - loss: 0.4108 - accuracy: 0.7599 - val_loss: 0.1365 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.7697\n",
            "Epoch 8: val_loss improved from 0.10512 to 0.08750, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 1s/step - loss: 0.3773 - accuracy: 0.7697 - val_loss: 0.0875 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4072 - accuracy: 0.7368\n",
            "Epoch 9: val_loss improved from 0.08750 to 0.06727, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 833ms/step - loss: 0.4072 - accuracy: 0.7368 - val_loss: 0.0673 - val_accuracy: 0.9677 - lr: 5.5578e-05\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.7730\n",
            "Epoch 10: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.3810 - accuracy: 0.7730 - val_loss: 0.0723 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.7664\n",
            "Epoch 11: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 192ms/step - loss: 0.3823 - accuracy: 0.7664 - val_loss: 0.0744 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3550 - accuracy: 0.7961\n",
            "Epoch 12: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 181ms/step - loss: 0.3550 - accuracy: 0.7961 - val_loss: 0.0773 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3661 - accuracy: 0.7763\n",
            "Epoch 13: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 186ms/step - loss: 0.3661 - accuracy: 0.7763 - val_loss: 0.0920 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.7763\n",
            "Epoch 14: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 175ms/step - loss: 0.3808 - accuracy: 0.7763 - val_loss: 0.1033 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3693 - accuracy: 0.7566\n",
            "Epoch 15: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 179ms/step - loss: 0.3693 - accuracy: 0.7566 - val_loss: 0.1017 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.7796\n",
            "Epoch 16: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 184ms/step - loss: 0.3596 - accuracy: 0.7796 - val_loss: 0.0911 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3673 - accuracy: 0.7434\n",
            "Epoch 17: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 179ms/step - loss: 0.3673 - accuracy: 0.7434 - val_loss: 0.0764 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3500 - accuracy: 0.7763\n",
            "Epoch 18: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 177ms/step - loss: 0.3500 - accuracy: 0.7763 - val_loss: 0.0722 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3736 - accuracy: 0.7697\n",
            "Epoch 19: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 177ms/step - loss: 0.3736 - accuracy: 0.7697 - val_loss: 0.0863 - val_accuracy: 0.9355 - lr: 5.5578e-05\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.7796\n",
            "Epoch 20: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 179ms/step - loss: 0.3577 - accuracy: 0.7796 - val_loss: 0.0883 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3586 - accuracy: 0.7829\n",
            "Epoch 21: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 177ms/step - loss: 0.3586 - accuracy: 0.7829 - val_loss: 0.0922 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3595 - accuracy: 0.7599\n",
            "Epoch 22: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 180ms/step - loss: 0.3595 - accuracy: 0.7599 - val_loss: 0.0954 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.7993\n",
            "Epoch 23: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 185ms/step - loss: 0.3495 - accuracy: 0.7993 - val_loss: 0.0978 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.7862\n",
            "Epoch 24: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 177ms/step - loss: 0.3588 - accuracy: 0.7862 - val_loss: 0.0997 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.7829\n",
            "Epoch 25: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 183ms/step - loss: 0.3407 - accuracy: 0.7829 - val_loss: 0.1026 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.7862\n",
            "Epoch 26: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 183ms/step - loss: 0.3486 - accuracy: 0.7862 - val_loss: 0.1043 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.8026\n",
            "Epoch 27: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 188ms/step - loss: 0.3345 - accuracy: 0.8026 - val_loss: 0.1048 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3591 - accuracy: 0.7697\n",
            "Epoch 28: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 186ms/step - loss: 0.3591 - accuracy: 0.7697 - val_loss: 0.1040 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3399 - accuracy: 0.7895\n",
            "Epoch 29: val_loss did not improve from 0.06727\n",
            "3/3 [==============================] - 1s 183ms/step - loss: 0.3399 - accuracy: 0.7895 - val_loss: 0.1029 - val_accuracy: 0.9355 - lr: 5.5578e-06\n",
            "Epoch 29: early stopping\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0673 - accuracy: 0.9677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:38:11,405] Trial 1 finished with value: 0.9677419066429138 and parameters: {'batch_size': 128, 'n_layers': 1, 'n_units_l0': 378, 'dropout_l0': 0.6000000000000001, 'lr': 5.557823634687059e-05}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1825.1227 - accuracy: 0.5362\n",
            "Epoch 1: val_loss improved from inf to 4.52302, saving model to slo.h5\n",
            "5/5 [==============================] - 13s 1s/step - loss: 1825.1227 - accuracy: 0.5362 - val_loss: 4.5230 - val_accuracy: 0.7097 - lr: 0.0057\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.5818 - accuracy: 0.5954\n",
            "Epoch 2: val_loss improved from 4.52302 to 0.26080, saving model to slo.h5\n",
            "5/5 [==============================] - 5s 1s/step - loss: 4.5818 - accuracy: 0.5954 - val_loss: 0.2608 - val_accuracy: 0.9032 - lr: 0.0057\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.6275 - accuracy: 0.5789\n",
            "Epoch 3: val_loss did not improve from 0.26080\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 1.6275 - accuracy: 0.5789 - val_loss: 0.2828 - val_accuracy: 0.9032 - lr: 0.0057\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0017 - accuracy: 0.6118\n",
            "Epoch 4: val_loss did not improve from 0.26080\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 1.0017 - accuracy: 0.6118 - val_loss: 0.6696 - val_accuracy: 0.8387 - lr: 0.0057\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.7403 - accuracy: 0.6579\n",
            "Epoch 5: val_loss improved from 0.26080 to 0.22565, saving model to slo.h5\n",
            "5/5 [==============================] - 4s 959ms/step - loss: 0.7403 - accuracy: 0.6579 - val_loss: 0.2256 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.9862 - accuracy: 0.6842\n",
            "Epoch 6: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.9862 - accuracy: 0.6842 - val_loss: 0.4232 - val_accuracy: 0.9032 - lr: 0.0057\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6772 - accuracy: 0.7171\n",
            "Epoch 7: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6772 - accuracy: 0.7171 - val_loss: 0.2433 - val_accuracy: 0.9032 - lr: 0.0057\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.7179 - accuracy: 0.6053\n",
            "Epoch 8: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.7179 - accuracy: 0.6053 - val_loss: 0.7534 - val_accuracy: 0.8065 - lr: 0.0057\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.9114 - accuracy: 0.6743\n",
            "Epoch 9: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.9114 - accuracy: 0.6743 - val_loss: 0.3272 - val_accuracy: 0.8387 - lr: 0.0057\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0128 - accuracy: 0.7237\n",
            "Epoch 10: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 1.0128 - accuracy: 0.7237 - val_loss: 0.2923 - val_accuracy: 0.9355 - lr: 0.0057\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0245 - accuracy: 0.7237\n",
            "Epoch 11: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 1.0245 - accuracy: 0.7237 - val_loss: 0.3820 - val_accuracy: 0.8387 - lr: 0.0057\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.2541 - accuracy: 0.6809\n",
            "Epoch 12: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 1.2541 - accuracy: 0.6809 - val_loss: 0.3345 - val_accuracy: 0.9032 - lr: 0.0057\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6400 - accuracy: 0.7368\n",
            "Epoch 13: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.6400 - accuracy: 0.7368 - val_loss: 0.6154 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.7467\n",
            "Epoch 14: val_loss did not improve from 0.22565\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5933 - accuracy: 0.7467 - val_loss: 0.4496 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.5748 - accuracy: 0.6776\n",
            "Epoch 15: val_loss improved from 0.22565 to 0.19636, saving model to slo.h5\n",
            "5/5 [==============================] - 2s 576ms/step - loss: 1.5748 - accuracy: 0.6776 - val_loss: 0.1964 - val_accuracy: 0.9355 - lr: 0.0057\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6643 - accuracy: 0.7204\n",
            "Epoch 16: val_loss improved from 0.19636 to 0.15150, saving model to slo.h5\n",
            "5/5 [==============================] - 4s 972ms/step - loss: 0.6643 - accuracy: 0.7204 - val_loss: 0.1515 - val_accuracy: 0.9032 - lr: 0.0057\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6768 - accuracy: 0.7039\n",
            "Epoch 17: val_loss did not improve from 0.15150\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.6768 - accuracy: 0.7039 - val_loss: 0.1976 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5203 - accuracy: 0.7336\n",
            "Epoch 18: val_loss did not improve from 0.15150\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5203 - accuracy: 0.7336 - val_loss: 0.9853 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.7237\n",
            "Epoch 19: val_loss did not improve from 0.15150\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.4892 - accuracy: 0.7237 - val_loss: 1.6881 - val_accuracy: 0.8387 - lr: 0.0057\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6383 - accuracy: 0.7204\n",
            "Epoch 20: val_loss did not improve from 0.15150\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.6383 - accuracy: 0.7204 - val_loss: 0.1709 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5948 - accuracy: 0.7039\n",
            "Epoch 21: val_loss improved from 0.15150 to 0.12743, saving model to slo.h5\n",
            "5/5 [==============================] - 3s 683ms/step - loss: 0.5948 - accuracy: 0.7039 - val_loss: 0.1274 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.7336\n",
            "Epoch 22: val_loss did not improve from 0.12743\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.4470 - accuracy: 0.7336 - val_loss: 0.4274 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.7237\n",
            "Epoch 23: val_loss did not improve from 0.12743\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.4596 - accuracy: 0.7237 - val_loss: 0.1662 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.7237\n",
            "Epoch 24: val_loss did not improve from 0.12743\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5052 - accuracy: 0.7237 - val_loss: 0.2293 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6462 - accuracy: 0.7401\n",
            "Epoch 25: val_loss improved from 0.12743 to 0.12119, saving model to slo.h5\n",
            "5/5 [==============================] - 3s 627ms/step - loss: 0.6462 - accuracy: 0.7401 - val_loss: 0.1212 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6081 - accuracy: 0.7171\n",
            "Epoch 26: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.6081 - accuracy: 0.7171 - val_loss: 0.1468 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5015 - accuracy: 0.6842\n",
            "Epoch 27: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5015 - accuracy: 0.6842 - val_loss: 0.1307 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.7237\n",
            "Epoch 28: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.4558 - accuracy: 0.7237 - val_loss: 0.1477 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.7171\n",
            "Epoch 29: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5480 - accuracy: 0.7171 - val_loss: 0.1789 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.7105\n",
            "Epoch 30: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.4479 - accuracy: 0.7105 - val_loss: 0.2629 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.7072\n",
            "Epoch 31: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.4477 - accuracy: 0.7072 - val_loss: 0.3789 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4342 - accuracy: 0.7204\n",
            "Epoch 32: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.4342 - accuracy: 0.7204 - val_loss: 0.4756 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.7270\n",
            "Epoch 33: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.4192 - accuracy: 0.7270 - val_loss: 0.5515 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4449 - accuracy: 0.7039\n",
            "Epoch 34: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.4449 - accuracy: 0.7039 - val_loss: 0.6045 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4460 - accuracy: 0.7007\n",
            "Epoch 35: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.4460 - accuracy: 0.7007 - val_loss: 0.6334 - val_accuracy: 0.8710 - lr: 0.0057\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.7105\n",
            "Epoch 36: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.4379 - accuracy: 0.7105 - val_loss: 0.6352 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.7237\n",
            "Epoch 37: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.4264 - accuracy: 0.7237 - val_loss: 0.6401 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.7171\n",
            "Epoch 38: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.4356 - accuracy: 0.7171 - val_loss: 0.6425 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.7138\n",
            "Epoch 39: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.4245 - accuracy: 0.7138 - val_loss: 0.6437 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.7072\n",
            "Epoch 40: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.4416 - accuracy: 0.7072 - val_loss: 0.6448 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.7204\n",
            "Epoch 41: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5043 - accuracy: 0.7204 - val_loss: 0.6403 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4160 - accuracy: 0.7401\n",
            "Epoch 42: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.4160 - accuracy: 0.7401 - val_loss: 0.6354 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.7171\n",
            "Epoch 43: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.4229 - accuracy: 0.7171 - val_loss: 0.6326 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4371 - accuracy: 0.7039\n",
            "Epoch 44: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.4371 - accuracy: 0.7039 - val_loss: 0.6302 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4286 - accuracy: 0.7270\n",
            "Epoch 45: val_loss did not improve from 0.12119\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.4286 - accuracy: 0.7270 - val_loss: 0.6306 - val_accuracy: 0.8710 - lr: 5.7207e-04\n",
            "Epoch 45: early stopping\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1212 - accuracy: 0.8710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:39:19,576] Trial 2 finished with value: 0.8709677457809448 and parameters: {'batch_size': 64, 'n_layers': 5, 'n_units_l0': 951, 'dropout_l0': 0.4, 'n_units_l1': 3302, 'dropout_l1': 0.30000000000000004, 'n_units_l2': 1950, 'dropout_l2': 0.30000000000000004, 'n_units_l3': 1774, 'dropout_l3': 0.6000000000000001, 'n_units_l4': 1303, 'dropout_l4': 0.4, 'lr': 0.0057207430488320134}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8763 - accuracy: 0.6480\n",
            "Epoch 1: val_loss improved from inf to 0.55796, saving model to slo.h5\n",
            "19/19 [==============================] - 12s 287ms/step - loss: 0.8763 - accuracy: 0.6480 - val_loss: 0.5580 - val_accuracy: 0.8387 - lr: 6.3743e-05\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.4639 - accuracy: 0.7500\n",
            "Epoch 2: val_loss improved from 0.55796 to 0.13515, saving model to slo.h5\n",
            "19/19 [==============================] - 5s 259ms/step - loss: 0.4639 - accuracy: 0.7500 - val_loss: 0.1351 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.4146 - accuracy: 0.7270\n",
            "Epoch 3: val_loss improved from 0.13515 to 0.13282, saving model to slo.h5\n",
            "19/19 [==============================] - 2s 89ms/step - loss: 0.4146 - accuracy: 0.7270 - val_loss: 0.1328 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.7599\n",
            "Epoch 4: val_loss did not improve from 0.13282\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3687 - accuracy: 0.7599 - val_loss: 0.2130 - val_accuracy: 0.9032 - lr: 6.3743e-05\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.7862\n",
            "Epoch 5: val_loss did not improve from 0.13282\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.3531 - accuracy: 0.7862 - val_loss: 0.1509 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.7697\n",
            "Epoch 6: val_loss improved from 0.13282 to 0.11511, saving model to slo.h5\n",
            "19/19 [==============================] - 2s 112ms/step - loss: 0.3541 - accuracy: 0.7697 - val_loss: 0.1151 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3498 - accuracy: 0.7961\n",
            "Epoch 7: val_loss did not improve from 0.11511\n",
            "19/19 [==============================] - 1s 44ms/step - loss: 0.3498 - accuracy: 0.7961 - val_loss: 0.1155 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.7928\n",
            "Epoch 8: val_loss did not improve from 0.11511\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3461 - accuracy: 0.7928 - val_loss: 0.1433 - val_accuracy: 0.9677 - lr: 6.3743e-05\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8059\n",
            "Epoch 9: val_loss improved from 0.11511 to 0.10532, saving model to slo.h5\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 0.3443 - accuracy: 0.8059 - val_loss: 0.1053 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.7763\n",
            "Epoch 10: val_loss improved from 0.10532 to 0.09780, saving model to slo.h5\n",
            "19/19 [==============================] - 2s 100ms/step - loss: 0.3509 - accuracy: 0.7763 - val_loss: 0.0978 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.7961\n",
            "Epoch 11: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 2s 91ms/step - loss: 0.3551 - accuracy: 0.7961 - val_loss: 0.1191 - val_accuracy: 0.9677 - lr: 6.3743e-05\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3399 - accuracy: 0.7796\n",
            "Epoch 12: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 49ms/step - loss: 0.3399 - accuracy: 0.7796 - val_loss: 0.1118 - val_accuracy: 0.9677 - lr: 6.3743e-05\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.7961\n",
            "Epoch 13: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 0.3450 - accuracy: 0.7961 - val_loss: 0.1000 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.7895\n",
            "Epoch 14: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.3375 - accuracy: 0.7895 - val_loss: 0.1050 - val_accuracy: 0.9677 - lr: 6.3743e-05\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.7928\n",
            "Epoch 15: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3437 - accuracy: 0.7928 - val_loss: 0.1005 - val_accuracy: 0.9677 - lr: 6.3743e-05\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.7829\n",
            "Epoch 16: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.3465 - accuracy: 0.7829 - val_loss: 0.1047 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.7895\n",
            "Epoch 17: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 50ms/step - loss: 0.3406 - accuracy: 0.7895 - val_loss: 0.1119 - val_accuracy: 0.9677 - lr: 6.3743e-05\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.7829\n",
            "Epoch 18: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 49ms/step - loss: 0.3448 - accuracy: 0.7829 - val_loss: 0.1028 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.7928\n",
            "Epoch 19: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 48ms/step - loss: 0.3409 - accuracy: 0.7928 - val_loss: 0.1020 - val_accuracy: 0.9677 - lr: 6.3743e-05\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.7928\n",
            "Epoch 20: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3429 - accuracy: 0.7928 - val_loss: 0.0982 - val_accuracy: 0.9355 - lr: 6.3743e-05\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.7895\n",
            "Epoch 21: val_loss did not improve from 0.09780\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3415 - accuracy: 0.7895 - val_loss: 0.0981 - val_accuracy: 0.9355 - lr: 6.3743e-06\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3399 - accuracy: 0.7928\n",
            "Epoch 22: val_loss improved from 0.09780 to 0.09736, saving model to slo.h5\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 0.3399 - accuracy: 0.7928 - val_loss: 0.0974 - val_accuracy: 0.9355 - lr: 6.3743e-06\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.7993\n",
            "Epoch 23: val_loss improved from 0.09736 to 0.09699, saving model to slo.h5\n",
            "19/19 [==============================] - 3s 162ms/step - loss: 0.3423 - accuracy: 0.7993 - val_loss: 0.0970 - val_accuracy: 0.9355 - lr: 6.3743e-06\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.7993\n",
            "Epoch 24: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 0.3382 - accuracy: 0.7993 - val_loss: 0.1042 - val_accuracy: 0.9677 - lr: 6.3743e-06\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3422 - accuracy: 0.7895\n",
            "Epoch 25: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 40ms/step - loss: 0.3422 - accuracy: 0.7895 - val_loss: 0.1108 - val_accuracy: 0.9677 - lr: 6.3743e-06\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.7928\n",
            "Epoch 26: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3441 - accuracy: 0.7928 - val_loss: 0.1091 - val_accuracy: 0.9677 - lr: 6.3743e-06\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.7928\n",
            "Epoch 27: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 0.3364 - accuracy: 0.7928 - val_loss: 0.1058 - val_accuracy: 0.9677 - lr: 6.3743e-06\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.7928\n",
            "Epoch 28: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.3384 - accuracy: 0.7928 - val_loss: 0.1064 - val_accuracy: 0.9677 - lr: 6.3743e-06\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3385 - accuracy: 0.7961\n",
            "Epoch 29: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.3385 - accuracy: 0.7961 - val_loss: 0.1079 - val_accuracy: 0.9677 - lr: 6.3743e-06\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.7895\n",
            "Epoch 30: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 50ms/step - loss: 0.3412 - accuracy: 0.7895 - val_loss: 0.1051 - val_accuracy: 0.9355 - lr: 6.3743e-06\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.7961\n",
            "Epoch 31: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.3410 - accuracy: 0.7961 - val_loss: 0.1026 - val_accuracy: 0.9355 - lr: 6.3743e-06\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.7993\n",
            "Epoch 32: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 43ms/step - loss: 0.3389 - accuracy: 0.7993 - val_loss: 0.1003 - val_accuracy: 0.9355 - lr: 6.3743e-06\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.7961\n",
            "Epoch 33: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 43ms/step - loss: 0.3371 - accuracy: 0.7961 - val_loss: 0.0988 - val_accuracy: 0.9355 - lr: 6.3743e-06\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.7961\n",
            "Epoch 34: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3395 - accuracy: 0.7961 - val_loss: 0.0985 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.7961\n",
            "Epoch 35: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 42ms/step - loss: 0.3377 - accuracy: 0.7961 - val_loss: 0.0984 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3363 - accuracy: 0.7961\n",
            "Epoch 36: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3363 - accuracy: 0.7961 - val_loss: 0.0983 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.7961\n",
            "Epoch 37: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 42ms/step - loss: 0.3437 - accuracy: 0.7961 - val_loss: 0.0983 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3380 - accuracy: 0.7928\n",
            "Epoch 38: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 42ms/step - loss: 0.3380 - accuracy: 0.7928 - val_loss: 0.0983 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.7928\n",
            "Epoch 39: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 42ms/step - loss: 0.3423 - accuracy: 0.7928 - val_loss: 0.0983 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.7928\n",
            "Epoch 40: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.3393 - accuracy: 0.7928 - val_loss: 0.0981 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.7928\n",
            "Epoch 41: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3405 - accuracy: 0.7928 - val_loss: 0.0981 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3366 - accuracy: 0.7961\n",
            "Epoch 42: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3366 - accuracy: 0.7961 - val_loss: 0.0983 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.7928\n",
            "Epoch 43: val_loss did not improve from 0.09699\n",
            "19/19 [==============================] - 1s 41ms/step - loss: 0.3405 - accuracy: 0.7928 - val_loss: 0.0980 - val_accuracy: 0.9355 - lr: 1.0000e-06\n",
            "Epoch 43: early stopping\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0970 - accuracy: 0.9355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:40:27,973] Trial 3 finished with value: 0.9354838728904724 and parameters: {'batch_size': 16, 'n_layers': 1, 'n_units_l0': 152, 'dropout_l0': 0.2, 'lr': 6.374302846199862e-05}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8397 - accuracy: 0.4704\n",
            "Epoch 1: val_loss improved from inf to 0.54296, saving model to slo.h5\n",
            "10/10 [==============================] - 15s 398ms/step - loss: 0.8397 - accuracy: 0.4704 - val_loss: 0.5430 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 2/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.7744 - accuracy: 0.5521\n",
            "Epoch 2: val_loss improved from 0.54296 to 0.53572, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.7700 - accuracy: 0.5559 - val_loss: 0.5357 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 3/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.7410 - accuracy: 0.5521\n",
            "Epoch 3: val_loss improved from 0.53572 to 0.52735, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.7446 - accuracy: 0.5395 - val_loss: 0.5274 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 4/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.7049 - accuracy: 0.5729\n",
            "Epoch 4: val_loss improved from 0.52735 to 0.52126, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 368ms/step - loss: 0.7155 - accuracy: 0.5592 - val_loss: 0.5213 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 5/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.7210 - accuracy: 0.5764\n",
            "Epoch 5: val_loss improved from 0.52126 to 0.51648, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.7293 - accuracy: 0.5724 - val_loss: 0.5165 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 6/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.7319 - accuracy: 0.5833\n",
            "Epoch 6: val_loss improved from 0.51648 to 0.50999, saving model to slo.h5\n",
            "10/10 [==============================] - 4s 416ms/step - loss: 0.7434 - accuracy: 0.5757 - val_loss: 0.5100 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 7/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6766 - accuracy: 0.5729\n",
            "Epoch 7: val_loss improved from 0.50999 to 0.50554, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.6720 - accuracy: 0.5789 - val_loss: 0.5055 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 8/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.7065 - accuracy: 0.5590\n",
            "Epoch 8: val_loss improved from 0.50554 to 0.49750, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 373ms/step - loss: 0.7078 - accuracy: 0.5559 - val_loss: 0.4975 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6622 - accuracy: 0.5724\n",
            "Epoch 9: val_loss improved from 0.49750 to 0.48386, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 210ms/step - loss: 0.6622 - accuracy: 0.5724 - val_loss: 0.4839 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 10/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6624 - accuracy: 0.5972\n",
            "Epoch 10: val_loss improved from 0.48386 to 0.47106, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 322ms/step - loss: 0.6546 - accuracy: 0.6053 - val_loss: 0.4711 - val_accuracy: 0.7097 - lr: 1.1241e-05\n",
            "Epoch 11/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6906 - accuracy: 0.5625\n",
            "Epoch 11: val_loss improved from 0.47106 to 0.46272, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.6963 - accuracy: 0.5526 - val_loss: 0.4627 - val_accuracy: 0.7419 - lr: 1.1241e-05\n",
            "Epoch 12/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6528 - accuracy: 0.6042\n",
            "Epoch 12: val_loss improved from 0.46272 to 0.45291, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 366ms/step - loss: 0.6575 - accuracy: 0.5954 - val_loss: 0.4529 - val_accuracy: 0.7742 - lr: 1.1241e-05\n",
            "Epoch 13/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6633 - accuracy: 0.5764\n",
            "Epoch 13: val_loss improved from 0.45291 to 0.44308, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.6785 - accuracy: 0.5757 - val_loss: 0.4431 - val_accuracy: 0.7742 - lr: 1.1241e-05\n",
            "Epoch 14/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6319 - accuracy: 0.6319\n",
            "Epoch 14: val_loss improved from 0.44308 to 0.43788, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 374ms/step - loss: 0.6247 - accuracy: 0.6447 - val_loss: 0.4379 - val_accuracy: 0.7742 - lr: 1.1241e-05\n",
            "Epoch 15/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6676 - accuracy: 0.5938\n",
            "Epoch 15: val_loss improved from 0.43788 to 0.43513, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.6689 - accuracy: 0.5822 - val_loss: 0.4351 - val_accuracy: 0.7742 - lr: 1.1241e-05\n",
            "Epoch 16/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5940 - accuracy: 0.6562\n",
            "Epoch 16: val_loss improved from 0.43513 to 0.42765, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 173ms/step - loss: 0.5976 - accuracy: 0.6480 - val_loss: 0.4277 - val_accuracy: 0.8065 - lr: 1.1241e-05\n",
            "Epoch 17/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6535 - accuracy: 0.5868\n",
            "Epoch 17: val_loss improved from 0.42765 to 0.42110, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 161ms/step - loss: 0.6567 - accuracy: 0.5789 - val_loss: 0.4211 - val_accuracy: 0.8065 - lr: 1.1241e-05\n",
            "Epoch 18/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6259 - accuracy: 0.6007\n",
            "Epoch 18: val_loss improved from 0.42110 to 0.41255, saving model to slo.h5\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 0.6246 - accuracy: 0.5954 - val_loss: 0.4126 - val_accuracy: 0.8065 - lr: 1.1241e-05\n",
            "Epoch 19/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6436 - accuracy: 0.5799\n",
            "Epoch 19: val_loss improved from 0.41255 to 0.40098, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 0.6430 - accuracy: 0.5789 - val_loss: 0.4010 - val_accuracy: 0.8065 - lr: 1.1241e-05\n",
            "Epoch 20/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6211 - accuracy: 0.6146\n",
            "Epoch 20: val_loss improved from 0.40098 to 0.39300, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 336ms/step - loss: 0.6174 - accuracy: 0.6118 - val_loss: 0.3930 - val_accuracy: 0.8065 - lr: 1.1241e-05\n",
            "Epoch 21/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6056 - accuracy: 0.6181\n",
            "Epoch 21: val_loss improved from 0.39300 to 0.38510, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 171ms/step - loss: 0.6029 - accuracy: 0.6217 - val_loss: 0.3851 - val_accuracy: 0.8065 - lr: 1.1241e-05\n",
            "Epoch 22/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6192 - accuracy: 0.6562\n",
            "Epoch 22: val_loss improved from 0.38510 to 0.37483, saving model to slo.h5\n",
            "10/10 [==============================] - 4s 454ms/step - loss: 0.6197 - accuracy: 0.6546 - val_loss: 0.3748 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 23/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6027 - accuracy: 0.6458\n",
            "Epoch 23: val_loss improved from 0.37483 to 0.36742, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.6039 - accuracy: 0.6480 - val_loss: 0.3674 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 24/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6093 - accuracy: 0.6111\n",
            "Epoch 24: val_loss improved from 0.36742 to 0.35981, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.6034 - accuracy: 0.6151 - val_loss: 0.3598 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 25/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5823 - accuracy: 0.6424\n",
            "Epoch 25: val_loss improved from 0.35981 to 0.35288, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 0.5852 - accuracy: 0.6382 - val_loss: 0.3529 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.6908\n",
            "Epoch 26: val_loss improved from 0.35288 to 0.34661, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 205ms/step - loss: 0.5862 - accuracy: 0.6908 - val_loss: 0.3466 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 27/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.6125 - accuracy: 0.6424\n",
            "Epoch 27: val_loss improved from 0.34661 to 0.33806, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 334ms/step - loss: 0.6054 - accuracy: 0.6414 - val_loss: 0.3381 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 28/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5443 - accuracy: 0.6667\n",
            "Epoch 28: val_loss improved from 0.33806 to 0.33121, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.5502 - accuracy: 0.6546 - val_loss: 0.3312 - val_accuracy: 0.8710 - lr: 1.1241e-05\n",
            "Epoch 29/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5767 - accuracy: 0.6562\n",
            "Epoch 29: val_loss improved from 0.33121 to 0.32682, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.5778 - accuracy: 0.6546 - val_loss: 0.3268 - val_accuracy: 0.8710 - lr: 1.1241e-05\n",
            "Epoch 30/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5670 - accuracy: 0.6771\n",
            "Epoch 30: val_loss improved from 0.32682 to 0.32184, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.5605 - accuracy: 0.6809 - val_loss: 0.3218 - val_accuracy: 0.8710 - lr: 1.1241e-05\n",
            "Epoch 31/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5847 - accuracy: 0.6458\n",
            "Epoch 31: val_loss improved from 0.32184 to 0.31689, saving model to slo.h5\n",
            "10/10 [==============================] - 5s 514ms/step - loss: 0.5767 - accuracy: 0.6480 - val_loss: 0.3169 - val_accuracy: 0.8710 - lr: 1.1241e-05\n",
            "Epoch 32/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5717 - accuracy: 0.6285\n",
            "Epoch 32: val_loss improved from 0.31689 to 0.31103, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 170ms/step - loss: 0.5631 - accuracy: 0.6382 - val_loss: 0.3110 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 33/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5530 - accuracy: 0.6632\n",
            "Epoch 33: val_loss improved from 0.31103 to 0.30457, saving model to slo.h5\n",
            "10/10 [==============================] - 4s 433ms/step - loss: 0.5545 - accuracy: 0.6678 - val_loss: 0.3046 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 34/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5427 - accuracy: 0.6771\n",
            "Epoch 34: val_loss improved from 0.30457 to 0.29838, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 172ms/step - loss: 0.5334 - accuracy: 0.6842 - val_loss: 0.2984 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 35/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5460 - accuracy: 0.7049\n",
            "Epoch 35: val_loss improved from 0.29838 to 0.29352, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 304ms/step - loss: 0.5418 - accuracy: 0.7039 - val_loss: 0.2935 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 36/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5356 - accuracy: 0.7049\n",
            "Epoch 36: val_loss improved from 0.29352 to 0.28662, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 0.5329 - accuracy: 0.7039 - val_loss: 0.2866 - val_accuracy: 0.8387 - lr: 1.1241e-05\n",
            "Epoch 37/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5377 - accuracy: 0.6840\n",
            "Epoch 37: val_loss improved from 0.28662 to 0.27976, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 318ms/step - loss: 0.5410 - accuracy: 0.6776 - val_loss: 0.2798 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 38/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5375 - accuracy: 0.6632\n",
            "Epoch 38: val_loss improved from 0.27976 to 0.27658, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.5366 - accuracy: 0.6678 - val_loss: 0.2766 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 39/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5376 - accuracy: 0.6806\n",
            "Epoch 39: val_loss improved from 0.27658 to 0.27312, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 375ms/step - loss: 0.5362 - accuracy: 0.6809 - val_loss: 0.2731 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 40/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5362 - accuracy: 0.6944\n",
            "Epoch 40: val_loss improved from 0.27312 to 0.26754, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 168ms/step - loss: 0.5339 - accuracy: 0.6974 - val_loss: 0.2675 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 41/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5365 - accuracy: 0.6979\n",
            "Epoch 41: val_loss improved from 0.26754 to 0.26512, saving model to slo.h5\n",
            "10/10 [==============================] - 4s 443ms/step - loss: 0.5446 - accuracy: 0.6941 - val_loss: 0.2651 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 42/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5283 - accuracy: 0.6875\n",
            "Epoch 42: val_loss improved from 0.26512 to 0.26305, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 167ms/step - loss: 0.5244 - accuracy: 0.6941 - val_loss: 0.2630 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 43/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5557 - accuracy: 0.6840\n",
            "Epoch 43: val_loss improved from 0.26305 to 0.26225, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 365ms/step - loss: 0.5492 - accuracy: 0.6875 - val_loss: 0.2623 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 44/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.4894 - accuracy: 0.7188\n",
            "Epoch 44: val_loss improved from 0.26225 to 0.25821, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.4854 - accuracy: 0.7270 - val_loss: 0.2582 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 45/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5292 - accuracy: 0.6562\n",
            "Epoch 45: val_loss improved from 0.25821 to 0.25082, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 373ms/step - loss: 0.5273 - accuracy: 0.6612 - val_loss: 0.2508 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 46/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5209 - accuracy: 0.7188\n",
            "Epoch 46: val_loss improved from 0.25082 to 0.24493, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 0.5213 - accuracy: 0.7171 - val_loss: 0.2449 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 47/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.4841 - accuracy: 0.7361\n",
            "Epoch 47: val_loss improved from 0.24493 to 0.24099, saving model to slo.h5\n",
            "10/10 [==============================] - 3s 280ms/step - loss: 0.4821 - accuracy: 0.7368 - val_loss: 0.2410 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 48/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.4982 - accuracy: 0.7153\n",
            "Epoch 48: val_loss improved from 0.24099 to 0.23812, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.5005 - accuracy: 0.7105 - val_loss: 0.2381 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 49/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.5268 - accuracy: 0.6944\n",
            "Epoch 49: val_loss improved from 0.23812 to 0.23629, saving model to slo.h5\n",
            "10/10 [==============================] - 4s 427ms/step - loss: 0.5235 - accuracy: 0.7039 - val_loss: 0.2363 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "Epoch 50/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.4960 - accuracy: 0.7500\n",
            "Epoch 50: val_loss improved from 0.23629 to 0.23185, saving model to slo.h5\n",
            "10/10 [==============================] - 2s 165ms/step - loss: 0.5047 - accuracy: 0.7401 - val_loss: 0.2318 - val_accuracy: 0.9032 - lr: 1.1241e-05\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2318 - accuracy: 0.9032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:42:49,896] Trial 4 finished with value: 0.9032257795333862 and parameters: {'batch_size': 32, 'n_layers': 4, 'n_units_l0': 170, 'dropout_l0': 0.6000000000000001, 'n_units_l1': 2485, 'dropout_l1': 0.4, 'n_units_l2': 372, 'dropout_l2': 0.6000000000000001, 'n_units_l3': 108, 'dropout_l3': 0.4, 'lr': 1.124096853136116e-05}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 3.0679 - accuracy: 0.5338"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:43:10,352] Trial 5 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.7082 - accuracy: 0.4549"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:43:32,118] Trial 6 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.8022 - accuracy: 0.6612\n",
            "Epoch 1: val_loss improved from inf to 4.12572, saving model to slo.h5\n",
            "5/5 [==============================] - 11s 1s/step - loss: 3.8022 - accuracy: 0.6612 - val_loss: 4.1257 - val_accuracy: 0.8710 - lr: 0.0024\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.6389 - accuracy: 0.7336\n",
            "Epoch 2: val_loss improved from 4.12572 to 0.73723, saving model to slo.h5\n",
            "5/5 [==============================] - 2s 396ms/step - loss: 1.6389 - accuracy: 0.7336 - val_loss: 0.7372 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.9605 - accuracy: 0.7303\n",
            "Epoch 3: val_loss did not improve from 0.73723\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.9605 - accuracy: 0.7303 - val_loss: 1.3486 - val_accuracy: 0.8710 - lr: 0.0024\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5399 - accuracy: 0.7368\n",
            "Epoch 4: val_loss improved from 0.73723 to 0.35990, saving model to slo.h5\n",
            "5/5 [==============================] - 2s 594ms/step - loss: 0.5399 - accuracy: 0.7368 - val_loss: 0.3599 - val_accuracy: 0.9677 - lr: 0.0024\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4163 - accuracy: 0.7730\n",
            "Epoch 5: val_loss improved from 0.35990 to 0.05442, saving model to slo.h5\n",
            "5/5 [==============================] - 1s 334ms/step - loss: 0.4163 - accuracy: 0.7730 - val_loss: 0.0544 - val_accuracy: 0.9677 - lr: 0.0024\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4390 - accuracy: 0.7697\n",
            "Epoch 6: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.4390 - accuracy: 0.7697 - val_loss: 0.0686 - val_accuracy: 0.9677 - lr: 0.0024\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.7961\n",
            "Epoch 7: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.3506 - accuracy: 0.7961 - val_loss: 0.4865 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.7961\n",
            "Epoch 8: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.3459 - accuracy: 0.7961 - val_loss: 0.8412 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.7928\n",
            "Epoch 9: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.3437 - accuracy: 0.7928 - val_loss: 0.8079 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.7961\n",
            "Epoch 10: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.3418 - accuracy: 0.7961 - val_loss: 0.6803 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.7961\n",
            "Epoch 11: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.3497 - accuracy: 0.7961 - val_loss: 0.6039 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.7961\n",
            "Epoch 12: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.3392 - accuracy: 0.7961 - val_loss: 0.5480 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.7961\n",
            "Epoch 13: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 124ms/step - loss: 0.3539 - accuracy: 0.7961 - val_loss: 0.5242 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3542 - accuracy: 0.7171\n",
            "Epoch 14: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.3542 - accuracy: 0.7171 - val_loss: 0.5028 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.7961\n",
            "Epoch 15: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.3567 - accuracy: 0.7961 - val_loss: 0.4996 - val_accuracy: 0.9355 - lr: 0.0024\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.7961\n",
            "Epoch 16: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 123ms/step - loss: 0.3415 - accuracy: 0.7961 - val_loss: 0.4973 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.7961\n",
            "Epoch 17: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.3414 - accuracy: 0.7961 - val_loss: 0.4945 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.7961\n",
            "Epoch 18: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.3387 - accuracy: 0.7961 - val_loss: 0.4945 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.7961\n",
            "Epoch 19: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 124ms/step - loss: 0.3387 - accuracy: 0.7961 - val_loss: 0.4939 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.7961\n",
            "Epoch 20: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.3384 - accuracy: 0.7961 - val_loss: 0.4946 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.7961\n",
            "Epoch 21: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.3381 - accuracy: 0.7961 - val_loss: 0.4949 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3385 - accuracy: 0.7961\n",
            "Epoch 22: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.3385 - accuracy: 0.7961 - val_loss: 0.4949 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3380 - accuracy: 0.7961\n",
            "Epoch 23: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 124ms/step - loss: 0.3380 - accuracy: 0.7961 - val_loss: 0.4951 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.7961\n",
            "Epoch 24: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.3381 - accuracy: 0.7961 - val_loss: 0.4955 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.7961\n",
            "Epoch 25: val_loss did not improve from 0.05442\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.3386 - accuracy: 0.7961 - val_loss: 0.4959 - val_accuracy: 0.9355 - lr: 2.3631e-04\n",
            "Epoch 25: early stopping\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0544 - accuracy: 0.9677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:44:09,941] Trial 7 finished with value: 0.9677419066429138 and parameters: {'batch_size': 64, 'n_layers': 1, 'n_units_l0': 216, 'dropout_l0': 0.0, 'lr': 0.0023630617040561524}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 6/38 [===>..........................] - ETA: 2s - loss: 2.3276 - accuracy: 0.4792"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0216s vs `on_train_batch_end` time: 0.0454s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - ETA: 0s - loss: 2.7508 - accuracy: 0.5033\n",
            "Epoch 1: val_loss improved from inf to 0.52169, saving model to slo.h5\n",
            "38/38 [==============================] - 15s 198ms/step - loss: 2.7508 - accuracy: 0.5033 - val_loss: 0.5217 - val_accuracy: 0.8710 - lr: 1.7509e-04\n",
            "Epoch 2/50\n",
            "38/38 [==============================] - ETA: 0s - loss: 2.9457 - accuracy: 0.5461"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:44:34,531] Trial 8 pruned. Trial was pruned at epoch 1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.7460 - accuracy: 0.6283"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:45:00,720] Trial 9 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 5/19 [======>.......................] - ETA: 0s - loss: 2154.7612 - accuracy: 0.6000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0163s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - ETA: 0s - loss: 593.3778 - accuracy: 0.4408"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:45:21,283] Trial 10 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1094 - accuracy: 0.5855"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:45:39,220] Trial 11 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3413 - accuracy: 0.5428"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ab2844a2cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[I 2024-02-27 06:45:59,735] Trial 12 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1264 - accuracy: 0.5164"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7ab2e0aa3c70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.28689, saving model to slo.h5\n",
            "3/3 [==============================] - 11s 2s/step - loss: 1.1264 - accuracy: 0.5164 - val_loss: 0.2869 - val_accuracy: 0.8710 - lr: 5.2412e-04\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0506 - accuracy: 0.5987\n",
            "Epoch 2: val_loss improved from 0.28689 to 0.23517, saving model to slo.h5\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.0506 - accuracy: 0.5987 - val_loss: 0.2352 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8831 - accuracy: 0.6414\n",
            "Epoch 3: val_loss did not improve from 0.23517\n",
            "3/3 [==============================] - 1s 180ms/step - loss: 0.8831 - accuracy: 0.6414 - val_loss: 0.4004 - val_accuracy: 0.8710 - lr: 5.2412e-04\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8097 - accuracy: 0.6875\n",
            "Epoch 4: val_loss did not improve from 0.23517\n",
            "3/3 [==============================] - 1s 189ms/step - loss: 0.8097 - accuracy: 0.6875 - val_loss: 0.3240 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6726 - accuracy: 0.6875\n",
            "Epoch 5: val_loss did not improve from 0.23517\n",
            "3/3 [==============================] - 1s 191ms/step - loss: 0.6726 - accuracy: 0.6875 - val_loss: 0.2916 - val_accuracy: 0.8710 - lr: 5.2412e-04\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.6414\n",
            "Epoch 6: val_loss did not improve from 0.23517\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.6991 - accuracy: 0.6414 - val_loss: 0.2447 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7654 - accuracy: 0.6382\n",
            "Epoch 7: val_loss did not improve from 0.23517\n",
            "3/3 [==============================] - 1s 195ms/step - loss: 0.7654 - accuracy: 0.6382 - val_loss: 0.2494 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.6513\n",
            "Epoch 8: val_loss improved from 0.23517 to 0.21868, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 725ms/step - loss: 0.5877 - accuracy: 0.6513 - val_loss: 0.2187 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6388 - accuracy: 0.6546\n",
            "Epoch 9: val_loss did not improve from 0.21868\n",
            "3/3 [==============================] - 1s 190ms/step - loss: 0.6388 - accuracy: 0.6546 - val_loss: 0.2352 - val_accuracy: 0.8710 - lr: 5.2412e-04\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.6184\n",
            "Epoch 10: val_loss did not improve from 0.21868\n",
            "3/3 [==============================] - 1s 185ms/step - loss: 0.6996 - accuracy: 0.6184 - val_loss: 0.2549 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.6184\n",
            "Epoch 11: val_loss did not improve from 0.21868\n",
            "3/3 [==============================] - 1s 179ms/step - loss: 0.5406 - accuracy: 0.6184 - val_loss: 0.2711 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.6414\n",
            "Epoch 12: val_loss did not improve from 0.21868\n",
            "3/3 [==============================] - 1s 182ms/step - loss: 0.5127 - accuracy: 0.6414 - val_loss: 0.2473 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.6349\n",
            "Epoch 13: val_loss improved from 0.21868 to 0.21502, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 1s/step - loss: 0.5150 - accuracy: 0.6349 - val_loss: 0.2150 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5257 - accuracy: 0.6151\n",
            "Epoch 14: val_loss improved from 0.21502 to 0.18783, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 567ms/step - loss: 0.5257 - accuracy: 0.6151 - val_loss: 0.1878 - val_accuracy: 1.0000 - lr: 5.2412e-04\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.6743\n",
            "Epoch 15: val_loss improved from 0.18783 to 0.17589, saving model to slo.h5\n",
            "3/3 [==============================] - 4s 2s/step - loss: 0.5217 - accuracy: 0.6743 - val_loss: 0.1759 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.6875\n",
            "Epoch 16: val_loss improved from 0.17589 to 0.17097, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 683ms/step - loss: 0.5070 - accuracy: 0.6875 - val_loss: 0.1710 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.6941\n",
            "Epoch 17: val_loss did not improve from 0.17097\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.4964 - accuracy: 0.6941 - val_loss: 0.1839 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4871 - accuracy: 0.7039\n",
            "Epoch 18: val_loss did not improve from 0.17097\n",
            "3/3 [==============================] - 1s 197ms/step - loss: 0.4871 - accuracy: 0.7039 - val_loss: 0.1849 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.7204\n",
            "Epoch 19: val_loss improved from 0.17097 to 0.16909, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 659ms/step - loss: 0.4859 - accuracy: 0.7204 - val_loss: 0.1691 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4821 - accuracy: 0.7138\n",
            "Epoch 20: val_loss improved from 0.16909 to 0.15435, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 562ms/step - loss: 0.4821 - accuracy: 0.7138 - val_loss: 0.1544 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4905 - accuracy: 0.7401\n",
            "Epoch 21: val_loss improved from 0.15435 to 0.14092, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 556ms/step - loss: 0.4905 - accuracy: 0.7401 - val_loss: 0.1409 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4794 - accuracy: 0.7303\n",
            "Epoch 22: val_loss improved from 0.14092 to 0.13692, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 730ms/step - loss: 0.4794 - accuracy: 0.7303 - val_loss: 0.1369 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.7072\n",
            "Epoch 23: val_loss improved from 0.13692 to 0.12563, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 634ms/step - loss: 0.4566 - accuracy: 0.7072 - val_loss: 0.1256 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4327 - accuracy: 0.7401\n",
            "Epoch 24: val_loss improved from 0.12563 to 0.12366, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 574ms/step - loss: 0.4327 - accuracy: 0.7401 - val_loss: 0.1237 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4445 - accuracy: 0.7368\n",
            "Epoch 25: val_loss improved from 0.12366 to 0.12345, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 1s/step - loss: 0.4445 - accuracy: 0.7368 - val_loss: 0.1234 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.7434\n",
            "Epoch 26: val_loss did not improve from 0.12345\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.4465 - accuracy: 0.7434 - val_loss: 0.1321 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4441 - accuracy: 0.7500\n",
            "Epoch 27: val_loss did not improve from 0.12345\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.4441 - accuracy: 0.7500 - val_loss: 0.1319 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.7336\n",
            "Epoch 28: val_loss improved from 0.12345 to 0.11757, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 883ms/step - loss: 0.4610 - accuracy: 0.7336 - val_loss: 0.1176 - val_accuracy: 0.9032 - lr: 5.2412e-04\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.7566\n",
            "Epoch 29: val_loss improved from 0.11757 to 0.10283, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 1s/step - loss: 0.4413 - accuracy: 0.7566 - val_loss: 0.1028 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4346 - accuracy: 0.7467\n",
            "Epoch 30: val_loss improved from 0.10283 to 0.08856, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 926ms/step - loss: 0.4346 - accuracy: 0.7467 - val_loss: 0.0886 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.7533\n",
            "Epoch 31: val_loss improved from 0.08856 to 0.08511, saving model to slo.h5\n",
            "3/3 [==============================] - 5s 2s/step - loss: 0.4321 - accuracy: 0.7533 - val_loss: 0.0851 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.7368\n",
            "Epoch 32: val_loss improved from 0.08511 to 0.08139, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 707ms/step - loss: 0.4448 - accuracy: 0.7368 - val_loss: 0.0814 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.7500\n",
            "Epoch 33: val_loss did not improve from 0.08139\n",
            "3/3 [==============================] - 1s 209ms/step - loss: 0.4394 - accuracy: 0.7500 - val_loss: 0.0820 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.7599\n",
            "Epoch 34: val_loss improved from 0.08139 to 0.08024, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 1s/step - loss: 0.4283 - accuracy: 0.7599 - val_loss: 0.0802 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4168 - accuracy: 0.7664\n",
            "Epoch 35: val_loss did not improve from 0.08024\n",
            "3/3 [==============================] - 1s 187ms/step - loss: 0.4168 - accuracy: 0.7664 - val_loss: 0.0803 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.7566\n",
            "Epoch 36: val_loss improved from 0.08024 to 0.07436, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 580ms/step - loss: 0.4187 - accuracy: 0.7566 - val_loss: 0.0744 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4066 - accuracy: 0.7796\n",
            "Epoch 37: val_loss improved from 0.07436 to 0.07118, saving model to slo.h5\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.4066 - accuracy: 0.7796 - val_loss: 0.0712 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.7467\n",
            "Epoch 38: val_loss improved from 0.07118 to 0.06695, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 568ms/step - loss: 0.4310 - accuracy: 0.7467 - val_loss: 0.0670 - val_accuracy: 1.0000 - lr: 5.2412e-04\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4205 - accuracy: 0.7632\n",
            "Epoch 39: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 190ms/step - loss: 0.4205 - accuracy: 0.7632 - val_loss: 0.0690 - val_accuracy: 1.0000 - lr: 5.2412e-04\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4228 - accuracy: 0.7500\n",
            "Epoch 40: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 184ms/step - loss: 0.4228 - accuracy: 0.7500 - val_loss: 0.0723 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.7763\n",
            "Epoch 41: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 185ms/step - loss: 0.4102 - accuracy: 0.7763 - val_loss: 0.0755 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.7599\n",
            "Epoch 42: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 180ms/step - loss: 0.4550 - accuracy: 0.7599 - val_loss: 0.0765 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3946 - accuracy: 0.7697\n",
            "Epoch 43: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 192ms/step - loss: 0.3946 - accuracy: 0.7697 - val_loss: 0.0753 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.7500\n",
            "Epoch 44: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 195ms/step - loss: 0.4173 - accuracy: 0.7500 - val_loss: 0.0831 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.7632\n",
            "Epoch 45: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 209ms/step - loss: 0.4208 - accuracy: 0.7632 - val_loss: 0.0753 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4490 - accuracy: 0.7401\n",
            "Epoch 46: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 201ms/step - loss: 0.4490 - accuracy: 0.7401 - val_loss: 0.0714 - val_accuracy: 0.9677 - lr: 5.2412e-04\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.7697\n",
            "Epoch 47: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 208ms/step - loss: 0.3998 - accuracy: 0.7697 - val_loss: 0.0825 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.7632\n",
            "Epoch 48: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 212ms/step - loss: 0.4102 - accuracy: 0.7632 - val_loss: 0.0917 - val_accuracy: 0.9355 - lr: 5.2412e-04\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4078 - accuracy: 0.7697\n",
            "Epoch 49: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 190ms/step - loss: 0.4078 - accuracy: 0.7697 - val_loss: 0.0915 - val_accuracy: 0.9355 - lr: 5.2412e-05\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4224 - accuracy: 0.7599\n",
            "Epoch 50: val_loss did not improve from 0.06695\n",
            "3/3 [==============================] - 1s 192ms/step - loss: 0.4224 - accuracy: 0.7599 - val_loss: 0.0908 - val_accuracy: 0.9355 - lr: 5.2412e-05\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0670 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:47:21,613] Trial 13 finished with value: 1.0 and parameters: {'batch_size': 128, 'n_layers': 2, 'n_units_l0': 79, 'dropout_l0': 0.7, 'n_units_l1': 42, 'dropout_l1': 0.2, 'lr': 0.0005241225689638107}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7926 - accuracy: 0.6349\n",
            "Epoch 1: val_loss improved from inf to 0.23914, saving model to slo.h5\n",
            "3/3 [==============================] - 12s 2s/step - loss: 0.7926 - accuracy: 0.6349 - val_loss: 0.2391 - val_accuracy: 0.8710 - lr: 4.7440e-04\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5913 - accuracy: 0.7007"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:47:41,318] Trial 14 pruned. Trial was pruned at epoch 1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0807 - accuracy: 0.4934"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:48:00,104] Trial 15 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 55289.3672 - accuracy: 0.5263"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:48:18,584] Trial 16 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0958 - accuracy: 0.5329"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:48:40,203] Trial 17 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 1.0639 - accuracy: 0.5139"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:48:58,452] Trial 18 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 5/38 [==>...........................] - ETA: 1s - loss: 16.3346 - accuracy: 0.6000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0165s vs `on_train_batch_end` time: 0.0219s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/38 [============================>.] - ETA: 0s - loss: 6.2815 - accuracy: 0.5372"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:49:17,367] Trial 19 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 5/19 [======>.......................] - ETA: 0s - loss: 23.2430 - accuracy: 0.5250"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0185s vs `on_train_batch_end` time: 0.0340s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - ETA: 0s - loss: 7.2470 - accuracy: 0.5461"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:49:38,286] Trial 20 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9594 - accuracy: 0.5230\n",
            "Epoch 1: val_loss improved from inf to 0.39219, saving model to slo.h5\n",
            "3/3 [==============================] - 10s 2s/step - loss: 0.9594 - accuracy: 0.5230 - val_loss: 0.3922 - val_accuracy: 0.8710 - lr: 5.6647e-05\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.6908\n",
            "Epoch 2: val_loss improved from 0.39219 to 0.27727, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 809ms/step - loss: 0.6061 - accuracy: 0.6908 - val_loss: 0.2773 - val_accuracy: 0.9032 - lr: 5.6647e-05\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.7368\n",
            "Epoch 3: val_loss did not improve from 0.27727\n",
            "3/3 [==============================] - 1s 196ms/step - loss: 0.5169 - accuracy: 0.7368 - val_loss: 0.2899 - val_accuracy: 0.9032 - lr: 5.6647e-05\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.7401\n",
            "Epoch 4: val_loss improved from 0.27727 to 0.20353, saving model to slo.h5\n",
            "3/3 [==============================] - 4s 2s/step - loss: 0.4758 - accuracy: 0.7401 - val_loss: 0.2035 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.7303\n",
            "Epoch 5: val_loss improved from 0.20353 to 0.14047, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 701ms/step - loss: 0.4749 - accuracy: 0.7303 - val_loss: 0.1405 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.7862\n",
            "Epoch 6: val_loss did not improve from 0.14047\n",
            "3/3 [==============================] - 1s 188ms/step - loss: 0.3970 - accuracy: 0.7862 - val_loss: 0.1437 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.7434\n",
            "Epoch 7: val_loss improved from 0.14047 to 0.11683, saving model to slo.h5\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.4472 - accuracy: 0.7434 - val_loss: 0.1168 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.7697\n",
            "Epoch 8: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 202ms/step - loss: 0.3855 - accuracy: 0.7697 - val_loss: 0.1349 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3701 - accuracy: 0.7467\n",
            "Epoch 9: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 214ms/step - loss: 0.3701 - accuracy: 0.7467 - val_loss: 0.1345 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.7401\n",
            "Epoch 10: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 215ms/step - loss: 0.3695 - accuracy: 0.7401 - val_loss: 0.1237 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.7664\n",
            "Epoch 11: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 246ms/step - loss: 0.3599 - accuracy: 0.7664 - val_loss: 0.1252 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.7664\n",
            "Epoch 12: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 212ms/step - loss: 0.3697 - accuracy: 0.7664 - val_loss: 0.1459 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3619 - accuracy: 0.7961\n",
            "Epoch 13: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 0.3619 - accuracy: 0.7961 - val_loss: 0.2054 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.8026\n",
            "Epoch 14: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 221ms/step - loss: 0.3499 - accuracy: 0.8026 - val_loss: 0.2142 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.7730\n",
            "Epoch 15: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 193ms/step - loss: 0.3621 - accuracy: 0.7730 - val_loss: 0.1703 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.7829\n",
            "Epoch 16: val_loss did not improve from 0.11683\n",
            "3/3 [==============================] - 1s 191ms/step - loss: 0.3620 - accuracy: 0.7829 - val_loss: 0.1225 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.7763\n",
            "Epoch 17: val_loss improved from 0.11683 to 0.09172, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 637ms/step - loss: 0.3536 - accuracy: 0.7763 - val_loss: 0.0917 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.7763\n",
            "Epoch 18: val_loss improved from 0.09172 to 0.08296, saving model to slo.h5\n",
            "3/3 [==============================] - 5s 2s/step - loss: 0.3509 - accuracy: 0.7763 - val_loss: 0.0830 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3651 - accuracy: 0.7467\n",
            "Epoch 19: val_loss improved from 0.08296 to 0.07628, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 699ms/step - loss: 0.3651 - accuracy: 0.7467 - val_loss: 0.0763 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.7697\n",
            "Epoch 20: val_loss improved from 0.07628 to 0.07345, saving model to slo.h5\n",
            "3/3 [==============================] - 3s 2s/step - loss: 0.3528 - accuracy: 0.7697 - val_loss: 0.0735 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.7632\n",
            "Epoch 21: val_loss did not improve from 0.07345\n",
            "3/3 [==============================] - 1s 186ms/step - loss: 0.3607 - accuracy: 0.7632 - val_loss: 0.0875 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.7829\n",
            "Epoch 22: val_loss did not improve from 0.07345\n",
            "3/3 [==============================] - 1s 191ms/step - loss: 0.3485 - accuracy: 0.7829 - val_loss: 0.1050 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.7829\n",
            "Epoch 23: val_loss did not improve from 0.07345\n",
            "3/3 [==============================] - 1s 189ms/step - loss: 0.3530 - accuracy: 0.7829 - val_loss: 0.1043 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3512 - accuracy: 0.7928\n",
            "Epoch 24: val_loss did not improve from 0.07345\n",
            "3/3 [==============================] - 1s 183ms/step - loss: 0.3512 - accuracy: 0.7928 - val_loss: 0.1016 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.7533\n",
            "Epoch 25: val_loss did not improve from 0.07345\n",
            "3/3 [==============================] - 1s 186ms/step - loss: 0.3530 - accuracy: 0.7533 - val_loss: 0.0972 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.7730\n",
            "Epoch 26: val_loss did not improve from 0.07345\n",
            "3/3 [==============================] - 1s 191ms/step - loss: 0.3471 - accuracy: 0.7730 - val_loss: 0.0865 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 0.8026\n",
            "Epoch 27: val_loss did not improve from 0.07345\n",
            "3/3 [==============================] - 1s 183ms/step - loss: 0.3344 - accuracy: 0.8026 - val_loss: 0.0775 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.7500\n",
            "Epoch 28: val_loss improved from 0.07345 to 0.07222, saving model to slo.h5\n",
            "3/3 [==============================] - 1s 638ms/step - loss: 0.3523 - accuracy: 0.7500 - val_loss: 0.0722 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.7862\n",
            "Epoch 29: val_loss improved from 0.07222 to 0.06806, saving model to slo.h5\n",
            "3/3 [==============================] - 5s 2s/step - loss: 0.3476 - accuracy: 0.7862 - val_loss: 0.0681 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3416 - accuracy: 0.7829\n",
            "Epoch 30: val_loss improved from 0.06806 to 0.06529, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 810ms/step - loss: 0.3416 - accuracy: 0.7829 - val_loss: 0.0653 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.7895\n",
            "Epoch 31: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 207ms/step - loss: 0.3479 - accuracy: 0.7895 - val_loss: 0.0696 - val_accuracy: 0.9677 - lr: 5.6647e-05\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3477 - accuracy: 0.7829\n",
            "Epoch 32: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 196ms/step - loss: 0.3477 - accuracy: 0.7829 - val_loss: 0.0836 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.7961\n",
            "Epoch 33: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 184ms/step - loss: 0.3563 - accuracy: 0.7961 - val_loss: 0.1132 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.7796\n",
            "Epoch 34: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 183ms/step - loss: 0.3474 - accuracy: 0.7796 - val_loss: 0.1577 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.7928\n",
            "Epoch 35: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 190ms/step - loss: 0.3495 - accuracy: 0.7928 - val_loss: 0.1808 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.7928\n",
            "Epoch 36: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 184ms/step - loss: 0.3451 - accuracy: 0.7928 - val_loss: 0.1703 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.7796\n",
            "Epoch 37: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 191ms/step - loss: 0.3506 - accuracy: 0.7796 - val_loss: 0.1535 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3435 - accuracy: 0.8026\n",
            "Epoch 38: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 186ms/step - loss: 0.3435 - accuracy: 0.8026 - val_loss: 0.1301 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.7961\n",
            "Epoch 39: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 193ms/step - loss: 0.3413 - accuracy: 0.7961 - val_loss: 0.1263 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8026\n",
            "Epoch 40: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 194ms/step - loss: 0.3417 - accuracy: 0.8026 - val_loss: 0.1215 - val_accuracy: 0.9355 - lr: 5.6647e-05\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.7895\n",
            "Epoch 41: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 186ms/step - loss: 0.3474 - accuracy: 0.7895 - val_loss: 0.1213 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.7993\n",
            "Epoch 42: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 185ms/step - loss: 0.3405 - accuracy: 0.7993 - val_loss: 0.1219 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.7928\n",
            "Epoch 43: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 184ms/step - loss: 0.3450 - accuracy: 0.7928 - val_loss: 0.1214 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3487 - accuracy: 0.7730\n",
            "Epoch 44: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 192ms/step - loss: 0.3487 - accuracy: 0.7730 - val_loss: 0.1205 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.7862\n",
            "Epoch 45: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 185ms/step - loss: 0.3401 - accuracy: 0.7862 - val_loss: 0.1192 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3457 - accuracy: 0.7829\n",
            "Epoch 46: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 185ms/step - loss: 0.3457 - accuracy: 0.7829 - val_loss: 0.1182 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.7697\n",
            "Epoch 47: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 192ms/step - loss: 0.3403 - accuracy: 0.7697 - val_loss: 0.1175 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3396 - accuracy: 0.8026\n",
            "Epoch 48: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 186ms/step - loss: 0.3396 - accuracy: 0.8026 - val_loss: 0.1176 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.7796\n",
            "Epoch 49: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 193ms/step - loss: 0.3509 - accuracy: 0.7796 - val_loss: 0.1173 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.7993\n",
            "Epoch 50: val_loss did not improve from 0.06529\n",
            "3/3 [==============================] - 1s 203ms/step - loss: 0.3401 - accuracy: 0.7993 - val_loss: 0.1166 - val_accuracy: 0.9355 - lr: 5.6647e-06\n",
            "Epoch 50: early stopping\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0653 - accuracy: 0.9677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:50:50,440] Trial 21 finished with value: 0.9677419066429138 and parameters: {'batch_size': 128, 'n_layers': 1, 'n_units_l0': 380, 'dropout_l0': 0.6000000000000001, 'lr': 5.664714849255395e-05}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7428 - accuracy: 0.5855\n",
            "Epoch 1: val_loss improved from inf to 0.28522, saving model to slo.h5\n",
            "3/3 [==============================] - 12s 2s/step - loss: 0.7428 - accuracy: 0.5855 - val_loss: 0.2852 - val_accuracy: 0.9032 - lr: 1.2159e-04\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5899 - accuracy: 0.6776\n",
            "Epoch 2: val_loss did not improve from 0.28522\n",
            "3/3 [==============================] - 1s 185ms/step - loss: 0.5899 - accuracy: 0.6776 - val_loss: 0.2889 - val_accuracy: 0.8710 - lr: 1.2159e-04\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5891 - accuracy: 0.6316\n",
            "Epoch 3: val_loss improved from 0.28522 to 0.24878, saving model to slo.h5\n",
            "3/3 [==============================] - 2s 727ms/step - loss: 0.5891 - accuracy: 0.6316 - val_loss: 0.2488 - val_accuracy: 0.8710 - lr: 1.2159e-04\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4646 - accuracy: 0.6875"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:51:12,049] Trial 22 pruned. Trial was pruned at epoch 3.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4790 - accuracy: 0.4309"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:51:30,917] Trial 23 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7496 - accuracy: 0.5987\n",
            "Epoch 1: val_loss improved from inf to 0.30137, saving model to slo.h5\n",
            "3/3 [==============================] - 12s 2s/step - loss: 0.7496 - accuracy: 0.5987 - val_loss: 0.3014 - val_accuracy: 0.8710 - lr: 2.6259e-04\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.6579"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:51:52,385] Trial 24 pruned. Trial was pruned at epoch 1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.8765 - accuracy: 0.5526"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:52:10,226] Trial 25 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0128 - accuracy: 0.5757"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:52:27,976] Trial 26 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7182 - accuracy: 0.5855"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:52:48,677] Trial 27 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.9920 - accuracy: 0.5395"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:53:07,250] Trial 28 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6780 - accuracy: 0.5822"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:53:25,963] Trial 29 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 6/38 [===>..........................] - ETA: 1s - loss: 15.0812 - accuracy: 0.6042"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0196s vs `on_train_batch_end` time: 0.0241s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/38 [============================>.] - ETA: 0s - loss: 21.5287 - accuracy: 0.5777\n",
            "Epoch 1: val_loss improved from inf to 3.00372, saving model to slo.h5\n",
            "38/38 [==============================] - 11s 134ms/step - loss: 21.1757 - accuracy: 0.5789 - val_loss: 3.0037 - val_accuracy: 0.9032 - lr: 0.0037\n",
            "Epoch 2/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 11.9367 - accuracy: 0.6689\n",
            "Epoch 2: val_loss improved from 3.00372 to 2.84097, saving model to slo.h5\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 11.8129 - accuracy: 0.6678 - val_loss: 2.8410 - val_accuracy: 0.9355 - lr: 0.0037\n",
            "Epoch 3/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 11.1996 - accuracy: 0.6858\n",
            "Epoch 3: val_loss improved from 2.84097 to 1.74626, saving model to slo.h5\n",
            "38/38 [==============================] - 3s 82ms/step - loss: 10.9115 - accuracy: 0.6908 - val_loss: 1.7463 - val_accuracy: 0.9355 - lr: 0.0037\n",
            "Epoch 4/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 4.7444 - accuracy: 0.7061\n",
            "Epoch 4: val_loss did not improve from 1.74626\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 4.6339 - accuracy: 0.7072 - val_loss: 5.5396 - val_accuracy: 0.8387 - lr: 0.0037\n",
            "Epoch 5/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 3.2000 - accuracy: 0.7196\n",
            "Epoch 5: val_loss did not improve from 1.74626\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 3.3257 - accuracy: 0.7138 - val_loss: 16.2304 - val_accuracy: 0.8065 - lr: 0.0037\n",
            "Epoch 6/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 4.1929 - accuracy: 0.6858\n",
            "Epoch 6: val_loss did not improve from 1.74626\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 4.2396 - accuracy: 0.6908 - val_loss: 3.1303 - val_accuracy: 0.9355 - lr: 0.0037\n",
            "Epoch 7/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.9496 - accuracy: 0.7601\n",
            "Epoch 7: val_loss improved from 1.74626 to 0.00002, saving model to slo.h5\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 1.9074 - accuracy: 0.7632 - val_loss: 1.8419e-05 - val_accuracy: 1.0000 - lr: 0.0037\n",
            "Epoch 8/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 3.6898 - accuracy: 0.7230\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 3.5993 - accuracy: 0.7303 - val_loss: 0.1273 - val_accuracy: 0.9677 - lr: 0.0037\n",
            "Epoch 9/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.7878 - accuracy: 0.7703\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 1.7521 - accuracy: 0.7697 - val_loss: 2.5618 - val_accuracy: 0.9355 - lr: 0.0037\n",
            "Epoch 10/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.7164 - accuracy: 0.7669\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 1.6831 - accuracy: 0.7632 - val_loss: 0.1987 - val_accuracy: 0.9355 - lr: 0.0037\n",
            "Epoch 11/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.4683 - accuracy: 0.7601\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 1.4389 - accuracy: 0.7599 - val_loss: 1.8301 - val_accuracy: 0.8710 - lr: 0.0037\n",
            "Epoch 12/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6248 - accuracy: 0.7872\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 0.6195 - accuracy: 0.7862 - val_loss: 1.7079e-04 - val_accuracy: 1.0000 - lr: 0.0037\n",
            "Epoch 13/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.8986 - accuracy: 0.7703\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 0.9961 - accuracy: 0.7632 - val_loss: 1.7272 - val_accuracy: 0.9032 - lr: 0.0037\n",
            "Epoch 14/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.4292 - accuracy: 0.7770\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 1.4034 - accuracy: 0.7730 - val_loss: 0.7229 - val_accuracy: 0.9355 - lr: 0.0037\n",
            "Epoch 15/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.7773 - accuracy: 0.7568\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 1.7383 - accuracy: 0.7632 - val_loss: 8.1775 - val_accuracy: 0.8387 - lr: 0.0037\n",
            "Epoch 16/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.4517 - accuracy: 0.7770\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 37ms/step - loss: 1.4280 - accuracy: 0.7697 - val_loss: 3.1815 - val_accuracy: 0.8710 - lr: 0.0037\n",
            "Epoch 17/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.3976 - accuracy: 0.7601\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 1.3673 - accuracy: 0.7632 - val_loss: 0.6875 - val_accuracy: 0.9355 - lr: 0.0037\n",
            "Epoch 18/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.8146 - accuracy: 0.7838\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.8016 - accuracy: 0.7862 - val_loss: 0.5267 - val_accuracy: 0.9355 - lr: 3.6637e-04\n",
            "Epoch 19/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6567 - accuracy: 0.7872\n",
            "Epoch 19: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 0.6467 - accuracy: 0.7862 - val_loss: 0.0147 - val_accuracy: 1.0000 - lr: 3.6637e-04\n",
            "Epoch 20/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7698 - accuracy: 0.7838\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.7550 - accuracy: 0.7829 - val_loss: 0.3892 - val_accuracy: 0.9677 - lr: 3.6637e-04\n",
            "Epoch 21/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6664 - accuracy: 0.7872\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.6599 - accuracy: 0.7862 - val_loss: 9.0939e-04 - val_accuracy: 1.0000 - lr: 3.6637e-04\n",
            "Epoch 22/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7761 - accuracy: 0.7736\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 0.7583 - accuracy: 0.7763 - val_loss: 0.4032 - val_accuracy: 0.9677 - lr: 3.6637e-04\n",
            "Epoch 23/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5995 - accuracy: 0.7905\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 0.5957 - accuracy: 0.7862 - val_loss: 0.3991 - val_accuracy: 0.9677 - lr: 3.6637e-04\n",
            "Epoch 24/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7363 - accuracy: 0.7804\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 0.7261 - accuracy: 0.7796 - val_loss: 0.0871 - val_accuracy: 0.9677 - lr: 3.6637e-04\n",
            "Epoch 25/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5029 - accuracy: 0.7872\n",
            "Epoch 25: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.4954 - accuracy: 0.7928 - val_loss: 0.0492 - val_accuracy: 0.9677 - lr: 3.6637e-04\n",
            "Epoch 26/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4499 - accuracy: 0.7804\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.4427 - accuracy: 0.7829 - val_loss: 0.2349 - val_accuracy: 0.9677 - lr: 3.6637e-04\n",
            "Epoch 27/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3427 - accuracy: 0.7939\n",
            "Epoch 27: val_loss did not improve from 0.00002\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 0.3401 - accuracy: 0.7961 - val_loss: 0.2366 - val_accuracy: 0.9677 - lr: 3.6637e-04\n",
            "Epoch 27: early stopping\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 1.8422e-05 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:55:02,514] Trial 30 finished with value: 1.0 and parameters: {'batch_size': 8, 'n_layers': 1, 'n_units_l0': 440, 'dropout_l0': 0.7, 'lr': 0.003663694803805462}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 6/38 [===>..........................] - ETA: 1s - loss: 47.8658 - accuracy: 0.4792"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0204s vs `on_train_batch_end` time: 0.0257s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - ETA: 0s - loss: 88.6564 - accuracy: 0.6086\n",
            "Epoch 1: val_loss improved from inf to 45.25130, saving model to slo.h5\n",
            "38/38 [==============================] - 12s 134ms/step - loss: 88.6564 - accuracy: 0.6086 - val_loss: 45.2513 - val_accuracy: 0.8710 - lr: 0.0163\n",
            "Epoch 2/50\n",
            "38/38 [==============================] - ETA: 0s - loss: 57.9901 - accuracy: 0.6513\n",
            "Epoch 2: val_loss improved from 45.25130 to 23.82442, saving model to slo.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 57.9901 - accuracy: 0.6513 - val_loss: 23.8244 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 3/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 39.2009 - accuracy: 0.6588\n",
            "Epoch 3: val_loss did not improve from 23.82442\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 38.9212 - accuracy: 0.6612 - val_loss: 25.6805 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 4/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 24.7355 - accuracy: 0.7264\n",
            "Epoch 4: val_loss improved from 23.82442 to 8.15570, saving model to slo.h5\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 24.5919 - accuracy: 0.7237 - val_loss: 8.1557 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 5/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 11.3490 - accuracy: 0.7432\n",
            "Epoch 5: val_loss improved from 8.15570 to 0.96378, saving model to slo.h5\n",
            "38/38 [==============================] - 2s 57ms/step - loss: 11.0596 - accuracy: 0.7434 - val_loss: 0.9638 - val_accuracy: 0.9677 - lr: 0.0163\n",
            "Epoch 6/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 3.5428 - accuracy: 0.7635\n",
            "Epoch 6: val_loss did not improve from 0.96378\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 3.4588 - accuracy: 0.7632 - val_loss: 3.8099 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 7/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 21.1802 - accuracy: 0.7264\n",
            "Epoch 7: val_loss did not improve from 0.96378\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 20.6311 - accuracy: 0.7303 - val_loss: 2.1643 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 8/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 9.5225 - accuracy: 0.7466\n",
            "Epoch 8: val_loss did not improve from 0.96378\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 9.2841 - accuracy: 0.7434 - val_loss: 11.2601 - val_accuracy: 0.8387 - lr: 0.0163\n",
            "Epoch 9/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 4.0463 - accuracy: 0.7264\n",
            "Epoch 9: val_loss did not improve from 0.96378\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 3.9444 - accuracy: 0.7303 - val_loss: 1.0152 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 10/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 2.0630 - accuracy: 0.7500\n",
            "Epoch 10: val_loss did not improve from 0.96378\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 2.1628 - accuracy: 0.7467 - val_loss: 1.8025 - val_accuracy: 0.9677 - lr: 0.0163\n",
            "Epoch 11/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 2.5353 - accuracy: 0.7264\n",
            "Epoch 11: val_loss did not improve from 0.96378\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 2.4737 - accuracy: 0.7336 - val_loss: 1.5642 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 12/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.0079 - accuracy: 0.7466\n",
            "Epoch 12: val_loss improved from 0.96378 to 0.80231, saving model to slo.h5\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.9894 - accuracy: 0.7500 - val_loss: 0.8023 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 13/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6542 - accuracy: 0.7601\n",
            "Epoch 13: val_loss did not improve from 0.80231\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.6463 - accuracy: 0.7599 - val_loss: 1.2169 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 14/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.9430 - accuracy: 0.7500\n",
            "Epoch 14: val_loss did not improve from 0.80231\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 0.9495 - accuracy: 0.7467 - val_loss: 2.5592 - val_accuracy: 0.8387 - lr: 0.0163\n",
            "Epoch 15/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.7226 - accuracy: 0.7365\n",
            "Epoch 15: val_loss did not improve from 0.80231\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 1.6874 - accuracy: 0.7401 - val_loss: 2.7238 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 16/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.8744 - accuracy: 0.7669\n",
            "Epoch 16: val_loss improved from 0.80231 to 0.25170, saving model to slo.h5\n",
            "38/38 [==============================] - 2s 57ms/step - loss: 0.8643 - accuracy: 0.7664 - val_loss: 0.2517 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 17/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.9565 - accuracy: 0.7635\n",
            "Epoch 17: val_loss did not improve from 0.25170\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.9435 - accuracy: 0.7599 - val_loss: 0.5859 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 18/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.4142 - accuracy: 0.7500\n",
            "Epoch 18: val_loss did not improve from 0.25170\n",
            "38/38 [==============================] - 1s 38ms/step - loss: 1.3898 - accuracy: 0.7500 - val_loss: 1.1181 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 19/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7028 - accuracy: 0.7466\n",
            "Epoch 19: val_loss improved from 0.25170 to 0.06199, saving model to slo.h5\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.7225 - accuracy: 0.7467 - val_loss: 0.0620 - val_accuracy: 0.9677 - lr: 0.0163\n",
            "Epoch 20/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5446 - accuracy: 0.7500\n",
            "Epoch 20: val_loss did not improve from 0.06199\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.5445 - accuracy: 0.7467 - val_loss: 0.0731 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 21/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4318 - accuracy: 0.7230\n",
            "Epoch 21: val_loss did not improve from 0.06199\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.4310 - accuracy: 0.7204 - val_loss: 0.0818 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 22/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.2859 - accuracy: 0.7399\n",
            "Epoch 22: val_loss did not improve from 0.06199\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 1.2650 - accuracy: 0.7401 - val_loss: 8.0259 - val_accuracy: 0.8387 - lr: 0.0163\n",
            "Epoch 23/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 2.0496 - accuracy: 0.7432\n",
            "Epoch 23: val_loss did not improve from 0.06199\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 2.0040 - accuracy: 0.7467 - val_loss: 3.7217 - val_accuracy: 0.8710 - lr: 0.0163\n",
            "Epoch 24/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.8402 - accuracy: 0.7162\n",
            "Epoch 24: val_loss improved from 0.06199 to 0.05677, saving model to slo.h5\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 1.8064 - accuracy: 0.7171 - val_loss: 0.0568 - val_accuracy: 0.9677 - lr: 0.0163\n",
            "Epoch 25/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.7500\n",
            "Epoch 25: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 0.4382 - accuracy: 0.7401 - val_loss: 0.1916 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 26/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5496 - accuracy: 0.7264\n",
            "Epoch 26: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.5509 - accuracy: 0.7237 - val_loss: 0.1776 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 27/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.7230\n",
            "Epoch 27: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 1.0235 - accuracy: 0.7138 - val_loss: 0.1790 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 28/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5452 - accuracy: 0.7264\n",
            "Epoch 28: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 0.5395 - accuracy: 0.7237 - val_loss: 0.1642 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 29/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7649 - accuracy: 0.7331\n",
            "Epoch 29: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 0.7549 - accuracy: 0.7368 - val_loss: 0.0736 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 30/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4368 - accuracy: 0.7399\n",
            "Epoch 30: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 0.4364 - accuracy: 0.7401 - val_loss: 0.0738 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 31/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4374 - accuracy: 0.7297\n",
            "Epoch 31: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.4363 - accuracy: 0.7270 - val_loss: 0.0742 - val_accuracy: 0.9355 - lr: 0.0163\n",
            "Epoch 32/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4946 - accuracy: 0.7601\n",
            "Epoch 32: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.4892 - accuracy: 0.7599 - val_loss: 0.2240 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 33/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4137 - accuracy: 0.7365\n",
            "Epoch 33: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.4146 - accuracy: 0.7401 - val_loss: 0.2243 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 34/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4394 - accuracy: 0.7230\n",
            "Epoch 34: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 0.4390 - accuracy: 0.7237 - val_loss: 0.2249 - val_accuracy: 0.9032 - lr: 0.0163\n",
            "Epoch 35/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4277 - accuracy: 0.7432\n",
            "Epoch 35: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 34ms/step - loss: 0.4258 - accuracy: 0.7500 - val_loss: 0.2248 - val_accuracy: 0.9032 - lr: 0.0016\n",
            "Epoch 36/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4317 - accuracy: 0.7365\n",
            "Epoch 36: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 35ms/step - loss: 0.4289 - accuracy: 0.7401 - val_loss: 0.2247 - val_accuracy: 0.9032 - lr: 0.0016\n",
            "Epoch 37/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.7399\n",
            "Epoch 37: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 36ms/step - loss: 0.4210 - accuracy: 0.7467 - val_loss: 0.2246 - val_accuracy: 0.9032 - lr: 0.0016\n",
            "Epoch 38/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4871 - accuracy: 0.7196\n",
            "Epoch 38: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 39ms/step - loss: 0.4890 - accuracy: 0.7204 - val_loss: 0.0729 - val_accuracy: 0.9355 - lr: 0.0016\n",
            "Epoch 39/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.7264\n",
            "Epoch 39: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 0.6904 - accuracy: 0.7204 - val_loss: 0.0761 - val_accuracy: 0.9355 - lr: 0.0016\n",
            "Epoch 40/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4313 - accuracy: 0.7432\n",
            "Epoch 40: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.4264 - accuracy: 0.7467 - val_loss: 0.0762 - val_accuracy: 0.9355 - lr: 0.0016\n",
            "Epoch 41/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4337 - accuracy: 0.7264\n",
            "Epoch 41: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.4324 - accuracy: 0.7303 - val_loss: 0.0762 - val_accuracy: 0.9355 - lr: 0.0016\n",
            "Epoch 42/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4327 - accuracy: 0.7365\n",
            "Epoch 42: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.4324 - accuracy: 0.7368 - val_loss: 0.0763 - val_accuracy: 0.9355 - lr: 0.0016\n",
            "Epoch 43/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4258 - accuracy: 0.7297\n",
            "Epoch 43: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 33ms/step - loss: 0.4237 - accuracy: 0.7368 - val_loss: 0.0763 - val_accuracy: 0.9355 - lr: 0.0016\n",
            "Epoch 44/50\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.7196\n",
            "Epoch 44: val_loss did not improve from 0.05677\n",
            "38/38 [==============================] - 1s 32ms/step - loss: 0.5875 - accuracy: 0.7237 - val_loss: 0.0762 - val_accuracy: 0.9355 - lr: 0.0016\n",
            "Epoch 44: early stopping\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0568 - accuracy: 0.9677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-02-27 06:56:40,595] Trial 31 finished with value: 0.9677419066429138 and parameters: {'batch_size': 8, 'n_layers': 1, 'n_units_l0': 325, 'dropout_l0': 0.7, 'lr': 0.016297644293488744}. Best is trial 0 with value: 1.0.\n",
            "[W 2024-02-27 06:56:58,937] Trial 32 failed with parameters: {'batch_size': 8} because of the following error: ResourceExhaustedError().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-19-9f7e61481ded>\", line 142, in objective\n",
            "    model = classifier (trial)\n",
            "  File \"<ipython-input-18-0af2df4b8f76>\", line 11, in classifier\n",
            "    base_model = tf.keras.applications.resnet.ResNet101(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\", line 556, in ResNet101\n",
            "    return ResNet(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\", line 192, in ResNet\n",
            "    x = stack_fn(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\", line 554, in stack_fn\n",
            "    return stack1(x, 512, 3, name=\"conv5\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\", line 311, in stack1\n",
            "    x = block1(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\", line 278, in block1\n",
            "    x = layers.Conv2D(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 2100, in random_uniform\n",
            "    return tf.random.stateless_uniform(\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: \n",
            "[W 2024-02-27 06:56:58,947] Trial 32 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-71d9bb1b13ff>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-9f7e61481ded>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0af2df4b8f76>\u001b[0m in \u001b[0;36mclassifier\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Load the pre-trained VGG16 model without the top (fully connected) layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   base_model = tf.keras.applications.resnet.ResNet101(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mResNet101\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     return ResNet(\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0mstack_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pool1_pool\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreact\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mstack_fn\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     return ResNet(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mstack1\u001b[0;34m(x, filters, blocks, stride1, name)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_block1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         x = block1(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_shortcut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_block\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/resnet.py\u001b[0m in \u001b[0;36mblock1\u001b[0;34m(x, filters, kernel_size, stride, conv_shortcut, name)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_1_relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     x = layers.Conv2D(\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_2_conv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     )(x)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m                 \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless_fold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m             return tf.random.stateless_uniform(\n\u001b[0m\u001b[1;32m   2101\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                 \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ('1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myw09Hadgqz1",
        "outputId": "3435b2bb-d5b2-46da-cd4c-ca209c85b094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "55CwNaKQvGZA",
        "outputId": "4818d220-e6bf-490c-be50-ec6ecb64eaf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  33\n",
            "  Number of pruned trials:  22\n",
            "  Number of complete trials:  10\n",
            "Best trial:\n",
            "  Value:  1.0\n",
            "  Params: \n",
            "    batch_size: 64\n",
            "    n_layers: 3\n",
            "    n_units_l0: 482\n",
            "    dropout_l0: 0.1\n",
            "    n_units_l1: 4019\n",
            "    dropout_l1: 0.30000000000000004\n",
            "    n_units_l2: 332\n",
            "    dropout_l2: 0.6000000000000001\n",
            "    lr: 0.0003720165816110892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/train_thickness_retina_iran.zip"
      ],
      "metadata": {
        "id": "Abghu8jri54J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/test_thickness_retina_iran.zip"
      ],
      "metadata": {
        "id": "FGd1V92Djg1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna Custom CNN"
      ],
      "metadata": {
        "id": "jwhjDizFBLVK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qF1IO0ku538H",
        "outputId": "1c9ab3d4-8bed-4c6e-85fa-34eee28b217c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.2 alembic-1.13.1 colorlog-6.8.2 optuna-3.5.0\n",
            "Collecting optuna-integration\n",
            "  Downloading optuna_integration-3.5.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (from optuna-integration) (3.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (2.0.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->optuna-integration) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration) (2.1.5)\n",
            "Installing collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from optuna.integration import TFKerasPruningCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SFwjWa-eQFQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmLlmRflQG61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input"
      ],
      "metadata": {
        "id": "oHmrmbMJQG9j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SELSRZwVQHAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "from keras.layers import Input\n",
        "\n",
        "def classifier (trial):\n",
        "  model = Sequential()\n",
        "  input_img = Input((60, 256, 1))\n",
        "  n_filters = trial.suggest_categorical(\"n_filters\", [8, 16, 20, 25])\n",
        "  print(f'n_filters = {n_filters}')\n",
        "\n",
        "  model.add(Conv2D(n_filters * 1, kernel_size=3, activation='relu',input_shape=input_img.shape[1:], padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "\n",
        "  model.add(Conv2D(n_filters * 2, kernel_size=3, activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "\n",
        "  model.add(Conv2D(n_filters * 4, kernel_size=3, activation='relu',padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "\n",
        "  model.add(Conv2D(n_filters * 8, kernel_size=3, activation='relu',padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "\n",
        "\n",
        "  model.add (Flatten ())\n",
        "\n",
        "  dropout_l0 = trial.suggest_float(\"dropout_l0\", 0, 0.7,step=0.1)\n",
        "\n",
        "  model.add (Dropout (dropout_l0))\n",
        "\n",
        "\n",
        "  n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "\n",
        "\n",
        "  for i in range(n_layers):\n",
        "\n",
        "      n_units = trial.suggest_int(\"n_units_l{}\".format(i), 8, 2048, log = True)\n",
        "\n",
        "      model.add (Dense (n_units, activation = 'relu'))\n",
        "\n",
        "      dropout = trial.suggest_float(\"dropout_l{}\".format (i+1), 0, 0.7,step=0.1)\n",
        "\n",
        "      model.add (Dropout (rate = dropout))\n",
        "\n",
        "  model.add (Dense (1, activation = 'sigmoid'))\n",
        "\n",
        "  lr = trial.suggest_float ('lr', 1e-5, 1e-3, log = True)\n",
        "\n",
        "  my_optimizer = tf.keras.optimizers.Adam (learning_rate= lr)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=my_optimizer, loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "EUBYqz_tM8bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = 42)\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "            data.append(np.array(x[i][j]))\n",
        "            label.append(y[i])\n",
        "\n",
        "\n",
        "    data = np.reshape(data, np.shape(data))\n",
        "    return data, np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def objective (trial):\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "  rotation_range= 20, # rotation\n",
        "  fill_mode='nearest',\n",
        "  data_format='channels_last',\n",
        "    )\n",
        "\n",
        "  datagen2 = ImageDataGenerator(\n",
        "  rotation_range= 5, # rotation\n",
        "  width_shift_range= [-30, 30], # horizontal shift\n",
        "  height_shift_range= [-5, 5] , # vertical shift\n",
        "  zoom_range= 0.2,\n",
        "  vertical_flip= True , # vertical flip\n",
        "  brightness_range= [0.2, 1.5]\n",
        "    )\n",
        "\n",
        "\n",
        "  images_train = pickle.load(open(\"/content/\"+\"train_thickness_retina.pkl\", 'rb'))\n",
        "  labels_train = pickle.load(open(\"/content/\"+\"train_labels.pkl\", 'rb'))\n",
        "\n",
        "  train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "  x_train = {i: images_train[list(images_train.keys())[i]]  for i in train_index}\n",
        "\n",
        "  x_valid = {i: images_train[list(images_train.keys())[i]]  for i in val_index}\n",
        "\n",
        "  y_train = {i: labels_train[list(labels_train.keys())[i]] for i in train_index}\n",
        "\n",
        "  y_valid = {i: labels_train[list(labels_train.keys())[i]] for i in val_index}\n",
        "\n",
        "  x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "  x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "\n",
        "  #rotation_angle  = trial.suggest_float(\"rotation_angle\", 0, 40, step = 10)\n",
        "  #zoom_range = trial.suggest_float(\"zoom_range\", 0, 0.5, step = 0.1)\n",
        "\n",
        "  #color_jitter_brightness = trial.suggest_float (\"color_jitter_brightness\", 0.1, 0.8, step = 0.1)\n",
        "  #shift_horizontal = trial.suggest_uniform(\"shift_horizontal\", 0, 0.7)\n",
        "  #shift_vertical = trial.suggest_uniform(\"shift_vertical\", 0, 0.7)\n",
        "  #vertical_flip_prob = trial.suggest_float (\"vertical_flip_prob\", 0, 1, step = 0.1)\n",
        "\n",
        "  indices = np.where(y_train == 1)[0]\n",
        "\n",
        "  x_train_ms = x_train[indices]\n",
        "  y_train_ms = y_train[indices]\n",
        "  #################Augmentation##################################\n",
        "\n",
        "  # First Augmentation\n",
        "\n",
        "  x_train_ms_aug =np.zeros_like(x_train_ms, dtype=np.float32)\n",
        "\n",
        "  y_train_ms_aug =np.zeros_like(y_train_ms, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_ms)):\n",
        "\n",
        "    x1= x_train_ms[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_ms_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_ms_aug [i] = y_train[i]\n",
        "\n",
        "  x_train = np.concatenate ((x_train, x_train_ms_aug), axis=0)\n",
        "  y_train = np.concatenate ((y_train, y_train_ms_aug), axis = 0)\n",
        "\n",
        "  indices = np.random.permutation (len (x_train))\n",
        "\n",
        "  x_train_shuf = x_train [indices]\n",
        "  y_train_shuf = y_train [indices]\n",
        "\n",
        "  # Second Augmentation\n",
        "\n",
        "  x_train_aug =np.zeros_like(x_train_shuf, dtype=np.float32)\n",
        "\n",
        "  y_train_aug =np.zeros_like(y_train_shuf, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_aug)):\n",
        "\n",
        "    x1= x_train_aug[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen2.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_aug [i] = y_train[i]\n",
        "\n",
        "    x_train = np.concatenate ((x_train_shuf, x_train_aug), axis=0)\n",
        "    y_train = np.concatenate ((y_train_shuf, y_train_aug), axis = 0)\n",
        "\n",
        "\n",
        "  ###############################################################################33\n",
        "\n",
        "\n",
        "  # x_train = np.repeat (x_train, repeats = 3, axis = 3)\n",
        "\n",
        "  # x_valid = np.repeat (x_valid, repeats = 3, axis = 3)\n",
        "        ####################################################################\n",
        "        # classification via my model\n",
        "        ####################################################################\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  model = classifier (trial)\n",
        "\n",
        "\n",
        "  # Generate our trial model.\n",
        "\n",
        "\n",
        "  model.fit(x_train,\n",
        "            np.asarray(y_train, dtype=np.float64),\n",
        "            batch_size= batch_size,\n",
        "            epochs=50,\n",
        "            callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\"), EarlyStopping(patience=10, verbose=1),\n",
        "            ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "            ModelCheckpoint(f'oct.h5', verbose=1, save_best_only=True, save_weights_only=True)],\n",
        "            validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)),\n",
        "            )\n",
        "  model.load_weights(f'oct.h5')\n",
        "  score = model.evaluate (x_valid, np.asarray(y_valid, dtype=np.float64), verbose = 1)\n",
        "\n",
        "  return score [1]\n"
      ],
      "metadata": {
        "id": "qSVsGFwCM8d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NPjODkANDkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8A9srEUdNDpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier (trial):\n",
        "\n",
        "  #num_conv_blocks = trial.suggest_int(\"num_conv_blocks\", 1, 2)\n",
        "\n",
        "  model = Sequential()\n",
        "  input_img = Input((60, 256, 1))\n",
        "  n_filters_1 = trial.suggest_categorical(\"n_filters_1\", [8, 16, 32, 64])\n",
        "  #print(f'n_filters = {n_filters}')\n",
        "\n",
        "  model.add(Conv2D(n_filters_1, kernel_size=3, activation='relu',input_shape=input_img.shape[1:], padding='same'))\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  n_filters_2 = trial.suggest_categorical(\"n_filters_2\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  model.add(Conv2D(n_filters_2, kernel_size=3, activation='relu', padding='same'))\n",
        "  model.add(MaxPool2D(2, 2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "  #for i in range(1,num_conv_blocks):\n",
        "\n",
        "    #num_filter = trial.suggest_categorical ('num_filter_l{}'.format (i+2), [16, 32, 64, 128])\n",
        "    #model.add (Conv2D (num_filter, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
        "    #model.add (MaxPool2D (2, 2))\n",
        "    #model.add (BatchNormalization ())\n",
        "\n",
        "  model.add (Flatten ())\n",
        "\n",
        "  num_layers = trial.suggest_int ('num_layers', 1, 4)\n",
        "\n",
        "  for j in range (num_layers):\n",
        "\n",
        "    n_units = trial.suggest_int(\"n_units_l{}\".format(j), 16, 4096, log = True)\n",
        "\n",
        "    model.add (Dense (n_units, activation = 'relu'))\n",
        "\n",
        "    dropout = trial.suggest_float(\"dropout_l{}\".format (j), 0, 0.7,step=0.1)\n",
        "\n",
        "    model.add (Dropout (rate = dropout))\n",
        "\n",
        "  model.add (Dense (1, activation = 'sigmoid'))\n",
        "\n",
        "  lr = trial.suggest_float ('lr', 1e-5, 1e-3, log = True)\n",
        "\n",
        "  my_optimizer = tf.keras.optimizers.Adam (learning_rate= lr)\n",
        "\n",
        "  model.compile(optimizer=my_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "mFFMZWwXIy33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold (n_splits = 5, shuffle = True, random_state = None)\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def preparing(x, y):\n",
        "\n",
        "    data  = []\n",
        "    label = []\n",
        "    for i in x:\n",
        "        for j in range(len(x[i])):\n",
        "            data.append(np.array(x[i][j]))\n",
        "            #data.append (resize (x [i] [j], (128, 128, 1), mode = 'constant', preserve_range = True))\n",
        "            label.append(y[i])\n",
        "\n",
        "\n",
        "    data = np.reshape(data, np.shape(data))\n",
        "    return data, np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def objective (trial):\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "  rotation_range= 5, # rotation\n",
        "  zoom_range =0.1,\n",
        "  fill_mode='nearest',\n",
        "  data_format='channels_last',\n",
        "    )\n",
        "\n",
        "  datagen2 = ImageDataGenerator(\n",
        "  #rotation_range= 5, # rotation\n",
        "  width_shift_range= [-30, 30], # horizontal shift\n",
        "  #height_shift_range= [-5, 5] , # vertical shift\n",
        "  zoom_range= 0.2,\n",
        "  vertical_flip= True , # vertical flip\n",
        "  #brightness_range= [0.2, 1.5]\n",
        "    )\n",
        "\n",
        "\n",
        "  images_train = pickle.load(open(\"/content/\"+\"train_thickness_retina.pkl\", 'rb'))\n",
        "  labels_train = pickle.load(open(\"/content/\"+\"train_labels.pkl\", 'rb'))\n",
        "\n",
        "  train_index, val_index = next (skf.split (images_train, list(labels_train.values())))\n",
        "\n",
        "  x_train = {i: images_train[list(images_train.keys())[i]]  for i in train_index}\n",
        "\n",
        "  x_valid = {i: images_train[list(images_train.keys())[i]]  for i in val_index}\n",
        "\n",
        "  y_train = {i: labels_train[list(labels_train.keys())[i]] for i in train_index}\n",
        "\n",
        "  y_valid = {i: labels_train[list(labels_train.keys())[i]] for i in val_index}\n",
        "\n",
        "  x_train,y_train = preparing(x_train,y_train)\n",
        "\n",
        "  x_valid,y_valid = preparing(x_valid,y_valid)\n",
        "\n",
        "\n",
        "  #rotation_angle  = trial.suggest_float(\"rotation_angle\", 0, 40, step = 10)\n",
        "  #zoom_range = trial.suggest_float(\"zoom_range\", 0, 0.5, step = 0.1)\n",
        "\n",
        "  #color_jitter_brightness = trial.suggest_float (\"color_jitter_brightness\", 0.1, 0.8, step = 0.1)\n",
        "  #shift_horizontal = trial.suggest_uniform(\"shift_horizontal\", 0, 0.7)\n",
        "  #shift_vertical = trial.suggest_uniform(\"shift_vertical\", 0, 0.7)\n",
        "  #vertical_flip_prob = trial.suggest_float (\"vertical_flip_prob\", 0, 1, step = 0.1)\n",
        "\n",
        "  indices = np.where(y_train == 1)[0]\n",
        "\n",
        "  x_train_ms = x_train[indices]\n",
        "  y_train_ms = y_train[indices]\n",
        "  #################Augmentation##################################\n",
        "\n",
        "  # First Augmentation\n",
        "\n",
        "  x_train_ms_aug =np.zeros_like(x_train_ms, dtype=np.float32)\n",
        "\n",
        "  y_train_ms_aug =np.zeros_like(y_train_ms, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_ms)):\n",
        "\n",
        "    x1= x_train_ms[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_ms_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_ms_aug [i] = y_train[i]\n",
        "\n",
        "  x_train = np.concatenate ((x_train, x_train_ms_aug), axis=0)\n",
        "  y_train = np.concatenate ((y_train, y_train_ms_aug), axis = 0)\n",
        "\n",
        "  indices = np.random.permutation (len (x_train))\n",
        "\n",
        "  x_train_shuf = x_train [indices]\n",
        "  y_train_shuf = y_train [indices]\n",
        "\n",
        "  # Second Augmentation\n",
        "\n",
        "  x_train_aug =np.zeros_like(x_train_shuf, dtype=np.float32)\n",
        "\n",
        "  y_train_aug  =np.zeros_like(y_train_shuf, dtype=np.float32)\n",
        "\n",
        "  for i in range(len(x_train_aug)):\n",
        "\n",
        "    x1= x_train_aug[i,:,:,:].copy()\n",
        "\n",
        "    x1=x1.reshape((1, ) + x1.shape)\n",
        "\n",
        "    x = datagen2.flow(x1, batch_size=1, seed=42) # to make the result reproducible\n",
        "\n",
        "    x_train_aug [i,:,:,:] = x.next()\n",
        "\n",
        "    y_train_aug [i] = y_train[i]\n",
        "\n",
        "    x_train = np.concatenate ((x_train_shuf, x_train_aug), axis=0)\n",
        "    y_train = np.concatenate ((y_train_shuf, y_train_aug), axis = 0)\n",
        "\n",
        "\n",
        "  ###############################################################################33\n",
        "\n",
        "\n",
        "  #x_train = np.repeat (x_train, repeats = 3, axis = 3)\n",
        "\n",
        "  #x_valid = np.repeat (x_valid, repeats = 3, axis = 3)\n",
        "        ####################################################################\n",
        "        # classification via my model\n",
        "        ####################################################################\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "\n",
        "  model = classifier (trial)\n",
        "\n",
        "\n",
        "  # Generate our trial model.\n",
        "\n",
        "\n",
        "  model.fit(x_train,\n",
        "            np.asarray(y_train, dtype=np.float64),\n",
        "            batch_size= batch_size,\n",
        "            epochs=50,\n",
        "            callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\"), EarlyStopping(patience=10, verbose=1),\n",
        "            ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-6),\n",
        "            ModelCheckpoint(f'slo.h5', verbose=1, save_best_only=True, save_weights_only=True)],\n",
        "            validation_data=(x_valid, np.asarray(y_valid, dtype=np.float64)),\n",
        "            )\n",
        "  model.load_weights(f'slo.h5')\n",
        "  score = model.evaluate (x_valid, np.asarray(y_valid, dtype=np.float64), verbose = 1)\n",
        "\n",
        "  return score [1]\n"
      ],
      "metadata": {
        "id": "anpfy1hrIy60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "metadata": {
        "id": "fOvpRl7Jh9c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "TEjYNByGh9c-",
        "outputId": "004beb4f-4a64-4d35-b5f3-5a3e1452ec53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  26\n",
            "  Number of pruned trials:  15\n",
            "  Number of complete trials:  10\n",
            "Best trial:\n",
            "  Value:  0.9333333373069763\n",
            "  Params: \n",
            "    batch_size: 8\n",
            "    num_conv_blocks: 3\n",
            "    n_filters_1: 32\n",
            "    num_layers: 1\n",
            "    n_units_l0: 20\n",
            "    dropout_l0: 0.4\n",
            "    lr: 0.0005522789895942119\n"
          ]
        }
      ]
    }
  ]
}